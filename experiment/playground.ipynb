{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:26:43.835029Z",
     "start_time": "2025-09-30T03:26:36.736736Z"
    }
   },
   "outputs": [],
   "source": [
    "#### FINAL VERSION ####\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer, extras_dict = get_models() \n",
    "\n",
    "l_global_step = 0\n",
    "l_epoch = 0 \n",
    "l_loss = 0 \n",
    "\n",
    "if extras_dict:\n",
    "    l_global_step = extras_dict[\"global_step\"] \n",
    "    l_epoch       = extras_dict[\"epoch\"] \n",
    "    l_loss        = extras_dict[\"loss\"]\n",
    "\n",
    "# torch.mps.empty_cache()\n",
    "# import gc; gc.collect()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoaderLite with captions + detection info.\n",
    "    Each batch element is a dict:\n",
    "      {\n",
    "        \"image\": Tensor[C,H,W],\n",
    "        \"input_ids\": Tensor[L],\n",
    "        \"attention_mask\": Tensor[L],\n",
    "        \"bboxes\": Tensor[num_img_tokens, 4],\n",
    "        \"class_labels\": Tensor[num_img_tokens],\n",
    "        \"objectness\": Tensor[num_img_tokens,1]\n",
    "      }\n",
    "    \"\"\"\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    \n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch],\n",
    "                             batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch],\n",
    "                                 batch_first=True, padding_value=0)\n",
    "\n",
    "    bboxes = torch.stack([item[\"bboxes\"] for item in batch])        # [B, num_img_tokens, 4]\n",
    "    class_labels = torch.stack([item[\"class_labels\"] for item in batch])  # [B, num_img_tokens]\n",
    "    objectness = torch.stack([item[\"objectness\"] for item in batch])       # [B, num_img_tokens, 1]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"bboxes\": bboxes,\n",
    "        \"class_labels\": class_labels,\n",
    "        \"objectness\": objectness\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection, id_to_name, name_to_id  = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, train_dataset_detection, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, val_dataset_detection, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "formatted_str+= f\"\\nTotal Training Records = {len(train_dataloader)}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:27:49.087304Z",
     "start_time": "2025-09-30T03:27:28.355910Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### TRAINING LOOP #####\n",
    "\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import numpy as np \n",
    "from utils import calculate_total_train_params, save_to_checkpoint\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "def eval():\n",
    "    decoder_model.eval()\n",
    "    encoder_model.eval()\n",
    "    val_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            # Unpack batch dict\n",
    "            image_tensor = val_batch[\"images\"].to(device)\n",
    "            caption_tensor = val_batch[\"input_ids\"].to(device)\n",
    "            attention_mask = val_batch[\"attention_mask\"].to(device)\n",
    "            bboxes = val_batch[\"bboxes\"].to(device)\n",
    "            class_labels = val_batch[\"class_labels\"].to(device)\n",
    "            objectness = val_batch[\"objectness\"].to(device)\n",
    "\n",
    "            with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "                x_embed = encoder_model(image_tensor)\n",
    "                # decoder_model should return caption_loss and detection_loss\n",
    "                #img_features, captions_tensor, attention_mask=None, bbox_targets=None, class_targets=None, objectness_targets=None, \n",
    "                logits, bbox_preds, objectness_pred, class_pred, total_loss = decoder_model(\n",
    "                    x_embed, \n",
    "                    caption_tensor, \n",
    "                    attention_mask,\n",
    "                    bbox_targets=bboxes,\n",
    "                    class_targets=class_labels,\n",
    "                    objectness_targets=objectness,\n",
    "                    mode=\"train\"\n",
    "                )\n",
    "                # combine losses\n",
    "                total_val_loss = total_loss\n",
    "\n",
    "            val_loss += total_val_loss.item()\n",
    "            count += 1\n",
    "            if count > 2: break  # quick validation\n",
    "    val_loss /= count\n",
    "    decoder_model.train()\n",
    "    encoder_model.train()\n",
    "    print(f\"Epoch {epoch+1}: train_loss={total_loss/len(train_dataloader):.4f}, val_loss={val_loss:.4f}\")\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def should_stop(loss_list):\n",
    "    last_ten_loss = loss_list[-50*4:]\n",
    "    threshold = 0.5\n",
    "    if len(last_ten_loss)==50*4 and len(loss_list)>=50*4:\n",
    "        diffs = np.diff(last_ten_loss)\n",
    "        step_trends = []\n",
    "        for d in diffs:\n",
    "            if d > threshold:\n",
    "                step_trends.append(\"increasing\")\n",
    "            elif d < -threshold:\n",
    "                step_trends.append(\"decreasing\")\n",
    "            else:\n",
    "                step_trends.append(\"steady\")\n",
    "\n",
    "        if all(t == \"steady\" for t in step_trends):\n",
    "            return True \n",
    "        else:\n",
    "            print (\"Trend\", step_trends)\n",
    "    return False \n",
    "\n",
    "\n",
    "\n",
    "##### Setup Training #####\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "all_params = calculate_total_train_params(encoder_model, decoder_model)\n",
    "\n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TrainingConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/TrainingConfig.accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 10\n",
    "stop = False \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "N_EPOCHS = TrainingConfig.epochs - l_epoch\n",
    "\n",
    "print (f\"PREVIOUS LOSS {l_loss} AT GLOBAL STEP {l_global_step} AT EPOCH {l_epoch}\")\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack batch\n",
    "        image_tensor = batch[\"images\"].to(device)\n",
    "        caption_tensor = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        bboxes = batch[\"bboxes\"].to(device)\n",
    "        class_labels = batch[\"class_labels\"].to(device)\n",
    "        objectness = batch[\"objectness\"].to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1 \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "            logits, bbox_preds, objectness_pred, class_pred, loss_1   = decoder_model(\n",
    "                x_embed, \n",
    "                caption_tensor, \n",
    "                attention_mask,\n",
    "                bbox_targets=bboxes,\n",
    "                class_targets=class_labels,\n",
    "                objectness_targets=objectness\n",
    "            ) # (B, T-1, vocab_size)\n",
    "\n",
    "            loss =  loss_1 / TrainingConfig.accumulation_steps\n",
    "\n",
    "        # x_embed = image_encoder(image_tensor) # (B, N, embed_size) \n",
    "        # logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "        # loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "        if (step + 1) % TrainingConfig.accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        if global_step %100==0:\n",
    "            val_loss = eval()\n",
    "            loss_list.append(val_loss)\n",
    "            save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "\n",
    "            if should_stop(loss_list):\n",
    "                stop = True \n",
    "                break\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{TrainingConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*TrainingConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*TrainingConfig.accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            # save_model(image_encoder=image_encoder, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % TrainingConfig.accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    if stop:\n",
    "        save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "        break\n",
    "    \n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "tup = get_models() \n",
    "\n",
    "TrainingConfig, tokenizer = tup[0], tup[-1]\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "\n",
    "#### FINAL VERSION ####\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer, extras_dict = get_models() \n",
    "\n",
    "l_global_step = 0\n",
    "l_epoch = 0 \n",
    "l_loss = 0 \n",
    "\n",
    "if extras_dict:\n",
    "    l_global_step = extras_dict[\"global_step\"] \n",
    "    l_epoch       = extras_dict[\"epoch\"] \n",
    "    l_loss        = extras_dict[\"loss\"]\n",
    "\n",
    "# torch.mps.empty_cache()\n",
    "# import gc; gc.collect()\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoaderLite with captions + detection info.\n",
    "    Each batch element is a dict:\n",
    "      {\n",
    "        \"image\": Tensor[C,H,W],\n",
    "        \"input_ids\": Tensor[L],\n",
    "        \"attention_mask\": Tensor[L],\n",
    "        \"bboxes\": Tensor[num_img_tokens, 4],\n",
    "        \"class_labels\": Tensor[num_img_tokens],\n",
    "        \"objectness\": Tensor[num_img_tokens,1]\n",
    "      }\n",
    "    \"\"\"\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    \n",
    "    input_ids = pad_sequence([item[\"input_ids\"] for item in batch],\n",
    "                             batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([item[\"attention_mask\"] for item in batch],\n",
    "                                 batch_first=True, padding_value=0)\n",
    "\n",
    "    bboxes = torch.stack([item[\"bboxes\"] for item in batch])        # [B, num_img_tokens, 4]\n",
    "    class_labels = torch.stack([item[\"class_labels\"] for item in batch])  # [B, num_img_tokens]\n",
    "    objectness = torch.stack([item[\"objectness\"] for item in batch])       # [B, num_img_tokens, 1]\n",
    "\n",
    "    return {\n",
    "        \"images\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"bboxes\": bboxes,\n",
    "        \"class_labels\": class_labels,\n",
    "        \"objectness\": objectness\n",
    "    }\n",
    "\n",
    "TrainingConfig.batch_size = 1 \n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection, id_to_name, name_to_id  = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, train_dataset_detection, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, val_dataset_detection, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "formatted_str+= f\"\\nTotal Training Records = {len(train_dataloader)}\"\n",
    "\n",
    "print (formatted_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "from transformers import LogitsProcessorList, MinLengthLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
    "\n",
    "from transformers.generation.logits_process import (\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "\n",
    "# Chain multiple warpers\n",
    "warpers = LogitsProcessorList([\n",
    "    TemperatureLogitsWarper(0.3),\n",
    "    TopKLogitsWarper(20),\n",
    "    TopPLogitsWarper(0.95),\n",
    "])\n",
    "\n",
    "processors = LogitsProcessorList([\n",
    "    RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "])\n",
    "\n",
    "from utils import load_from_checkpoint \n",
    "\n",
    "encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def generate_caption(image_tensor, max_len=30, use_image=True):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "\n",
    "    # Only process image if use_image is True\n",
    "    if use_image:\n",
    "        x_embed = encoder(image_tensor)\n",
    "    else:\n",
    "        # Create dummy embeddings if not using image\n",
    "        x_embed = torch.zeros((1, 50, decoder.embed_size), device=device)\n",
    "    \n",
    "    # Start with the START token\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    generated_ids = torch.tensor([[start_id]], device=device)\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "\n",
    "    # Setup warpers and processors for better text generation\n",
    "    warpers_list = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(0.7),  # Lower temperature for less randomness\n",
    "        TopKLogitsWarper(50),         # More top-k options\n",
    "        TopPLogitsWarper(0.92),       # Slightly more conservative top-p\n",
    "    ])\n",
    "    \n",
    "    processors_list = LogitsProcessorList([\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=1.5),  # Stronger repetition penalty\n",
    "        # MinLengthLogitsProcessor(5, end_token_id)      # Ensure minimum caption length\n",
    "    ])\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            # The decoder internally handles creating the appropriate attention mask\n",
    "            # Just pass a simple mask for the text tokens - decoder will handle the rest\n",
    "            attn_mask = torch.ones(1, generated_ids.shape[1], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Important: Use mode=\"inference\" for generation\n",
    "            logits, bbox_preds, objectness_pred, class_pred, _ = decoder(x_embed, generated_ids, attn_mask, mode=\"inference\")\n",
    "\n",
    "            # Get the logits for the next token only (the last position)\n",
    "            # The -1 index accounts for the concatenated image tokens in the decoder\n",
    "            next_token_idx = -1\n",
    "            next_logits = logits[:, next_token_idx, :]\n",
    "            \n",
    "            # Process logits to avoid repetition\n",
    "            next_logits = processors_list(generated_ids, next_logits)\n",
    "            \n",
    "            # Apply temperature and top-k/top-p filtering\n",
    "            next_logits = warpers_list(generated_ids, next_logits)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            \n",
    "            # Sample the next token\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Add the new token to our sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "            \n",
    "            # Stop if we generated an END token\n",
    "            if next_token_id.item() == end_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the generated tokens to text, skipping special tokens\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    # Optional: visualize predicted boxes\n",
    "    image_tensor = image_tensor.to(torch.float32)\n",
    "\n",
    "    print (\"=*=\"*60)\n",
    "    print (f\"\\t bbox_preds {bbox_preds.shape}\")\n",
    "    print (f\"\\t objectness_pred {objectness_pred.shape}\")\n",
    "    print (f\"\\t class_pred {class_pred.shape}\")\n",
    "    print (\"=*=\"*60)\n",
    "\n",
    "\n",
    "    if 1:\n",
    "        bboxes = bbox_preds[0].to(torch.float32).cpu().numpy()   # normalized [cx,cy,w,h]. @[1,64,4]\n",
    "        class_pred = class_pred[0].to(torch.float32).cpu().numpy() #[1, 64]\n",
    "        scores = objectness_pred[0].to(torch.float32).cpu().numpy() #[1, 64]\n",
    "\n",
    "        detections = []\n",
    "        for (cx, cy, w, h), score, cls_id in zip(bboxes, scores, class_pred):\n",
    "            if score > 0.2:  # confidence threshold\n",
    "                x1 = (cx - w/2) * image_tensor.shape[-1]\n",
    "                y1 = (cy - h/2) * image_tensor.shape[-2]\n",
    "                x2 = (cx + w/2) * image_tensor.shape[-1]\n",
    "                y2 = (cy + h/2) * image_tensor.shape[-2]\n",
    "\n",
    "                detections.append({\n",
    "                    \"class_id\": int(cls_id),\n",
    "                    \"class_name\": id_to_name[int(cls_id)] if int(cls_id) < len(id_to_name) else f\"cls_{int(cls_id)}\",\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(x1), float(y1), float(x2), float(y2)]\n",
    "                })\n",
    "\n",
    "        # Sort detections by confidence\n",
    "        detections = sorted(detections, key=lambda x: x[\"score\"], reverse=True)\n",
    "    \n",
    "        # Print detections\n",
    "        print(\"Detected objects:\")\n",
    "        for det in detections:\n",
    "            print(f\" - {det['class_name']} (ID: {det['class_id']}), Score: {det['score']:.2f}, BBox: {det['bbox']}\")\n",
    "            \n",
    "\n",
    "    \n",
    "    return caption\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "# Example usage\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor = batch[\"images\"].to(device)\n",
    "    caption_tensor = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    bboxes = batch[\"bboxes\"].to(device)\n",
    "    class_labels = batch[\"class_labels\"].to(device)\n",
    "    objectness = batch[\"objectness\"].to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "\n",
    "\n",
    "    #caption_without_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=False)\n",
    "\n",
    "    caption_with_image = generate_caption(image_tensor[0], use_image=True)\n",
    "\n",
    "    plt.imshow(image_tensor[0].float().permute(1,2,0).cpu().numpy())\n",
    "    print(\"With image context:\\t \\t\", caption_with_image)\n",
    "    print (\"Actual\\t\\t\", tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True))\n",
    "    # print(\"Without image context: \", caption_without_image)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75993e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate_caption, visualize_caption\n",
    "import torch \n",
    "\n",
    "from utils import load_from_checkpoint \n",
    "\n",
    "encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "# Pick a random sample from the validation set\n",
    "import random\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True) \n",
    "    print(\"Ground Truth:\\t\", gt_caption)\n",
    "\n",
    "\n",
    "    # Generate a caption\n",
    "    caption = generate_caption(\n",
    "        image_tensor,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        temperature=0.7,  # Lower for less randomness\n",
    "        repetition_penalty=1.5  # Higher to avoid repetition\n",
    "    )\n",
    "    # Visualize the result\n",
    "    visualize_caption(image_tensor, caption, gt_caption)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45828345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed MHA implementation with multiple images\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_on_samples(num_samples=5, temperature_values=[0.6, 0.7, 0.8]):\n",
    "    \"\"\"Test the model on multiple samples with different temperature settings\"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Get a few random samples from validation set\n",
    "    val_batches = []\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        val_batches.append(batch)\n",
    "    \n",
    "    print(f\"Evaluating on {len(val_batches)} validation samples with different temperatures\")\n",
    "    \n",
    "    # Try different temperature values\n",
    "    for temp in temperature_values:\n",
    "        print(f\"\\nUsing temperature: {temp}\")\n",
    "        \n",
    "        for batch in val_batches:\n",
    "            image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2]\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            \n",
    "            # Get ground truth\n",
    "            gt = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True)\n",
    "            \n",
    "            # Generate caption with current temperature\n",
    "            caption = generate_caption(\n",
    "                image_tensor[0],\n",
    "                encoder,\n",
    "                decoder,\n",
    "                tokenizer,\n",
    "                device,\n",
    "                temperature=temp,\n",
    "                repetition_penalty=1.5\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            print(f\"GT: {gt}\")\n",
    "            print(f\"Generated: {caption}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            all_results.append({\n",
    "                \"temperature\": temp,\n",
    "                \"ground_truth\": gt,\n",
    "                \"generated\": caption\n",
    "            })\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run evaluation\n",
    "try:\n",
    "    results = evaluate_on_samples(num_samples=3, temperature_values=[0.6, 0.8])\n",
    "    print(\"MHA fix successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in MHA: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6032ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention between image features and text\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def visualize_attention(image_tensor, encoder, decoder, tokenizer, device):\n",
    "    \"\"\"Visualize the cross-attention between image features and text tokens\"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Process image\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        img_features = encoder(image_tensor)\n",
    "        \n",
    "    # Create a simple prompt\n",
    "    start_token = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    input_ids = torch.tensor([[start_token]], device=device)\n",
    "    \n",
    "    # Get the embeddings\n",
    "    B = 1\n",
    "    queries = decoder.img_queries.unsqueeze(0).expand(B, -1, -1)  # (B, num_img_tokens, D)\n",
    "    \n",
    "    # Project image features\n",
    "    k = decoder.key_proj(img_features)\n",
    "    v = decoder.value_proj(img_features)\n",
    "    \n",
    "    # Get query projections\n",
    "    with torch.no_grad():\n",
    "        q = decoder.query_proj(queries)\n",
    "        \n",
    "    # Compute attention weights manually\n",
    "    q = q / np.sqrt(decoder.embed_size)\n",
    "    attention_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    attention_probs = F.softmax(attention_weights, dim=-1)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Show the image\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(image_tensor[0].float().permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show the attention map (average over heads)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    attention_map = attention_probs[0].mean(0).cpu().numpy()\n",
    "    plt.imshow(attention_map, cmap='viridis')\n",
    "    plt.title(\"Image-Text Attention Map\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Show a few sample attention distributions\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i in range(min(5, decoder.num_img_tokens)):\n",
    "        plt.plot(attention_probs[0, i].cpu().numpy(), \n",
    "                 label=f\"Query token {i}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Attention Distribution for First 5 Query Tokens\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return attention_probs\n",
    "\n",
    "# Get a sample image\n",
    "for batch in val_dataloader:\n",
    "    image_tensor = batch[0][0]\n",
    "    break\n",
    "\n",
    "# Visualize attention\n",
    "try:\n",
    "    attention_probs = visualize_attention(image_tensor, encoder, decoder, tokenizer, device)\n",
    "    print(\"Successfully visualized attention!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in attention visualization: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved model with anti-hallucination measures\n",
    "from utils import load_from_checkpoint\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the model with our improvements\n",
    "encoder, decoder, epoch, loss, global_step, tokenizer = load_from_checkpoint()\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "def test_hallucination_reduction():\n",
    "    \"\"\"Test if our improvements have reduced hallucinations\"\"\"\n",
    "    # Get some validation samples\n",
    "    val_samples = []\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        val_samples.append(batch)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing hallucination reduction with improved model:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for batch in val_samples:\n",
    "        image_tensor, caption_tensor, attention_mask = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        # Generate caption with our improved model\n",
    "        try:\n",
    "            from generate import generate_caption\n",
    "            \n",
    "            caption = generate_caption(\n",
    "                image_tensor[0],\n",
    "                encoder,\n",
    "                decoder,\n",
    "                tokenizer,\n",
    "                device,\n",
    "                temperature=0.6,\n",
    "                repetition_penalty=1.8\n",
    "            )\n",
    "            \n",
    "            # Display results\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(image_tensor[0].float().permute(1, 2, 0).cpu().numpy())\n",
    "            plt.title(f\"Generated: {caption}\\nGround Truth: {gt_caption}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            results.append({\n",
    "                'gt': gt_caption,\n",
    "                'generated': caption,\n",
    "                'success': True\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption: {e}\")\n",
    "            results.append({\n",
    "                'gt': gt_caption,\n",
    "                'generated': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "hallucination_test_results = test_hallucination_reduction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Q-Former style implementation\n",
    "import torch\n",
    "from utils import load_from_checkpoint\n",
    "from generate import generate_caption\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Testing the Q-Former style implementation...\")\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    encoder, decoder, epoch, loss, global_step, tokenizer = load_from_checkpoint()\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Get a batch from the validation set\n",
    "    for batch in val_dataloader:\n",
    "        image_tensor, caption_tensor, attention_mask = [x.to(device) for x in batch]\n",
    "        break\n",
    "\n",
    "    # Get ground truth caption\n",
    "    gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True)\n",
    "    print(f\"Ground truth: {gt_caption}\")\n",
    "    \n",
    "    # Generate caption with Q-Former style model\n",
    "    caption = generate_caption(\n",
    "        image_tensor[0],\n",
    "        encoder,\n",
    "        decoder,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    \n",
    "    print(f\"Q-Former generated: {caption}\")\n",
    "    \n",
    "    # Visualize the image and captions\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image_tensor[0].cpu().permute(1, 2, 0).numpy())\n",
    "    plt.title(f\"Generated: {caption}\\nGround Truth: {gt_caption}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Q-Former test completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing Q-Former implementation: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69862487",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tokens = [1, 2, 3, 4]\n",
    "image_mask = torch.tensor([1, 1, 1, 1], dtype=torch.long).bool()\n",
    "\n",
    "text_tokens = [10, 11, 12, 13] \n",
    "causal_mask = torch.tril(torch.ones((len(text_tokens), len(text_tokens)))).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d454ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tensor = torch.cat([torch.tensor(image_tokens), torch.tensor(text_tokens)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c01a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b01fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.tril(torch.ones((len(embed_tensor), len(embed_tensor)))).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960812f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask[0:image_mask.shape[0], :image_mask.shape[0]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask.unsqueeze(0).expand(4, -1, -1) .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf45578",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask = torch.cat([image_mask, image_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e174959",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask = image_mask.unsqueeze(0).unsqueeze(1) \n",
    "image_mask, image_mask.shape, causal_mask.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d83fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = image_mask & causal_mask\n",
    "r, r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Linear(4, 1, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ca346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (next(layer.parameters()).dtype), print (next(layer.parameters()).device), print (next(layer.parameters()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
