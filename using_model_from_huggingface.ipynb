{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16960f0",
   "metadata": {},
   "source": [
    "# Using QVision MultiModel from Hugging Face Hub\n",
    "\n",
    "This notebook demonstrates how to load and use the QVision MultiModel directly from Hugging Face Hub for image captioning tasks.\n",
    "\n",
    "The model is available at: [verma75preetam/qvision-mutlimodel-base](https://huggingface.co/verma75preetam/qvision-mutlimodel-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c69d2",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install huggingface_hub safetensors transformers matplotlib torch pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6ad6c",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da633de",
   "metadata": {},
   "source": [
    "## 3. Download Model Files from Hugging Face Hub\n",
    "\n",
    "Now, we'll download the model files directly from Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c788a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model repository on Hugging Face Hub\n",
    "model_name = \"verma75preetam/qvision-mutlimodel-base\"\n",
    "\n",
    "# Download model files\n",
    "print(\"Downloading model files...\")\n",
    "config_path = hf_hub_download(repo_id=model_name, filename=\"config.json\")\n",
    "encoder_path = hf_hub_download(repo_id=model_name, filename=\"encoder.safetensors\")\n",
    "decoder_path = hf_hub_download(repo_id=model_name, filename=\"decoder.safetensors\")\n",
    "gpt_decoder_path = hf_hub_download(repo_id=model_name, filename=\"gpt_decoder.safetensors\")\n",
    "\n",
    "# Download helper scripts\n",
    "encoder_py_path = hf_hub_download(repo_id=model_name, filename=\"encoder.py\")\n",
    "decoder_model_py_path = hf_hub_download(repo_id=model_name, filename=\"decoder_model.py\")\n",
    "generate_py_path = hf_hub_download(repo_id=model_name, filename=\"generate.py\")\n",
    "utils_py_path = hf_hub_download(repo_id=model_name, filename=\"utils.py\")\n",
    "\n",
    "# Print paths for confirmation\n",
    "print(f\"Config path: {config_path}\")\n",
    "print(f\"Encoder model path: {encoder_path}\")\n",
    "print(f\"Decoder model path: {decoder_path}\")\n",
    "print(f\"GPT decoder model path: {gpt_decoder_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c028e",
   "metadata": {},
   "source": [
    "## 4. Import Model Classes\n",
    "\n",
    "Add the downloaded scripts to the Python path and import the model classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f7140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directory containing the scripts to the path\n",
    "sys.path.append(os.path.dirname(encoder_py_path))\n",
    "\n",
    "# Import the model classes\n",
    "try:\n",
    "    from encoder import CLIPEncoder\n",
    "    from decoder_model import ResnetGPT2Wrapper\n",
    "    from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "    from generate import generate_caption, visualize_caption\n",
    "    print(\"Successfully imported model classes!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing model classes: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96928a1b",
   "metadata": {},
   "source": [
    "## 5. Initialize the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GPT-NEO-350M\")\n",
    "special_tokens = {\"additional_special_tokens\": [\"<START>\", \"<END>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Set pad token if not defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "print(f\"Tokenizer ready with vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c475b",
   "metadata": {},
   "source": [
    "## 6. Load Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "print(\"Loading model configuration...\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a84c0",
   "metadata": {},
   "source": [
    "## 7. Initialize Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee955bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the encoder\n",
    "print(\"Initializing encoder...\")\n",
    "encoder = CLIPEncoder(config['embed_size'])\n",
    "\n",
    "# Initialize GPT model\n",
    "print(\"Initializing GPT model...\")\n",
    "gpt_model = GPTNeoForCausalLM.from_pretrained(\"GPT-NEO-350M\")\n",
    "gpt_model.resize_token_embeddings(gpt_model.get_input_embeddings().num_embeddings + 2)  # For special tokens\n",
    "\n",
    "# Initialize decoder\n",
    "print(\"Initializing decoder...\")\n",
    "decoder = ResnetGPT2Wrapper(\n",
    "    gpt_decoder=gpt_model,\n",
    "    embed_size=config['embed_size'],\n",
    "    vocab_size=config['vocab_size'],\n",
    "    num_img_tokens=config['num_img_tokens'],\n",
    "    pad_token_id=pad_token_id\n",
    ")\n",
    "\n",
    "print(\"Model components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be923cf",
   "metadata": {},
   "source": [
    "## 8. Load Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c344d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights from safetensors\n",
    "print(\"Loading model weights from safetensors...\")\n",
    "encoder_state_dict = load_file(encoder_path)\n",
    "decoder_state_dict = load_file(decoder_path)\n",
    "gpt_decoder_state_dict = load_file(gpt_decoder_path)\n",
    "\n",
    "# Load state dictionaries into models\n",
    "print(\"Applying weights to model components...\")\n",
    "encoder.load_state_dict(encoder_state_dict)\n",
    "decoder.load_state_dict(decoder_state_dict)\n",
    "decoder.gpt_decoder.load_state_dict(gpt_decoder_state_dict)\n",
    "\n",
    "print(\"Model weights loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe57219",
   "metadata": {},
   "source": [
    "## 9. Prepare Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cuda\" if torch.cuda.is_available() else \n",
    "                     \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move models to device and convert to bfloat16 for efficiency\n",
    "encoder = encoder.to(device).to(torch.bfloat16)\n",
    "decoder = decoder.to(device).to(torch.bfloat16)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"Model ready for inference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4cdb1",
   "metadata": {},
   "source": [
    "## 10. Define Helper Functions for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path, device):\n",
    "    \"\"\"Load and preprocess an image for the model.\"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Apply transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Process image\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    return image, image_tensor\n",
    "\n",
    "def display_image_with_caption(image, caption):\n",
    "    \"\"\"Display an image with its generated caption.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.text(0.1, 0.5, f\"Caption: {caption}\", fontsize=12, wrap=True)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Caption\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6417ac4b",
   "metadata": {},
   "source": [
    "## 11. Download a Sample Image for Testing\n",
    "\n",
    "We'll download a sample image from Hugging Face Hub for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image\n",
    "sample_image_path = hf_hub_download(repo_id=model_name, filename=\"sample_image.jpg\")\n",
    "print(f\"Sample image downloaded to: {sample_image_path}\")\n",
    "\n",
    "# Display the sample image\n",
    "sample_image = Image.open(sample_image_path)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bda918",
   "metadata": {},
   "source": [
    "## 12. Generate Caption for the Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa184f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the sample image\n",
    "image, image_tensor = process_image(sample_image_path, device)\n",
    "\n",
    "# Generate caption\n",
    "print(\"Generating caption...\")\n",
    "with torch.no_grad():\n",
    "    caption, generated_ids, _ = generate_caption(\n",
    "        image_tensor,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "print(f\"\\nGenerated caption: {caption}\")\n",
    "\n",
    "# Display the image with its caption\n",
    "display_image_with_caption(image, caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434a9cb",
   "metadata": {},
   "source": [
    "## 13. Try with Your Own Images\n",
    "\n",
    "You can upload your own images and generate captions for them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3848a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle image upload and caption generation\n",
    "def caption_uploaded_image(uploaded_file):\n",
    "    # Save the uploaded file\n",
    "    with open(\"uploaded_image.jpg\", \"wb\") as f:\n",
    "        f.write(uploaded_file.getvalue())\n",
    "    \n",
    "    # Process the image\n",
    "    image, image_tensor = process_image(\"uploaded_image.jpg\", device)\n",
    "    \n",
    "    # Generate caption\n",
    "    print(\"Generating caption...\")\n",
    "    with torch.no_grad():\n",
    "        caption, generated_ids, _ = generate_caption(\n",
    "            image_tensor,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.5\n",
    "        )\n",
    "    \n",
    "    # Display the image with its caption\n",
    "    display_image_with_caption(image, caption)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Note: If running in Jupyter Notebook, you can use the following code for file upload\n",
    "# from ipywidgets import FileUpload\n",
    "# uploader = FileUpload(accept='image/*', multiple=False)\n",
    "# display(uploader)\n",
    "\n",
    "# After upload, you can call:\n",
    "# if uploader.value:\n",
    "#     caption = caption_uploaded_image(list(uploader.value.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459cef5b",
   "metadata": {},
   "source": [
    "## 14. Generate Captions for Multiple Images\n",
    "\n",
    "If you have a directory of images, you can generate captions for all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8853b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_captions_for_directory(image_dir, limit=5):\n",
    "    \"\"\"Generate captions for all images in a directory.\"\"\"\n",
    "    # Get all image files\n",
    "    image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"]\n",
    "    image_files = []\n",
    "    \n",
    "    for file in os.listdir(image_dir):\n",
    "        if any(file.lower().endswith(ext) for ext in image_extensions):\n",
    "            image_files.append(os.path.join(image_dir, file))\n",
    "    \n",
    "    # Limit the number of images processed\n",
    "    if len(image_files) > limit:\n",
    "        print(f\"Found {len(image_files)} images, limiting to {limit} as specified.\")\n",
    "        image_files = image_files[:limit]\n",
    "    else:\n",
    "        print(f\"Found {len(image_files)} images.\")\n",
    "    \n",
    "    # Process each image\n",
    "    results = []\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        print(f\"\\nProcessing image {i+1}/{len(image_files)}: {os.path.basename(img_path)}\")\n",
    "        \n",
    "        # Process image\n",
    "        try:\n",
    "            image, image_tensor = process_image(img_path, device)\n",
    "            \n",
    "            # Generate caption\n",
    "            with torch.no_grad():\n",
    "                caption, generated_ids, _ = generate_caption(\n",
    "                    image_tensor,\n",
    "                    encoder,\n",
    "                    decoder,\n",
    "                    tokenizer,\n",
    "                    device,\n",
    "                    temperature=0.7,\n",
    "                    repetition_penalty=1.5\n",
    "                )\n",
    "            \n",
    "            print(f\"Caption: {caption}\")\n",
    "            results.append((img_path, caption, image))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path}: {e}\")\n",
    "    \n",
    "    # Display results\n",
    "    n_images = len(results)\n",
    "    if n_images > 0:\n",
    "        fig = plt.figure(figsize=(15, 5 * n_images))\n",
    "        \n",
    "        for i, (img_path, caption, image) in enumerate(results):\n",
    "            plt.subplot(n_images, 2, 2*i + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Image: {os.path.basename(img_path)}\")\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "            plt.subplot(n_images, 2, 2*i + 2)\n",
    "            plt.text(0.1, 0.5, f\"Caption: {caption}\", fontsize=12, wrap=True)\n",
    "            plt.axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage (uncomment to run):\n",
    "# image_directory = \"path/to/your/images\"\n",
    "# results = generate_captions_for_directory(image_directory, limit=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb17eb0",
   "metadata": {},
   "source": [
    "## 15. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "\n",
    "1. Load the QVision MultiModel directly from Hugging Face Hub\n",
    "2. Initialize all model components\n",
    "3. Generate captions for individual images\n",
    "4. Process multiple images in a directory\n",
    "\n",
    "The model uses a Q-Former architecture that efficiently connects CLIP's visual representations with GPT-Neo's text generation capabilities, making it suitable for various vision-language tasks beyond just captioning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
