{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9147827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from transformers import GroundingDinoConfig\n",
    "import torch \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "config = GroundingDinoConfig()\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "\n",
    "\n",
    "if not hasattr(torch.nn.functional, \"_original_grid_sample\"):\n",
    "    torch.nn.functional._original_grid_sample = torch.nn.functional.grid_sample\n",
    "\n",
    "    def safe_grid_sample(input, grid, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=None):\n",
    "        if input.device.type == \"mps\":\n",
    "            # Move to CPU + float32\n",
    "            input_cpu = input.to(\"cpu\", dtype=torch.float32)\n",
    "            grid_cpu = grid.to(\"cpu\", dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                output_cpu = torch.nn.functional._original_grid_sample(\n",
    "                    input_cpu, grid_cpu, mode=mode, padding_mode=padding_mode, align_corners=align_corners\n",
    "                )\n",
    "            return output_cpu.to(\"mps\", dtype=input.dtype)\n",
    "        else:\n",
    "            return torch.nn.functional._original_grid_sample(\n",
    "                input, grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners\n",
    "            )\n",
    "\n",
    "torch.nn.functional.grid_sample = safe_grid_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707da0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer\n",
    "import re \n",
    "\n",
    "\n",
    "UI_VOCAB = [\n",
    "    \"button\", \"icon\", \"text\", \"image\", \"input\", \"checkbox\", \"link\",\n",
    "    \"menu item\", \"banner\", \"avatar\", \"logo\", \"label\", \"switch\",\n",
    "    \"tab\", \"card\", \"popup\", \"dropdown\", \"textfield\", \"container\"\n",
    "]\n",
    "\n",
    "UI_TEXT_MAP = {\n",
    "    \"image\": [\"picture\", \"photo\", \"avatar\", \"logo\", \"icon\"],\n",
    "    \"button\": [\"button\", \"tap\", \"click\", \"submit\"],\n",
    "    \"text\": [\"text\", \"label\"],\n",
    "    \"input\": [\"input\", \"field\", \"search\", \"textbox\"],\n",
    "}\n",
    "\n",
    "\n",
    "class UIElementClassifier:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.vocab_emb = self.model.encode(vocab, normalize_embeddings=True)\n",
    "        self.processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "\n",
    "\n",
    "\n",
    "    def build_positive_map(self, text: str, phrases: list[str]) -> torch.Tensor:\n",
    "        # Tokenize text using DINO processor to get offsets\n",
    "        encoding = self.processor(\n",
    "            text=[text],\n",
    "            return_tensors=\"pt\",\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        offsets = encoding[\"offset_mapping\"][0]  # [seq_len, 2]\n",
    "        positive_map = torch.zeros((len(phrases), offsets.shape[0]), dtype=torch.bool)\n",
    "\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            # Map to candidate synonyms if available\n",
    "            candidates = UI_TEXT_MAP.get(phrase.lower().strip(), [phrase])\n",
    "            candidates+=[phrase]\n",
    "\n",
    "            for cand in candidates:\n",
    "                for match in re.finditer(re.escape(cand), text_lower):\n",
    "                    start, end = match.span()\n",
    "                    # Mark all tokens that overlap the match\n",
    "                    for j, (s_tok, e_tok) in enumerate(offsets):\n",
    "                        if e_tok > start and s_tok < end:\n",
    "                            positive_map[i, j] = True\n",
    "\n",
    "        return positive_map\n",
    "\n",
    "        \n",
    "    def classify(self, text):\n",
    "        \"\"\"Return best-matching UI term for the referring expression.\"\"\"\n",
    "        query_emb = self.model.encode(text, normalize_embeddings=True)\n",
    "        sim = util.cos_sim(query_emb, self.vocab_emb)[0]\n",
    "        idx = torch.argmax(sim).item()\n",
    "        return self.vocab[idx], sim[idx].item()\n",
    "\n",
    "\n",
    "# text = \"choose the last row second image at the bottom\"\n",
    "# obj = UIElementClassifier(UI_VOCAB)\n",
    "# ans, sim = obj.classify(text)\n",
    "\n",
    "# print (ans, sim)\n",
    "# positive_map = obj.build_positive_map(text, [ans])\n",
    "# print (positive_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367a5893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/58142.jpg', (0.032, 0.564, 0.483, 0.818))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13912.jpg', (0.357, 0.102, 0.643, 0.262))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9125.jpg', (0.049, 0.762, 0.952, 0.817))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9125.jpg', (0.499, 0.817, 0.952, 0.882))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9125.jpg', (0.049, 0.614, 0.952, 0.669))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9125.jpg', (0.049, 0.336, 0.952, 0.383))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9125.jpg', (0.049, 0.682, 0.952, 0.748))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6902.jpg', (0.039, 0.116, 0.209, 0.212))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/69504.jpg', (0.138, 0.109, 0.862, 0.173))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38433.jpg', (0.456, 0.853, 0.544, 0.903))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38433.jpg', (0.039, 0.63, 0.16, 0.698))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/53010.jpg', (0.872, 0.242, 0.952, 0.285))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/481.jpg', (0.51, 0.396, 0.952, 0.669))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6357.jpg', (0.879, 0.033, 1.0, 0.109))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6357.jpg', (0.499, 0.895, 0.552, 0.924))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6357.jpg', (0.71, 0.895, 0.855, 0.924))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6357.jpg', (0.856, 0.895, 1.0, 0.924))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62491.jpg', (0.039, 0.833, 0.961, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/45161.jpg', (0.185, 0.668, 0.341, 0.755))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/45161.jpg', (0.526, 0.668, 0.681, 0.755))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/47184.jpg', (0.8, 0.874, 1.0, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/47184.jpg', (0.208, 0.874, 0.391, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/47184.jpg', (0.093, 0.654, 0.907, 0.722))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/47184.jpg', (0.408, 0.874, 0.591, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64404.jpg', (0.024, 0.761, 0.268, 0.795))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/10191.jpg', (0.028, 0.123, 0.972, 0.423))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/10191.jpg', (0.865, 0.038, 1.0, 0.103))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/49718.jpg', (0.879, 0.299, 0.927, 0.327))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/50395.jpg', (0.379, 0.142, 0.621, 0.279))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/44067.jpg', (0.75, 0.826, 1.0, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/44067.jpg', (0.807, 0.086, 1.0, 0.194))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/44067.jpg', (0.685, 0.086, 0.807, 0.194))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24772.jpg', (0.01, 0.115, 0.495, 0.356))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24772.jpg', (0.505, 0.361, 0.991, 0.602))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26379.jpg', (0.331, 0.387, 0.669, 0.417))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26379.jpg', (0.355, 0.271, 0.646, 0.337))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/37691.jpg', (0.709, 0.051, 0.782, 0.092))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/61351.jpg', (0.039, 0.681, 0.961, 0.76))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/47237.jpg', (0.588, 0.097, 0.673, 0.121))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/47237.jpg', (0.361, 0.789, 0.499, 0.866))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/50060.jpg', (0.816, 0.758, 0.971, 0.845))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/31763.jpg', (0.506, 0.607, 0.991, 0.848))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/31763.jpg', (0.009, 0.853, 0.494, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/227.jpg', (0.5, 0.703, 0.75, 0.844))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/36487.jpg', (0.844, 0.343, 0.937, 0.408))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/36487.jpg', (0.844, 0.255, 0.937, 0.321))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/39517.jpg', (0.631, 0.039, 0.767, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9633.jpg', (0.446, 0.75, 0.554, 0.809))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9633.jpg', (0.407, 0.817, 0.593, 0.921))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9633.jpg', (0.872, 0.493, 0.976, 0.551))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38714.jpg', (0.063, 0.645, 0.891, 0.711))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38714.jpg', (0.063, 0.178, 0.891, 0.244))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38714.jpg', (0.063, 0.311, 0.891, 0.378))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38714.jpg', (0.063, 0.378, 0.891, 0.444))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/25099.jpg', (0.01, 0.115, 0.495, 0.356))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/25099.jpg', (0.505, 0.853, 0.991, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/25099.jpg', (0.01, 0.853, 0.495, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6488.jpg', (0.124, 0.169, 0.891, 0.235))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6488.jpg', (0.124, 0.123, 0.173, 0.15))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6488.jpg', (0.124, 0.169, 0.891, 0.235))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6488.jpg', (0.185, 0.102, 0.891, 0.168))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6488.jpg', (0.063, 0.123, 0.112, 0.15))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28026.jpg', (0.781, 0.804, 0.879, 0.858))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28026.jpg', (0.869, 0.744, 0.963, 0.796))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54896.jpg', (0.505, 0.607, 0.991, 0.848))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43781.jpg', (0.452, 0.112, 0.549, 0.16))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43781.jpg', (0.6, 0.098, 0.8, 0.175))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/1225.jpg', (0.5, 0.109, 0.75, 0.208))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/1225.jpg', (0.413, 0.873, 0.588, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19638.jpg', (0.337, 0.481, 0.666, 0.667))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19638.jpg', (0.671, 0.481, 1.0, 0.667))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/44969.jpg', (0.515, 0.236, 0.569, 0.267))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/44969.jpg', (0.404, 0.066, 0.598, 0.175))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26550.jpg', (0.389, 0.455, 0.493, 0.494))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26550.jpg', (0.389, 0.143, 0.493, 0.181))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6324.jpg', (0.048, 0.203, 0.952, 0.677))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/18755.jpg', (0.822, 0.542, 0.9, 0.585))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14280.jpg', (0.864, 0.039, 1.0, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/8006.jpg', (0.674, 0.315, 0.973, 0.38))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/56748.jpg', (0.033, 0.449, 0.111, 0.493))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/66435.jpg', (0.919, 0.211, 0.995, 0.254))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24156.jpg', (0.135, 0.624, 0.221, 0.672))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52884.jpg', (0.787, 0.039, 0.904, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40183.jpg', (0.383, 0.284, 0.481, 0.339))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40183.jpg', (0.383, 0.613, 0.481, 0.667))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/10022.jpg', (0.932, 0.046, 1.0, 0.078))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/10022.jpg', (0.834, 0.046, 0.907, 0.078))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/22699.jpg', (0.864, 0.033, 1.0, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6986.jpg', (0.052, 0.044, 0.13, 0.087))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6986.jpg', (0.241, 0.442, 0.759, 0.485))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/29857.jpg', (0.864, 0.313, 0.971, 0.376))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/35896.jpg', (0.5, 0.87, 0.75, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/48717.jpg', (0.786, 0.039, 0.903, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63048.jpg', (0.01, 0.109, 0.495, 0.191))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63048.jpg', (0.505, 0.689, 0.991, 0.929))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63463.jpg', (0.335, 0.098, 0.668, 0.164))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63463.jpg', (0.018, 0.257, 0.149, 0.331))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64471.jpg', (0.889, 0.754, 0.961, 0.796))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/2891.jpg', (0.054, 0.259, 0.946, 0.636))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/2891.jpg', (0.054, 0.259, 0.946, 0.636))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/2891.jpg', (0.054, 0.259, 0.946, 0.636))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/2891.jpg', (0.836, 0.812, 0.914, 0.856))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/37058.jpg', (0.132, 0.123, 0.201, 0.161))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43143.jpg', (0.75, 0.191, 1.0, 0.245))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/30166.jpg', (0.019, 0.044, 0.135, 0.087))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54144.jpg', (0.734, 0.676, 0.925, 0.71))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54144.jpg', (0.078, 0.282, 0.922, 0.334))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54144.jpg', (0.078, 0.589, 0.922, 0.655))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/71153.jpg', (0.891, 0.05, 0.963, 0.091))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/476.jpg', (0.039, 0.208, 0.117, 0.252))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64991.jpg', (0.059, 0.196, 0.943, 0.261))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64991.jpg', (0.059, 0.402, 0.943, 0.463))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/67551.jpg', (0.815, 0.758, 0.97, 0.846))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/58037.jpg', (0.654, 0.153, 0.961, 0.198))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24501.jpg', (0.826, 0.609, 0.961, 0.685))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24501.jpg', (0.904, 0.039, 1.0, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24501.jpg', (0.787, 0.039, 0.904, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/2973.jpg', (0.84, 0.126, 0.918, 0.169))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28009.jpg', (0.767, 0.039, 0.883, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/65880.jpg', (0.037, 0.206, 0.095, 0.239))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/61589.jpg', (0.894, 0.041, 1.0, 0.101))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14107.jpg', (0.786, 0.039, 0.903, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14107.jpg', (0.903, 0.039, 1.0, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/30160.jpg', (0.506, 0.115, 0.991, 0.355))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/30160.jpg', (0.009, 0.115, 0.494, 0.355))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24037.jpg', (0.702, 0.196, 0.962, 0.305))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/27113.jpg', (0.078, 0.542, 0.922, 0.613))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/27113.jpg', (0.078, 0.613, 0.922, 0.683))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/21553.jpg', (0.069, 0.044, 0.146, 0.087))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/68164.jpg', (0.049, 0.276, 0.122, 0.317))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6750.jpg', (0.455, 0.17, 0.545, 0.247))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6750.jpg', (0.455, 0.17, 0.545, 0.247))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/8629.jpg', (0.842, 0.036, 0.976, 0.111))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/8629.jpg', (0.768, 0.193, 0.975, 0.261))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/8629.jpg', (0.007, 0.862, 0.993, 0.93))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63561.jpg', (0.588, 0.093, 0.913, 0.276))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9506.jpg', (0.01, 0.249, 0.991, 0.641))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9506.jpg', (0.019, 0.256, 0.991, 0.641))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9649.jpg', (0.024, 0.225, 0.976, 0.281))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9649.jpg', (0.787, 0.039, 0.904, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9649.jpg', (0.033, 0.35, 0.217, 0.454))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/49334.jpg', (0.33, 0.769, 0.67, 0.842))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6334.jpg', (0.039, 0.339, 0.481, 0.404))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6334.jpg', (0.903, 0.039, 1.0, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54728.jpg', (0.906, 0.507, 0.998, 0.573))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54728.jpg', (0.906, 0.684, 0.998, 0.75))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40802.jpg', (0.013, 0.282, 0.33, 0.518))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40802.jpg', (0.671, 0.176, 0.988, 0.412))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40802.jpg', (0.671, 0.743, 0.988, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40379.jpg', (0.58, 0.771, 0.735, 0.843))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40379.jpg', (0.825, 0.771, 0.961, 0.843))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40379.jpg', (0.407, 0.771, 0.58, 0.843))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40379.jpg', (0.302, 0.861, 0.472, 0.923))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/23345.jpg', (0.005, 0.165, 0.102, 0.22))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/58537.jpg', (0.024, 0.593, 0.5, 0.658))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/58537.jpg', (0.033, 0.158, 0.07, 0.179))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/23206.jpg', (0.03, 0.157, 0.971, 0.229))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/30120.jpg', (0.904, 0.629, 0.961, 0.661))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14034.jpg', (0.655, 0.109, 0.957, 0.175))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/33061.jpg', (0.631, 0.033, 0.767, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/37933.jpg', (0.825, 0.836, 0.961, 0.913))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/22061.jpg', (0.843, 0.135, 0.93, 0.184))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/22061.jpg', (0.687, 0.033, 0.793, 0.109))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54716.jpg', (0.786, 0.039, 0.903, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/61062.jpg', (0.913, 0.855, 0.961, 0.882))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/61062.jpg', (0.803, 0.766, 0.861, 0.81))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62522.jpg', (0.505, 0.115, 0.991, 0.356))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62522.jpg', (0.01, 0.361, 0.495, 0.602))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62522.jpg', (0.505, 0.607, 0.991, 0.848))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62522.jpg', (0.505, 0.115, 0.991, 0.356))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/20232.jpg', (0.141, 0.47, 0.86, 0.53))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24522.jpg', (0.443, 0.228, 0.528, 0.276))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54210.jpg', (0.024, 0.396, 0.317, 0.445))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19221.jpg', (0.808, 0.585, 0.956, 0.636))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19221.jpg', (0.808, 0.422, 0.956, 0.473))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/37760.jpg', (0.027, 0.418, 0.974, 0.475))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38736.jpg', (0.925, 0.535, 0.949, 0.554))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40787.jpg', (0.855, 0.107, 0.976, 0.175))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13361.jpg', (0.101, 0.882, 0.149, 0.91))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13361.jpg', (0.037, 0.529, 0.146, 0.591))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13361.jpg', (0.183, 0.331, 1.0, 0.367))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/20788.jpg', (0.049, 0.468, 0.17, 0.536))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/20788.jpg', (0.049, 0.304, 0.17, 0.372))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/20788.jpg', (0.049, 0.714, 0.17, 0.782))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/68556.jpg', (0.816, 0.758, 0.971, 0.845))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/16070.jpg', (0.034, 0.566, 0.966, 0.621))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/16070.jpg', (0.032, 0.109, 0.5, 0.164))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/16070.jpg', (0.034, 0.249, 0.966, 0.304))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/16070.jpg', (0.034, 0.407, 0.966, 0.462))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/16070.jpg', (0.034, 0.495, 0.131, 0.549))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/53839.jpg', (0.505, 0.115, 0.991, 0.356))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13860.jpg', (0.056, 0.644, 0.144, 0.693))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13860.jpg', (0.056, 0.644, 0.144, 0.693))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/33933.jpg', (0.902, 0.12, 0.961, 0.151))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/70122.jpg', (0.752, 0.459, 0.989, 0.569))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/53209.jpg', (0.285, 0.89, 0.489, 0.921))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/70339.jpg', (0.505, 0.607, 0.991, 0.848))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/18178.jpg', (0.865, 0.76, 0.981, 0.88))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/10180.jpg', (0.865, 0.038, 1.0, 0.103))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/10180.jpg', (0.028, 0.118, 0.972, 0.416))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24544.jpg', (0.767, 0.039, 0.883, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/60746.jpg', (0.864, 0.033, 1.0, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/48054.jpg', (0.389, 0.691, 0.583, 0.801))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43797.jpg', (0.83, 0.043, 0.871, 0.066))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43797.jpg', (0.819, 0.243, 0.854, 0.256))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43797.jpg', (0.767, 0.033, 0.883, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/57939.jpg', (0.65, 0.039, 0.767, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/57939.jpg', (0.333, 0.145, 0.667, 0.186))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/20269.jpg', (0.502, 0.392, 0.999, 0.671))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/61057.jpg', (0.618, 0.046, 0.705, 0.096))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/61057.jpg', (0.864, 0.544, 1.0, 0.62))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/71977.jpg', (0.018, 0.52, 0.095, 0.564))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/51771.jpg', (0.112, 0.656, 0.214, 0.714))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/39569.jpg', (0.037, 0.683, 0.134, 0.738))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6142.jpg', (0.474, 0.055, 0.552, 0.099))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/55581.jpg', (0.238, 0.129, 0.763, 0.418))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/55581.jpg', (0.238, 0.129, 0.763, 0.418))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62348.jpg', (0.904, 0.383, 0.966, 0.421))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/34947.jpg', (0.024, 0.399, 0.302, 0.514))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41107.jpg', (0.024, 0.205, 0.976, 0.333))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40997.jpg', (0.365, 0.482, 0.635, 0.551))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40997.jpg', (0.883, 0.039, 1.0, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/25279.jpg', (0.147, 0.263, 0.852, 0.659))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28082.jpg', (0.063, 0.425, 0.937, 0.492))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63893.jpg', (0.602, 0.407, 0.943, 0.473))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/56771.jpg', (0.013, 0.043, 0.093, 0.094))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/56771.jpg', (0.946, 0.631, 0.963, 0.647))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14730.jpg', (0.53, 0.882, 0.549, 0.893))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14730.jpg', (0.491, 0.882, 0.51, 0.893))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/55812.jpg', (0.874, 0.006, 0.991, 0.071))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/55812.jpg', (0.689, 0.809, 0.806, 0.875))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64448.jpg', (0.049, 0.264, 0.952, 0.31))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64448.jpg', (0.049, 0.166, 0.952, 0.213))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/64448.jpg', (0.049, 0.215, 0.952, 0.262))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52083.jpg', (0.907, 0.814, 0.952, 0.856))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24860.jpg', (0.024, 0.446, 0.976, 0.494))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/18746.jpg', (0.616, 0.879, 0.669, 0.901))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/70481.jpg', (0.039, 0.079, 0.194, 0.167))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9757.jpg', (0.01, 0.853, 0.495, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26148.jpg', (0.855, 0.792, 1.0, 0.86))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41738.jpg', (0.457, 0.886, 0.543, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41738.jpg', (0.225, 0.89, 0.292, 0.931))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/60921.jpg', (0.776, 0.058, 0.83, 0.091))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14450.jpg', (0.035, 0.726, 0.966, 0.801))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/35991.jpg', (0.439, 0.033, 0.56, 0.101))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/35991.jpg', (0.03, 0.77, 0.971, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19365.jpg', (0.844, 0.887, 0.881, 0.908))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19365.jpg', (0.042, 0.495, 0.958, 0.785))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28426.jpg', (0.167, 0.689, 0.333, 0.812))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28426.jpg', (0.667, 0.566, 0.833, 0.689))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/8533.jpg', (0.189, 0.275, 0.811, 0.334))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/38415.jpg', (0.756, 0.098, 1.0, 0.164))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/59323.jpg', (0.881, 0.033, 1.0, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26152.jpg', (0.855, 0.792, 1.0, 0.86))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/69326.jpg', (0.039, 0.776, 0.146, 0.836))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6194.jpg', (0.01, 0.159, 0.126, 0.184))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/12797.jpg', (0.037, 0.721, 0.172, 0.798))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/36855.jpg', (0.01, 0.677, 0.495, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/58298.jpg', (0.003, 0.291, 0.372, 0.499))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9153.jpg', (0.107, 0.49, 0.369, 0.65))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/39598.jpg', (0.547, 0.877, 0.664, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/39728.jpg', (0.798, 0.858, 0.998, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/34093.jpg', (0.666, 0.798, 0.831, 0.892))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/34093.jpg', (0.208, 0.727, 0.5, 0.781))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/324.jpg', (0.657, 0.052, 0.794, 0.095))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/324.jpg', (0.657, 0.353, 0.794, 0.396))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/11275.jpg', (0.073, 0.607, 0.488, 0.642))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/11275.jpg', (0.073, 0.444, 0.488, 0.593))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/11275.jpg', (0.513, 0.444, 0.927, 0.593))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/17082.jpg', (0.039, 0.178, 0.376, 0.228))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/17082.jpg', (0.394, 0.178, 0.689, 0.228))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/17082.jpg', (0.787, 0.039, 0.904, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/747.jpg', (0.874, 0.081, 0.986, 0.146))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19498.jpg', (0.333, 0.033, 0.667, 0.093))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/26175.jpg', (0.039, 0.126, 0.097, 0.181))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9748.jpg', (0.883, 0.826, 0.961, 0.87))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/7897.jpg', (0.03, 0.164, 0.088, 0.179))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/68109.jpg', (0.864, 0.033, 1.0, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/15754.jpg', (0.503, 0.655, 0.997, 0.873))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/69893.jpg', (0.25, 0.139, 0.498, 0.244))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13907.jpg', (0.357, 0.102, 0.643, 0.262))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13907.jpg', (0.357, 0.102, 0.643, 0.262))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/39250.jpg', (0.209, 0.115, 0.585, 0.158))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/34692.jpg', (0.539, 0.869, 0.681, 0.932))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/70203.jpg', (0.03, 0.208, 0.83, 0.241))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/70203.jpg', (0.531, 0.129, 0.976, 0.166))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/19508.jpg', (0.03, 0.098, 0.081, 0.127))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/50234.jpg', (0.552, 0.676, 0.679, 0.747))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/35288.jpg', (0.039, 0.21, 0.961, 0.3))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/11292.jpg', (0.399, 0.812, 0.601, 0.926))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/57898.jpg', (0.866, 0.022, 0.961, 0.076))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/21775.jpg', (0.039, 0.611, 0.156, 0.677))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/49599.jpg', (0.815, 0.739, 0.97, 0.826))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28150.jpg', (0.049, 0.453, 0.952, 0.548))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/18944.jpg', (0.928, 0.202, 0.976, 0.23))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24624.jpg', (0.013, 0.897, 0.158, 0.928))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/57897.jpg', (0.644, 0.459, 0.703, 0.492))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/44246.jpg', (0.039, 0.526, 0.117, 0.569))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/70172.jpg', (0.5, 0.297, 1.0, 0.383))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/27569.jpg', (0.007, 0.007, 0.173, 0.062))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/27569.jpg', (0.328, 0.104, 0.37, 0.128))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/27569.jpg', (0.305, 0.359, 0.346, 0.383))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/25544.jpg', (0.03, 0.382, 0.082, 0.412))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/5028.jpg', (0.037, 0.432, 0.963, 0.501))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/5028.jpg', (0.037, 0.291, 0.963, 0.344))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/32155.jpg', (0.421, 0.645, 0.58, 0.734))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14778.jpg', (0.024, 0.307, 0.365, 0.498))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/7197.jpg', (0.024, 0.04, 0.139, 0.103))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63772.jpg', (0.333, 0.807, 0.667, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14541.jpg', (0.424, 0.555, 0.952, 0.612))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14541.jpg', (0.454, 0.471, 0.952, 0.528))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41431.jpg', (0.19, 0.666, 0.263, 0.707))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41431.jpg', (0.534, 0.107, 0.578, 0.132))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41431.jpg', (0.447, 0.713, 0.637, 0.877))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14572.jpg', (0.049, 0.298, 0.952, 0.321))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14572.jpg', (0.039, 0.033, 0.336, 0.095))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/14572.jpg', (0.312, 0.095, 0.556, 0.156))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/40194.jpg', (0.577, 0.047, 0.659, 0.089))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/7891.jpg', (0.719, 0.041, 0.855, 0.09))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/41543.jpg', (0.883, 0.039, 1.0, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/60616.jpg', (0.65, 0.039, 0.767, 0.104))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/6053.jpg', (0.963, 0.184, 1.0, 0.205))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/58018.jpg', (0.161, 0.593, 0.278, 0.659))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/56255.jpg', (0.339, 0.594, 0.661, 0.659))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/15882.jpg', (0.515, 0.751, 0.971, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52882.jpg', (0.087, 0.125, 0.913, 0.213))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52882.jpg', (0.087, 0.567, 0.913, 0.654))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52882.jpg', (0.087, 0.39, 0.913, 0.477))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52882.jpg', (0.087, 0.214, 0.913, 0.301))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/52882.jpg', (0.087, 0.655, 0.913, 0.743))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/24175.jpg', (0.504, 0.114, 0.994, 0.482))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/67166.jpg', (0.891, 0.055, 0.963, 0.078))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/67166.jpg', (0.865, 0.783, 0.976, 0.846))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/67166.jpg', (0.865, 0.783, 0.976, 0.846))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54075.jpg', (0.843, 0.729, 0.916, 0.77))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/43558.jpg', (0.019, 0.501, 0.981, 0.774))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/9241.jpg', (0.817, 0.242, 0.865, 0.269))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/54477.jpg', (0.864, 0.033, 1.0, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/71814.jpg', (0.344, 0.082, 0.657, 0.259))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/63496.jpg', (0.039, 0.478, 0.156, 0.543))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/13483.jpg', (0.058, 0.179, 0.131, 0.22))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/28362.jpg', (0.202, 0.129, 0.798, 0.272))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/67508.jpg', (0.069, 0.51, 0.166, 0.565))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/69254.jpg', (0.379, 0.514, 0.471, 0.567))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/69254.jpg', (0.055, 0.764, 0.48, 0.934))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/46756.jpg', (0.728, 0.033, 0.864, 0.098))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62178.jpg', (0.682, 0.18, 0.945, 0.328))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62178.jpg', (0.055, 0.37, 0.318, 0.518))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62178.jpg', (0.682, 0.18, 0.945, 0.328))\n",
      "\n",
      "\t\t\t Skipping duplicate: ('/Users/preetamverma/Downloads/screenshots/62178.jpg', (0.369, 0.56, 0.631, 0.708))\n",
      " Loaded 16310 referring expressions from 5682 images\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import pandas as pd\n",
    "import os \n",
    "import ast \n",
    "import numpy as np \n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "\n",
    "UI_VOCAB = [\n",
    "    \"button\", \"icon\", \"text\", \"image\", \"input\", \"checkbox\", \"link\",\n",
    "    \"menu item\", \"banner\", \"avatar\", \"logo\", \"label\", \"switch\",\n",
    "    \"tab\", \"card\", \"popup\", \"dropdown\", \"textfield\", \"container\"\n",
    "]\n",
    "\n",
    "UI_TEXT_MAP = {\n",
    "    \"image\": [\"picture\", \"photo\", \"avatar\", \"logo\", \"icon\"],\n",
    "    \"button\": [\"button\", \"tap\", \"click\", \"submit\"],\n",
    "    \"text\": [\"text\", \"label\"],\n",
    "    \"input\": [\"input\", \"field\", \"search\", \"textbox\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class OSAtlasRefDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Converts OS-Atlas GUI grounding dataset to GroundingDINO format.\n",
    "    Each image may contain multiple referring expressions and bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path, image_root, tokenizer, \n",
    "                 split=\"train\", auto_add_tokens=False):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.auto_add_tokens = auto_add_tokens\n",
    "        self.image_root = image_root\n",
    "\n",
    "        # Load raw JSON\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        duplicate_dict = {}\n",
    "\n",
    "        # Flatten image + elements into one dataframe\n",
    "        records = []\n",
    "        for item in self.data:\n",
    "            img_path = os.path.join(image_root, item[\"img_filename\"])\n",
    "\n",
    "            # if img_path!='/Users/preetamverma/Downloads/screenshots/67892.jpg':\n",
    "            #     continue\n",
    "\n",
    "            for el in item[\"elements\"]:\n",
    "\n",
    "                key = (img_path,  tuple(el[\"bbox\"]))\n",
    "                if key in duplicate_dict:\n",
    "                    print (\"\\n\\t\\t\\t Skipping duplicate:\", key)\n",
    "                    continue\n",
    "                duplicate_dict[key]=True\n",
    "\n",
    "                records.append({\n",
    "                    \"img_path\": img_path,\n",
    "                    \"instruction\": el[\"instruction\"].strip(),\n",
    "                    \"bbox\": el[\"bbox\"]  # assumed xyxy (absolute or normalized)\n",
    "                })\n",
    "        self.df = pd.DataFrame(records)\n",
    "        print(f\" Loaded {len(self.df)} referring expressions from {len(self.data)} images\")\n",
    "\n",
    "        self.ui_classifier = UIElementClassifier(UI_VOCAB)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_xyxy(bbox, width, height):\n",
    "        \"\"\"Ensure bbox is normalized xyxy in [0,1]. Accept list/tuple of 4 numbers.\"\"\"\n",
    "        x0, y0, x1, y1 = bbox\n",
    "        # If values look already normalized keep them (heuristic: all <= 1.2)\n",
    "        if max(x0, y0, x1, y1) > 1.2:  # treat as pixel coords\n",
    "            x0 /= width\n",
    "            x1 /= width\n",
    "            y0 /= height\n",
    "            y1 /= height\n",
    "        # Clamp to [0,1]\n",
    "        x0 = min(max(x0, 0.0), 1.0)\n",
    "        y0 = min(max(y0, 0.0), 1.0)\n",
    "        x1 = min(max(x1, 0.0), 1.0)\n",
    "        y1 = min(max(y1, 0.0), 1.0)\n",
    "        # Fix inverted boxes if any\n",
    "        if x1 < x0: x0, x1 = x1, x0\n",
    "        if y1 < y0: y0, y1 = y1, y0\n",
    "        return [x0, y0, x1, y1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"img_path\"]\n",
    "\n",
    "        #print (f\"Processing {img_path}\")\n",
    "\n",
    "        text = row[\"instruction\"]\n",
    "        bbox = row[\"bbox\"]  # expected xyxy\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        W, H = image.size\n",
    "\n",
    "        # Normalize bbox to [0,1] xyxy\n",
    "        bbox = self._normalize_xyxy(bbox, W, H)\n",
    "\n",
    "        ui_label, score = self.ui_classifier.classify(text.lower())\n",
    "        phrases = [ui_label]\n",
    "        bbox_final = [bbox] * len(phrases)  # one box per phrase currently\n",
    "\n",
    "        positive_map = self.ui_classifier.build_positive_map(text, phrases)\n",
    "        if not torch.any(positive_map):\n",
    "            # Skip logic handled in collate; still return placeholder\n",
    "            pass\n",
    "\n",
    "        class_labels = [index for index in range(len(phrases))]\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text\": text,\n",
    "            \"bbox\": bbox_final,          # list of normalized xyxy\n",
    "            \"class_labels\": class_labels,\n",
    "            \"positive_map\": positive_map,\n",
    "            \"phrases\": phrases,\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = OSAtlasRefDataset(\n",
    "    json_path=\"/Users/preetamverma/Downloads/uibert_raw.json\",  \n",
    "    image_root=\"/Users/preetamverma/Downloads\",      \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c65ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from PIL import Image\n",
    "\n",
    "UNIFORM_SIZE = (512, 512)  # width, height\n",
    "\n",
    "\n",
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _xyxy_to_cxcywh_norm(xyxy: torch.Tensor):\n",
    "    x0, y0, x1, y1 = xyxy.unbind(-1)\n",
    "    cx = (x0 + x1) / 2.0\n",
    "    cy = (y0 + y1) / 2.0\n",
    "    w = (x1 - x0).clamp(min=1e-6)\n",
    "    h = (y1 - y0).clamp(min=1e-6)\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def _resize_all(images):\n",
    "    resized = []\n",
    "    for img in images:\n",
    "        if img.size != UNIFORM_SIZE:\n",
    "            resized.append(img.resize(UNIFORM_SIZE))\n",
    "        else:\n",
    "            resized.append(img)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    l1 = len(batch)\n",
    "    batch = [item for item in batch if torch.any(item[\"positive_map\"]) and item[\"text\"].strip()]\n",
    "\n",
    "    l2 = len(batch)\n",
    "\n",
    "    if l1 != l2:\n",
    "        print (f\"  Skipped {l1 - l2} samples with no positive mappings or empty text.\")\n",
    "        return None \n",
    "\n",
    "    if len(batch) == 0:\n",
    "        print (f\" Collated batch of 0 samples\")\n",
    "        return None\n",
    "\n",
    "    images  = [item[\"image\"] for item in batch]\n",
    "    #images  = _resize_all(images)  # enforce uniform size\n",
    "    texts   = [item[\"text\"] for item in batch]\n",
    "    phrases = [item[\"phrases\"] for item in batch]\n",
    "\n",
    "\n",
    "    encodings = processor(images=images, text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    max_seq_len = encodings[\"input_ids\"].shape[-1]\n",
    "\n",
    "    labels = []\n",
    "    for sample_idx, item in enumerate(batch):\n",
    "        class_labels = torch.as_tensor(item[\"class_labels\"], dtype=torch.long)\n",
    "        boxes_list = []\n",
    "        for b in item[\"bbox\"]:\n",
    "            b_tensor = torch.tensor(b, dtype=torch.float32)\n",
    "            boxes_list.append(b_tensor)\n",
    "        boxes = torch.stack(boxes_list, dim=0)\n",
    "        boxes_cxcywh = _xyxy_to_cxcywh_norm(boxes)\n",
    "\n",
    "        pos_map = item[\"positive_map\"].to(torch.bool)\n",
    "        orig_len = pos_map.shape[-1]\n",
    "        if orig_len < max_seq_len:\n",
    "            pad_cols = max_seq_len - orig_len\n",
    "            pos_map = torch.cat([pos_map, torch.zeros((pos_map.shape[0], pad_cols), dtype=torch.bool)], dim=-1)\n",
    "        elif orig_len > max_seq_len:\n",
    "            pos_map = pos_map[:, :max_seq_len]\n",
    "\n",
    "        # Assertions / validations\n",
    "        if boxes_cxcywh.ndim != 2 or boxes_cxcywh.shape[-1] != 4:\n",
    "            raise RuntimeError(f\"Invalid boxes shape {boxes_cxcywh.shape} for sample {sample_idx}\")\n",
    "        if class_labels.shape[0] != boxes_cxcywh.shape[0]:\n",
    "            raise RuntimeError(f\"Class labels length {class_labels.shape[0]} != boxes count {boxes_cxcywh.shape[0]} sample {sample_idx}\")\n",
    "        if pos_map.shape[0] != boxes_cxcywh.shape[0]:\n",
    "            raise RuntimeError(f\"Positive map rows {pos_map.shape[0]} != boxes count {boxes_cxcywh.shape[0]} sample {sample_idx}\")\n",
    "        if torch.any(torch.isnan(boxes_cxcywh)) or torch.any(torch.isinf(boxes_cxcywh)):\n",
    "            raise RuntimeError(\"NaN/Inf in boxes\")\n",
    "\n",
    "        labels.append({\n",
    "            \"class_labels\": class_labels.to(device),\n",
    "            \"boxes\": boxes_cxcywh.to(device),\n",
    "            \"positive_map\": pos_map.to(device)\n",
    "        })\n",
    "\n",
    "    print (f\" Collated batch of {len(images)} samples\")\n",
    "    return encodings, labels , images, tuple(map(tuple, phrases)),  tuple(map(tuple, boxes_cxcywh.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b9a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in encoder model: 172.24909 M\n",
      "  Skipped 1 samples with no positive mappings or empty text.\n",
      "  Skipped 1 samples with no positive mappings or empty text.\n",
      " Collated batch of 4 samples\n",
      "  Skipped 1 samples with no positive mappings or empty text.\n",
      " Collated batch of 4 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:350: UserWarning: In MPS autocast, but the target dtype is not supported. Disabling autocast.\n",
      "MPS Autocast only supports dtype of torch.bfloat16 and torch.float16 currently.\n",
      "  warnings.warn(error_message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, -1, 4, 256]' is invalid for input of size 34816",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m     95\u001b[39m amp_ctx = torch.autocast(\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m, enabled=use_autocast, dtype=torch.float32)\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m amp_ctx:\n\u001b[32m     97\u001b[39m \n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# print (\"=*=\"*60)\u001b[39;00m\n\u001b[32m     99\u001b[39m     \u001b[38;5;66;03m# print (f\"PHRASES: {phrases}\")\u001b[39;00m\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# print (f\"enc\", enc)\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# print (f\"labels: {labels}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     loss_1 = outputs.loss \n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# print (f\"STEP LOSS {loss_1.item()}\")\u001b[39;00m\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# print (\"=*=\"*60)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:2529\u001b[39m, in \u001b[36mGroundingDinoForObjectDetection.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict, labels)\u001b[39m\n\u001b[32m   2526\u001b[39m     attention_mask = torch.ones_like(input_ids)\n\u001b[32m   2528\u001b[39m \u001b[38;5;66;03m# First, sent images through Grounding DINO base model to obtain encoder + decoder outputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2529\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2531\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2536\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2537\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2539\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2541\u001b[39m idx = \u001b[32m5\u001b[39m + (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m) + (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m)\n\u001b[32m   2542\u001b[39m enc_text_hidden_state = outputs.encoder_last_hidden_state_text \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;28;01melse\u001b[39;00m outputs[idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:2191\u001b[39m, in \u001b[36mGroundingDinoModel.forward\u001b[39m\u001b[34m(self, pixel_values, input_ids, token_type_ids, attention_mask, pixel_mask, encoder_outputs, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   2188\u001b[39m \u001b[38;5;66;03m# Fourth, sent source_flatten + mask_flatten + lvl_pos_embed_flatten (backbone + proj layer output) through encoder\u001b[39;00m\n\u001b[32m   2189\u001b[39m \u001b[38;5;66;03m# Also provide spatial_shapes, level_start_index and valid_ratios\u001b[39;00m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2191\u001b[39m     encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mmask_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlvl_pos_embed_flatten\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2196\u001b[39m \u001b[43m        \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mtext_token_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m~\u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2204\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2205\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a GroundingDinoEncoderOutput when return_dict=True\u001b[39;00m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, GroundingDinoEncoderOutput):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:1580\u001b[39m, in \u001b[36mGroundingDinoEncoder.forward\u001b[39m\u001b[34m(self, vision_features, vision_attention_mask, vision_position_embedding, spatial_shapes, spatial_shapes_list, level_start_index, valid_ratios, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1577\u001b[39m     encoder_vision_states += (vision_features,)\n\u001b[32m   1578\u001b[39m     encoder_text_states += (text_features,)\n\u001b[32m-> \u001b[39m\u001b[32m1580\u001b[39m (vision_features, text_features), attentions = \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1581\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1582\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1583\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mspatial_shapes_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel_start_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_self_attention_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m   1596\u001b[39m     all_attn_fused_vision += (attentions[\u001b[32m0\u001b[39m],)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:1130\u001b[39m, in \u001b[36mGroundingDinoEncoderLayer.forward\u001b[39m\u001b[34m(self, vision_features, vision_position_embedding, spatial_shapes, spatial_shapes_list, level_start_index, key_padding_mask, reference_points, text_features, text_attention_mask, text_position_embedding, text_self_attention_masks, text_position_ids)\u001b[39m\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1112\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1113\u001b[39m     vision_features: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1124\u001b[39m     text_position_ids: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1125\u001b[39m ):\n\u001b[32m   1126\u001b[39m     text_position_embedding = \u001b[38;5;28mself\u001b[39m.get_text_position_embeddings(\n\u001b[32m   1127\u001b[39m         text_features, text_position_embedding, text_position_ids\n\u001b[32m   1128\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     (vision_features, vision_fused_attn), (text_features, text_fused_attn) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfusion_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask_vision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     (text_features, text_enhanced_attn) = \u001b[38;5;28mself\u001b[39m.text_enhancer_layer(\n\u001b[32m   1138\u001b[39m         hidden_states=text_features,\n\u001b[32m   1139\u001b[39m         attention_masks=~text_self_attention_masks,  \u001b[38;5;66;03m# note we use ~ for mask here\u001b[39;00m\n\u001b[32m   1140\u001b[39m         position_embeddings=(text_position_embedding \u001b[38;5;28;01mif\u001b[39;00m text_position_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   1141\u001b[39m     )\n\u001b[32m   1143\u001b[39m     (vision_features, vision_deformable_attn) = \u001b[38;5;28mself\u001b[39m.deformable_layer(\n\u001b[32m   1144\u001b[39m         hidden_states=vision_features,\n\u001b[32m   1145\u001b[39m         attention_mask=~key_padding_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1150\u001b[39m         level_start_index=level_start_index,\n\u001b[32m   1151\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:946\u001b[39m, in \u001b[36mGroundingDinoFusionLayer.forward\u001b[39m\u001b[34m(self, vision_features, text_features, attention_mask_vision, attention_mask_text)\u001b[39m\n\u001b[32m    944\u001b[39m vision_features = \u001b[38;5;28mself\u001b[39m.layer_norm_vision(vision_features)\n\u001b[32m    945\u001b[39m text_features = \u001b[38;5;28mself\u001b[39m.layer_norm_text(text_features)\n\u001b[32m--> \u001b[39m\u001b[32m946\u001b[39m (delta_v, vision_attn), (delta_t, text_attn) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvision_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask_vision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m vision_features = vision_features + \u001b[38;5;28mself\u001b[39m.drop_path(\u001b[38;5;28mself\u001b[39m.vision_param * delta_v)\n\u001b[32m    953\u001b[39m text_features = text_features + \u001b[38;5;28mself\u001b[39m.drop_path(\u001b[38;5;28mself\u001b[39m.text_param * delta_t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:781\u001b[39m, in \u001b[36mGroundingDinoBiMultiHeadAttention.forward\u001b[39m\u001b[34m(self, vision_features, text_features, vision_attention_mask, text_attention_mask)\u001b[39m\n\u001b[32m    778\u001b[39m vision_query_states = \u001b[38;5;28mself\u001b[39m._reshape(vision_query_states, tgt_len, batch_size)\n\u001b[32m    780\u001b[39m text_key_states = \u001b[38;5;28mself\u001b[39m.text_proj(text_features)\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m text_key_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_key_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m vision_value_states = \u001b[38;5;28mself\u001b[39m.values_vision_proj(vision_features)\n\u001b[32m    784\u001b[39m vision_value_states = \u001b[38;5;28mself\u001b[39m._reshape(vision_value_states, -\u001b[32m1\u001b[39m, batch_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/transformers/models/grounding_dino/modeling_grounding_dino.py:739\u001b[39m, in \u001b[36mGroundingDinoBiMultiHeadAttention._reshape\u001b[39m\u001b[34m(self, tensor, seq_len, batch_size)\u001b[39m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_reshape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: torch.Tensor, seq_len: \u001b[38;5;28mint\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[4, -1, 4, 256]' is invalid for input of size 34816"
     ]
    }
   ],
   "source": [
    "#### TRAINING LOOP #####\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "from utils import calculate_total_train_params, save_to_checkpoint\n",
    "import torch.nn as nn \n",
    "from typing import ClassVar\n",
    "from pydantic import BaseModel\n",
    "import torch.nn.functional as F\n",
    "import time \n",
    "import math\n",
    "\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    batch_size: ClassVar[int] = 4\n",
    "    steps: ClassVar[int] = 0\n",
    "    epochs: ClassVar[int] = 1\n",
    "    lr: ClassVar[float] = 5e-4\n",
    "    accumulation_steps: ClassVar[int] = 4\n",
    "    save_every : ClassVar[int] = 1000\n",
    "    checkpoint_path : ClassVar[str] = \"checkpoint.pth\"\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=TrainingConfig.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "##### Setup Training #####\n",
    "\n",
    "all_params = calculate_total_train_params(model)\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "\n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TrainingConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/TrainingConfig.accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 1\n",
    "stop = False \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "l_epoch = 0 \n",
    "l_loss = 0 \n",
    "l_epoch =0\n",
    "l_global_step = 0\n",
    "\n",
    "\n",
    "N_EPOCHS = TrainingConfig.epochs - l_epoch\n",
    "\n",
    "# print (f\"PREVIOUS LOSS {l_loss} AT GLOBAL STEP {l_global_step} AT EPOCH {l_epoch}\")\n",
    "\n",
    "# Optional: disable autocast if instability persists\n",
    "use_autocast = True\n",
    "duplicate_dict = {}\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1 \n",
    "\n",
    "        if batch is None:\n",
    "            continue\n",
    "\n",
    "        if global_step > 100: break\n",
    "\n",
    "        # print (f\"GLOBAL STEP {global_step}\")\n",
    "\n",
    "        enc , labels, images, phrases, bbox = batch\n",
    "\n",
    "        # key = (bbox, phrases)\n",
    "        # #print (f\"KEY: {key}\")\n",
    "        # if key in duplicate_dict:\n",
    "        #     #print (\"\\n\\t\\t\\t Skipping duplicate:\", key)\n",
    "        #     continue\n",
    "\n",
    "        # duplicate_dict[key]=True\n",
    "\n",
    "        enc = {k: v.to(device) if torch.is_tensor(v) else v for k, v in enc.items()}\n",
    "\n",
    "        # Debug targets before forward\n",
    "        for li, l in enumerate(labels):\n",
    "            if torch.any(torch.isnan(l[\"boxes\"])) or torch.any(torch.isinf(l[\"boxes\"])):\n",
    "                print(f\"[WARN] NaN/Inf in label boxes idx={li}\", l[\"boxes\"])\n",
    "            if (l[\"boxes\"] < 0).any() or (l[\"boxes\"] > 1).any():\n",
    "                print(f\"[WARN] Out-of-range boxes idx={li}\", l[\"boxes\"])    \n",
    "            if l[\"boxes\"].ndim != 2 or l[\"boxes\"].shape[-1] != 4:\n",
    "                print(f\"[WARN] Unexpected box shape {l['boxes'].shape}\")\n",
    "\n",
    "        amp_ctx = torch.autocast(\"mps\", enabled=use_autocast, dtype=torch.float32)\n",
    "        with amp_ctx:\n",
    "\n",
    "            # print (\"=*=\"*60)\n",
    "            # print (f\"PHRASES: {phrases}\")\n",
    "            # print (f\"enc\", enc)\n",
    "            # print (f\"labels: {labels}\")\n",
    "\n",
    "            outputs = model(**enc, labels=labels)\n",
    "            loss_1 = outputs.loss \n",
    "\n",
    "            # print (f\"STEP LOSS {loss_1.item()}\")\n",
    "            # print (\"=*=\"*60)\n",
    "\n",
    "            loss_dict = getattr(outputs, 'loss_dict', {})\n",
    "            loss =  loss_1 / TrainingConfig.accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=15.0)\n",
    "\n",
    "        if (step + 1) % TrainingConfig.accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * TrainingConfig.accumulation_steps\n",
    "\n",
    "        # estimate remaining time every 100 steps\n",
    "        if global_step % 50 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{TrainingConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*TrainingConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\\\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  )\n",
    "            \n",
    "\n",
    "    if (step + 1) % TrainingConfig.accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "    # del enc , labels, images, phrases\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Suppose two images:\n",
    "images = [\n",
    "    Image.open(\"/Users/preetamverma/Downloads/screenshots/67892.jpg\").convert(\"RGB\"),\n",
    "\n",
    "]\n",
    "\n",
    "texts = [[\"click on the button\"]]\n",
    "\n",
    "inputs = processor(\n",
    "    images=images,\n",
    "    text=texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99192a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    threshold=0.02,\n",
    "    target_sizes=[img.size[::-1] for img in images]\n",
    ")\n",
    "\n",
    "result = results[0]\n",
    "\n",
    "boxes = result[\"boxes\"].detach().cpu().numpy().tolist()  # detach first\n",
    "result_labels = result[\"labels\"]  # if labels are tensors\n",
    "scores = result[\"scores\"].detach().cpu().numpy().tolist()  # if scores are tensors\n",
    "\n",
    "\n",
    "top_k = 2\n",
    "\n",
    "indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "boxes = [boxes[i] for i in indices]\n",
    "result_labels = [result_labels[i] for i in indices]\n",
    "scores = [scores[i] for i in indices]\n",
    "\n",
    "\n",
    "# for i in indices:\n",
    "#     print(boxes[i], result_labels[i], scores[i])\n",
    "\n",
    "\n",
    "\n",
    "#output = {key:[val_list[0:2]]for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(images[0], boxes, result_labels, scores) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea58db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
