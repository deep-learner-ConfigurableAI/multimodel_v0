{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ed6278",
   "metadata": {},
   "source": [
    "# Token Similarity Analysis for Multimodal Model\n",
    "\n",
    "This notebook demonstrates how to use the cosine similarity debugging functionality to analyze whether global tokens in our multimodal model are learning useful representations. We'll visualize the similarities between different tokens and evaluate how the global context token relates to image features and query tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a929585",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our model and utility classes\n",
    "from decoder_model import ResnetGPT2Wrapper\n",
    "from encoder import CLIPEncoder\n",
    "from setup_model import  get_models\n",
    "from token_similarity_debug import analyze_token_similarities, visualize_image_with_similarities, compare_multi_image_similarities\n",
    "\n",
    "# Check for MPS (Apple Silicon) support\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac397e91",
   "metadata": {},
   "source": [
    "## 2. Setup Model with Similarity Debugging Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ea437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup encoder and decoder\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer = get_models() \n",
    "\n",
    "# try:\n",
    "#     checkpoint = torch.load('checkpoint.pth', map_location=device)\n",
    "#     decoder_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     print(\"Loaded model from checkpoint\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"No checkpoint found, using initialized model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f304c21",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions for Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf3551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image_paths(num_samples=5):\n",
    "    \"\"\"Get a random sample of image paths from the train2017 directory\"\"\"\n",
    "    image_dir = \"train2017\"\n",
    "    image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "    return random.sample(image_files, min(num_samples, len(image_files)))\n",
    "\n",
    "\n",
    "def extract_embeddings(model, image_path, caption):\n",
    "    \"\"\"Extract all embeddings from the model for analysis\"\"\"\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get image features from CLIP encoder\n",
    "    with torch.no_grad():\n",
    "        img_features = encoder_model(img_tensor)\n",
    "\n",
    "    # Tokenize caption\n",
    "    caption_tokens = tokenizer(caption, return_tensors=\"pt\").input_ids.to(device)\n",
    "    attention_mask = torch.ones_like(caption_tokens)\n",
    "    \n",
    "    # Store original query tokens and global context\n",
    "    original_query = model.query_tokens.data.clone()\n",
    "    original_global = model.img_context.data.clone()\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='mps', dtype=torch.float32):\n",
    "            # Run forward pass to get all updated embeddings\n",
    "            _, _ = model(img_features, caption_tokens, attention_mask=attention_mask, mode=\"inference\")\n",
    "        \n",
    "        # Get the most recent similarity data\n",
    "        similarities = model.similarity_logs[-1] if model.similarity_logs else None\n",
    "    \n",
    "    # Extract embeddings we want to analyze\n",
    "    embeddings = {\n",
    "        'original_query': original_query.squeeze().cpu(),\n",
    "        'original_global': original_global.squeeze().cpu(),\n",
    "        'img_features': img_features[0].cpu(),\n",
    "        'similarities': similarities\n",
    "    }\n",
    "    \n",
    "    return embeddings, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc997c2d",
   "metadata": {},
   "source": [
    "## 4. Analyze Single Image Token Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f879f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample image\n",
    "sample_images = get_sample_image_paths(1)\n",
    "sample_image_path = sample_images[0]\n",
    "\n",
    "# Create a simple caption\n",
    "sample_caption = \"a photo of\"\n",
    "\n",
    "# Analyze token similarities\n",
    "similarities = analyze_token_similarities(decoder_model, sample_image_path, sample_caption, encoder_model)\n",
    "\n",
    "# Visualize the image alongside the token similarities\n",
    "visualize_image_with_similarities(sample_image_path, similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2e289",
   "metadata": {},
   "source": [
    "## 5. Compare Similarities Across Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be51dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multiple sample images\n",
    "multi_sample_images = get_sample_image_paths(5)\n",
    "\n",
    "# Compare similarities across images\n",
    "similarities_list = compare_multi_image_similarities(decoder_model, multi_sample_images, encoder_model, save_dir=\"temp\")\n",
    "\n",
    "# Display the first similarity visualization\n",
    "from IPython.display import Image as IPImage\n",
    "display(IPImage(os.path.join(\"temp\", \"similarity_image_0.png\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8f109a",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis of Token Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63828e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistics from similarities\n",
    "global_query_means = [sim['global_to_query'].mean() for sim in similarities_list if sim is not None]\n",
    "global_query_maxes = [sim['global_to_query'].max() for sim in similarities_list if sim is not None]\n",
    "global_img_means = [sim['global_to_img'].mean() for sim in similarities_list if sim is not None]\n",
    "global_img_maxes = [sim['global_to_img'].max() for sim in similarities_list if sim is not None]\n",
    "\n",
    "# Create a bar chart comparing average similarities\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(global_query_means))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, global_query_means, width, label='Global-Query Mean')\n",
    "plt.bar(x + width/2, global_img_means, width, label='Global-Image Mean')\n",
    "\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Average Cosine Similarity')\n",
    "plt.title('Token Similarities Across Images')\n",
    "plt.xticks(x)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2abb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation plot\n",
    "from token_similarity_debug import plot_similarity_correlation\n",
    "plot_similarity_correlation(similarities_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997d524",
   "metadata": {},
   "source": [
    "## 7. Training Loop with Similarity Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c153dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_training_with_similarity_tracking(model, num_steps=10):\n",
    "    \"\"\"Simulate training steps and track token similarity changes\"\"\"\n",
    "    # Sample image to use consistently\n",
    "    image_path = get_sample_image_paths(1)[0]\n",
    "    caption = \"a photo of\"\n",
    "    \n",
    "    # Reset similarity logs\n",
    "    model.similarity_logs = []\n",
    "    \n",
    "    # Load and process image once\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get image features\n",
    "    with torch.no_grad():\n",
    "        img_features = encoder_model(img_tensor)\n",
    "    \n",
    "    # Tokenize caption\n",
    "    caption_tokens = tokenizer(caption, return_tensors=\"pt\").input_ids.to(device)\n",
    "    attention_mask = torch.ones_like(caption_tokens)\n",
    "    \n",
    "    # Create small noise for parameter updates (simulating training)\n",
    "    for step in range(num_steps):\n",
    "        # Add small noise to query tokens (simulating parameter updates)\n",
    "        noise_scale = 0.01\n",
    "        query_noise = torch.randn_like(model.query_tokens.data) * noise_scale\n",
    "        global_noise = torch.randn_like(model.img_context.data) * noise_scale\n",
    "        \n",
    "        # Apply noise\n",
    "        model.query_tokens.data += query_noise\n",
    "        model.img_context.data += global_noise\n",
    "        \n",
    "        # Compute similarities\n",
    "        with torch.no_grad():\n",
    "            model(img_features, caption_tokens, attention_mask=attention_mask, mode=\"inference\")\n",
    "    \n",
    "    # Visualize similarity changes over time\n",
    "    model.visualize_similarities(save_path=\"temp/similarity_evolution.png\")\n",
    "    \n",
    "    return model.similarity_logs\n",
    "\n",
    "# Simulate training and track similarities\n",
    "simulation_logs = simulate_training_with_similarity_tracking(decoder_model, num_steps=20)\n",
    "\n",
    "# Display the visualization\n",
    "from IPython.display import Image as IPImage\n",
    "display(IPImage(\"temp/similarity_evolution.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facccc0",
   "metadata": {},
   "source": [
    "## 8. Analyzing Global Token Information Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_token_information_content(model, image_paths):\n",
    "    \"\"\"Analyze how much unique information the global token contains\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        # Process the image\n",
    "        dummy_caption = \"a photo of\"\n",
    "        embeddings, image = extract_embeddings(model, img_path, dummy_caption)\n",
    "        \n",
    "        if 'similarities' not in embeddings or embeddings['similarities'] is None:\n",
    "            continue\n",
    "            \n",
    "        # Get similarity data\n",
    "        global_to_query = embeddings['similarities']['global_to_query']\n",
    "        global_to_img = embeddings['similarities']['global_to_img']\n",
    "        \n",
    "        # Calculate uniqueness metrics\n",
    "        max_query_sim = np.max(global_to_query)\n",
    "        mean_query_sim = np.mean(global_to_query)\n",
    "        std_query_sim = np.std(global_to_query)\n",
    "        \n",
    "        max_img_sim = np.max(global_to_img)\n",
    "        mean_img_sim = np.mean(global_to_img)\n",
    "        std_img_sim = np.std(global_to_img)\n",
    "        \n",
    "        # Higher std means more discriminative information\n",
    "        results.append({\n",
    "            'image_path': img_path,\n",
    "            'max_query_sim': max_query_sim,\n",
    "            'mean_query_sim': mean_query_sim,\n",
    "            'std_query_sim': std_query_sim,\n",
    "            'max_img_sim': max_img_sim,\n",
    "            'mean_img_sim': mean_img_sim,\n",
    "            'std_img_sim': std_img_sim,\n",
    "            'discriminative_power': std_query_sim / mean_query_sim  # Higher is better\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Get sample images\n",
    "analysis_images = get_sample_image_paths(10)\n",
    "\n",
    "# Analyze token information content\n",
    "info_results = analyze_token_information_content(decoder_model, analysis_images)\n",
    "\n",
    "# Display results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(info_results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca6c50",
   "metadata": {},
   "source": [
    "## 9. Visualizing Discriminative Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot discriminative power metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(info_results)), [r['discriminative_power'] for r in info_results], alpha=0.7)\n",
    "plt.xlabel('Image Index')\n",
    "plt.ylabel('Discriminative Power')\n",
    "plt.title('Global Token Discriminative Power Across Images')\n",
    "plt.axhline(y=np.mean([r['discriminative_power'] for r in info_results]), \n",
    "            color='r', linestyle='--', label='Average')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average discriminative power\n",
    "avg_power = np.mean([r['discriminative_power'] for r in info_results])\n",
    "print(f\"Average Discriminative Power: {avg_power:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if avg_power > 0.5:\n",
    "    print(\"The global token appears to be learning useful, discriminative information\")\n",
    "elif avg_power > 0.2:\n",
    "    print(\"The global token is learning some discriminative information, but could be improved\")\n",
    "else:\n",
    "    print(\"The global token does not appear to be learning sufficiently discriminative information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e83fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "tup = get_models() \n",
    "\n",
    "TrainingConfig, tokenizer = tup[0], tup[-2]\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "TrainingConfig.batch_size = 1 \n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23886c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import LogitsProcessorList, MinLengthLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
    "from transformers.generation.logits_process import (\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def visualize_caption(image_tensor, caption, gt_caption=None, figsize=(10, 8), similarity_data=None):\n",
    "    import torch, matplotlib.pyplot as plt\n",
    "\n",
    "    # Create figure with appropriate subplots\n",
    "    if similarity_data is not None:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    else:\n",
    "        fig, ax1 = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Convert tensor to numpy for visualization\n",
    "    if isinstance(image_tensor, torch.Tensor):\n",
    "        image_tensor = image_tensor.detach().cpu()\n",
    "        if image_tensor.dim() == 4:\n",
    "            image_tensor = image_tensor[0]\n",
    "        img = image_tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Normalize to [0,1] for display\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-5)\n",
    "    else:\n",
    "        img = image_tensor\n",
    "\n",
    "    # Plot image\n",
    "    ax1.imshow(img)\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    title = f\"Generated: {caption}\"\n",
    "    if gt_caption:\n",
    "        title += f\"\\nGround truth: {gt_caption}\"\n",
    "    ax1.set_title(title, fontsize=12)\n",
    "\n",
    "    # Plot similarity map if provided\n",
    "    if similarity_data is not None:\n",
    "        global_to_query = similarity_data.get(\"global_to_query\")\n",
    "        if global_to_query is not None:\n",
    "            im = ax2.imshow(global_to_query.reshape(1, -1), cmap=\"viridis\", aspect=\"auto\")\n",
    "            ax2.set_title(\"Global-Query Token Similarity\")\n",
    "            ax2.set_xlabel(\"Query Token Index\")\n",
    "            fig.colorbar(im, ax=ax2, label=\"Cosine Similarity\")\n",
    "\n",
    "            stats_text = f\"Global-Query Mean: {global_to_query.mean():.4f}\\n\"\n",
    "            stats_text += f\"Global-Query Max: {global_to_query.max():.4f}\\n\"\n",
    "            if \"global_to_img\" in similarity_data:\n",
    "                global_to_img = similarity_data[\"global_to_img\"]\n",
    "                stats_text += f\"Global-Image Mean: {global_to_img.mean():.4f}\"\n",
    "\n",
    "            ax2.text(0.05, 0.05, stats_text, transform=ax2.transAxes,\n",
    "                     fontsize=9, verticalalignment=\"bottom\",\n",
    "                     bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=True)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def generate_caption_3(\n",
    "    image_tensor,\n",
    "    encoder,\n",
    "    decoder,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_len=30,\n",
    "    use_image=True,\n",
    "    temperature=0.6,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.8,\n",
    "    min_length=5,\n",
    "    debug_similarity=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate caption for an image using the encoder-decoder model (sampling version)\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Enable similarity logging if requested\n",
    "    if debug_similarity:\n",
    "        decoder.debug_similarity = True\n",
    "        decoder.similarity_logs = []\n",
    "\n",
    "    # Add batch dimension to image if needed\n",
    "    if image_tensor.dim() == 3:\n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Encode image\n",
    "    if use_image:\n",
    "        x_embed = encoder(image_tensor)\n",
    "    else:\n",
    "        x_embed = torch.zeros((1, 49, decoder.embed_size), device=device)\n",
    "\n",
    "    # Token setup\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "    generated_ids = torch.tensor([[start_id]], device=device)\n",
    "\n",
    "    # Logit warpers and processors\n",
    "    warpers = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(temperature),\n",
    "        TopKLogitsWarper(top_k),\n",
    "        TopPLogitsWarper(top_p),\n",
    "    ])\n",
    "    processors = LogitsProcessorList([\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty),\n",
    "        MinLengthLogitsProcessor(min_length, end_token_id),\n",
    "    ])\n",
    "\n",
    "    # Autoregressive generation loop (no beam search)\n",
    "    for step in range(max_len):\n",
    "        attn_mask = torch.ones(1, generated_ids.size(1), dtype=torch.long, device=device)\n",
    "        with torch.no_grad():\n",
    "            logits, _ = decoder(x_embed, generated_ids, attn_mask, mode=\"inference\")\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "\n",
    "        # Apply processors & warpers\n",
    "        next_token_logits = processors(generated_ids.to(\"cpu\"), next_token_logits.to(\"cpu\"))\n",
    "        next_token_logits = warpers(generated_ids.to(\"cpu\"), next_token_logits.to(\"cpu\"))\n",
    "\n",
    "        # Convert to probabilities and sample\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append token\n",
    "        generated_ids = torch.cat([generated_ids.to(device), next_token.to(device)], dim=1)\n",
    "\n",
    "        # Stop if END token generated\n",
    "        if next_token.item() == end_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode caption\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    # Return with similarity logs if debugging\n",
    "    if debug_similarity and hasattr(decoder, \"similarity_logs\") and decoder.similarity_logs:\n",
    "        return caption, generated_ids, decoder.similarity_logs[-1]\n",
    "    else:\n",
    "        return caption, generated_ids, None \n",
    "\n",
    "def generate_caption_2(\n",
    "    image_tensor,\n",
    "    encoder,\n",
    "    decoder, \n",
    "    tokenizer, \n",
    "    device,\n",
    "    max_len=30, \n",
    "    use_image=True,\n",
    "    temperature=0.6,  # Reduced temperature for less randomness\n",
    "    top_k=40,         # More focused token selection\n",
    "    top_p=0.9,        # More conservative nucleus sampling\n",
    "    repetition_penalty=1.8,  # Stronger penalty to avoid repetition\n",
    "    min_length=5,\n",
    "    debug_similarity=False  # Enable similarity debugging\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate caption for an image using the encoder-decoder model\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Image tensor of shape (C, H, W)\n",
    "        encoder: Image encoder model\n",
    "        decoder: GPT decoder model wrapper\n",
    "        tokenizer: Tokenizer for text generation\n",
    "        device: Device to run inference on\n",
    "        max_len: Maximum length of generated caption\n",
    "        use_image: Whether to use image features or generate without image\n",
    "        temperature: Temperature for sampling (lower = less random)\n",
    "        top_k: Number of highest probability tokens to keep\n",
    "        top_p: Cumulative probability for nucleus sampling\n",
    "        repetition_penalty: Penalty for repeating tokens\n",
    "        min_length: Minimum length of caption before allowing END token\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated caption\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Configure similarity debugging if requested\n",
    "    if debug_similarity:\n",
    "        decoder.debug_similarity = True\n",
    "        decoder.similarity_logs = []  # Reset logs\n",
    "\n",
    "    # Add batch dimension to image if needed\n",
    "    if image_tensor.dim() == 3:\n",
    "        image_tensor = image_tensor.unsqueeze(0)\n",
    "    \n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Process image through encoder or create dummy embeddings\n",
    "    if use_image:\n",
    "        x_embed = encoder(image_tensor)\n",
    "    else:\n",
    "        x_embed = torch.zeros((1, 50, decoder.embed_size), device=device)\n",
    "    \n",
    "    # Get special token IDs\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "    \n",
    "    # Start with START token\n",
    "    generated_ids = torch.tensor([[start_id]], device=device)\n",
    "\n",
    "    # Setup logits processors for text generation\n",
    "    warpers_list = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(temperature),\n",
    "        TopKLogitsWarper(top_k),\n",
    "        TopPLogitsWarper(top_p),\n",
    "    ])\n",
    "    \n",
    "    processors_list = LogitsProcessorList([\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=repetition_penalty),\n",
    "        MinLengthLogitsProcessor(min_length, end_token_id)\n",
    "    ])\n",
    "\n",
    "    # Implementation of beam search for more coherent captions\n",
    "    beam_size = 3  # Number of beams to track\n",
    "    \n",
    "    # Initialize with start token\n",
    "    beam_scores = torch.zeros(1, device=device)\n",
    "    beam_seqs = torch.tensor([[start_id]], device=device)\n",
    "    beam_finished = [False]\n",
    "    \n",
    "    # Generate caption using beam search\n",
    "    for step in range(max_len):\n",
    "        # Expand all current beams\n",
    "        curr_batch_size = beam_seqs.size(0)\n",
    "        attn_mask = torch.ones(curr_batch_size, beam_seqs.size(1), dtype=torch.long, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Run inference\n",
    "            logits, _ = decoder(x_embed, beam_seqs, attn_mask, mode=\"inference\")\n",
    "\n",
    "            print (\"\\t logits Shape \\t\", logits.shape)\n",
    "            \n",
    "            # Get logits for next token prediction\n",
    "            next_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Apply logits processors for each beam\n",
    "            for i in range(curr_batch_size):\n",
    "                next_logits[i] = processors_list(beam_seqs[i:i+1], next_logits[i:i+1].squeeze(0))\n",
    "                next_logits[i] = warpers_list(beam_seqs[i:i+1], next_logits[i:i+1].squeeze(0))\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            vocab_size = next_logits.size(-1)\n",
    "            probs = F.log_softmax(next_logits, dim=-1)  # Using log probabilities for numerical stability\n",
    "            \n",
    "            # Add log probs to beam scores\n",
    "            next_scores = beam_scores.unsqueeze(1) + probs\n",
    "            \n",
    "            # Flatten for top-k selection\n",
    "            flat_next_scores = next_scores.view(-1)\n",
    "            \n",
    "            # Select top-k\n",
    "            best_scores, best_indices = flat_next_scores.topk(beam_size, largest=True, sorted=True)\n",
    "            \n",
    "            # Convert flat indices to beam indices and token indices\n",
    "            beam_indices = best_indices // vocab_size\n",
    "            token_indices = best_indices % vocab_size\n",
    "            \n",
    "            # Create new beam sequences\n",
    "            new_seqs = []\n",
    "            new_scores = []\n",
    "            new_finished = []\n",
    "            \n",
    "            for i, (beam_idx, token_idx) in enumerate(zip(beam_indices, token_indices)):\n",
    "                # Skip if this beam is already finished\n",
    "                if beam_finished[beam_idx] and i < len(beam_indices) - 1:\n",
    "                    continue\n",
    "                    \n",
    "                new_seq = torch.cat([beam_seqs[beam_idx], token_indices[i:i+1].unsqueeze(0)], dim=1)\n",
    "                new_seqs.append(new_seq)\n",
    "                new_scores.append(best_scores[i])\n",
    "                \n",
    "                # Check if sequence is finished\n",
    "                is_finished = token_idx.item() == end_token_id or step == max_len - 1\n",
    "                new_finished.append(is_finished)\n",
    "                \n",
    "                # If we have enough beams, stop\n",
    "                if len(new_seqs) == beam_size:\n",
    "                    break\n",
    "            \n",
    "            # Update beam state\n",
    "            beam_seqs = torch.cat(new_seqs, dim=0)\n",
    "            beam_scores = torch.tensor(new_scores, device=device)\n",
    "            beam_finished = new_finished\n",
    "            \n",
    "            # If all beams are finished, stop\n",
    "            if all(beam_finished):\n",
    "                break\n",
    "    \n",
    "    # Select best beam\n",
    "    best_beam_idx = beam_scores.argmax().item()\n",
    "    generated_ids = beam_seqs[best_beam_idx:best_beam_idx+1]\n",
    "\n",
    "    # Decode the generated tokens\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    # Return caption along with similarity data if debugging was enabled\n",
    "    if debug_similarity and hasattr(decoder, 'similarity_logs') and decoder.similarity_logs:\n",
    "        return caption, decoder.similarity_logs[-1]\n",
    "    else:\n",
    "        return caption\n",
    "\n",
    "\n",
    "\n",
    "def batch_generate_captions(image_batch, encoder, decoder, tokenizer, device, **kwargs):\n",
    "    \"\"\"\n",
    "    Generate captions for a batch of images\n",
    "    \n",
    "    Args:\n",
    "        image_batch: Batch of image tensors (B, C, H, W)\n",
    "        encoder: Image encoder\n",
    "        decoder: Text decoder\n",
    "        tokenizer: Tokenizer\n",
    "        device: Device to run on\n",
    "        **kwargs: Additional arguments for generate_caption\n",
    "        \n",
    "    Returns:\n",
    "        list: List of generated captions and similarity data if debug_similarity is True\n",
    "    \"\"\"\n",
    "    debug_similarity = kwargs.get('debug_similarity', False)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(image_batch.size(0)):\n",
    "        result = generate_caption_2(\n",
    "            image_batch[i], \n",
    "            encoder, \n",
    "            decoder, \n",
    "            tokenizer, \n",
    "            device, \n",
    "            **kwargs\n",
    "        )\n",
    "        results.append(result)\n",
    "    \n",
    "    # If debugging is enabled, results contain (caption, similarity_data) tuples\n",
    "    if debug_similarity:\n",
    "        captions = [r[0] for r in results]\n",
    "        similarity_data = [r[1] for r in results]\n",
    "        return captions, similarity_data\n",
    "    else:\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45202cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from generate import generate_caption, visualize_caption\n",
    "import torch \n",
    "from setup_model import get_models\n",
    "\n",
    "# encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer, extras_dict = get_models() \n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "decoder_model.similarity_logs = []\n",
    "decoder_model.all_cross_attn = []   ##This will store all cross attention map \n",
    "decoder_model.debug_similarity = True \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Pick a random sample from the validation set\n",
    "import random\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True) \n",
    "    print(\"Ground Truth:\\t\", gt_caption)\n",
    "\n",
    "    # Generate a caption\n",
    "    caption, generated_ids, similarity_logs = generate_caption_3(\n",
    "        image_tensor,\n",
    "        encoder_model,\n",
    "        decoder_model,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        temperature=0.7,  # Lower for less randomness\n",
    "        repetition_penalty=1.5,  # Higher to avoid repetition,\n",
    "        debug_similarity=True,\n",
    "        use_image=True\n",
    "    )\n",
    "    # Visualize the result\n",
    "    visualize_caption(image_tensor, caption, gt_caption)\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95be8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[idx, item, tokenizer.decode(item)] for idx, item in enumerate(generated_ids[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be2269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assume Q_attn, D_attn, decoder_model, tokenizer, caption_tensor, and image_tensor are defined\n",
    "\n",
    "Q_attn = decoder_model.last_cross_attn_weights_2[0].mean(0)          # (num_queries+1, num_patches)\n",
    "h_patches = w_patches = int(Q_attn.shape[-1] ** 0.5)\n",
    "\n",
    "\n",
    "\n",
    "cleaned_attn = []\n",
    "for attn in decoder_model.all_cross_attn:\n",
    "    # attn shape: [batch, heads, seq_len, num_queries]\n",
    "    # Take the last timestep (latest generated token)\n",
    "    last_step = attn[:, :, -1, :]    # shape [batch, heads, num_queries]\n",
    "    mean_last = last_step.mean(1)    # average over heads → [batch, num_queries]\n",
    "    cleaned_attn.append(mean_last.squeeze(0))\n",
    "\n",
    "D_attn = torch.stack(cleaned_attn, dim=0)  # (seq_len, num_queries)\n",
    "\n",
    "\n",
    "print (\"D_attn shape\", D_attn.shape)\n",
    "\n",
    "# --- Prepare image ---\n",
    "if isinstance(image_tensor, torch.Tensor):\n",
    "    image_tensor = image_tensor.detach().cpu()\n",
    "    if image_tensor.dim() == 4:\n",
    "        image_tensor = image_tensor[0]\n",
    "    img = image_tensor.permute(1, 2, 0).numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min() + 1e-5)\n",
    "else:\n",
    "    img = image_tensor\n",
    "\n",
    "caption_ids = caption_tensor[0].tolist() if caption_tensor.dim() > 1 else caption_tensor.tolist()\n",
    "caption_tokens = tokenizer.convert_ids_to_tokens(caption_ids)\n",
    "\n",
    "# Find best matching token (partial match for robustness)\n",
    "target = \"tie\"\n",
    "matches = [i for i, tok in enumerate(caption_tokens) if target in tok.lower()]\n",
    "\n",
    "if len(matches) > 0:\n",
    "    t_idx = matches[0]\n",
    "    print(f\"Matched token '{caption_tokens[t_idx]}' at position {t_idx}\")\n",
    "else:\n",
    "    print(f\"⚠️ Token '{target}' not found in caption! Using last token.\")\n",
    "    t_idx = len(caption_tokens) - 1\n",
    "\n",
    "\n",
    "print (\"Before\", t_idx)\n",
    "t_idx = 8\n",
    "\n",
    "\n",
    "# --- Combine attentions ---\n",
    "word_to_patch = D_attn[t_idx] @ Q_attn \n",
    "heatmap = word_to_patch.reshape(h_patches, w_patches)\n",
    "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-5)\n",
    "heatmap = heatmap.to(torch.float32).cpu().numpy()\n",
    "\n",
    "# --- Plot in separate figure ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# 1️⃣ Original image\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# 2️⃣ Transparent heatmap (no overlay)\n",
    "im = axes[1].imshow(heatmap, cmap='jet', alpha=0.7)\n",
    "axes[1].set_title(f\"Cross Attention for 'person'\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Add colorbar for clarity\n",
    "fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04, label=\"Attention Strength\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53960b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetlite import DataLoaderLite \n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder_path, tokenizer, dummy_caption=\"A photo.\", max_images=None):\n",
    "    \"\"\"\n",
    "    Loads all images from a folder and wraps them into DataLoaderLite.\n",
    "    If captions are not available, uses a dummy caption.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    supported_ext = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "    for i, file_name in enumerate(os.listdir(folder_path)):\n",
    "        if max_images and i >= max_images:\n",
    "            break\n",
    "        if os.path.splitext(file_name)[1].lower() not in supported_ext:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            # You can replace dummy_caption with something dynamic if needed\n",
    "            image_list.append((img, [dummy_caption]))\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {file_name}: {e}\")\n",
    "\n",
    "    dataset = DataLoaderLite(image_list, tokenizer=tokenizer)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad493444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from setup_model import get_models\n",
    "\n",
    "# encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer, extras_dict = get_models() \n",
    "\n",
    "print (f\" ====== TrainingConfig\", TrainingConfig)\n",
    "\n",
    "\n",
    "# create dataloader from a local folder\n",
    "folder_path = \"/Users/preetamverma/Desktop/image_cap_model_test_images\"\n",
    "test_loader = load_images_from_folder(folder_path, tokenizer)\n",
    "\n",
    "# Iterate over the test loader\n",
    "for img_tensor, input_ids, attn_mask in test_loader:\n",
    "    print(\"Image shape:\", img_tensor.shape)\n",
    "    print(\"Token IDs:\", input_ids)\n",
    "    print(\"Attention mask:\", attn_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db311c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
