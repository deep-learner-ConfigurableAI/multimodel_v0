{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:26:43.835029Z",
     "start_time": "2025-09-30T03:26:36.736736Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in  model:\n",
      "218376192\n",
      "loading annotations into memory...\n",
      "Done (t=0.30s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Training details vocab size 50259 batch size 4 image size 224x224 total steps 160000 epochs 10Max loss 10.824944914361643Perplexity 50259.00000000004\n",
      "Total Training Records = 16000\n"
     ]
    }
   ],
   "source": [
    "#### FINAL VERSION ####\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer, extras_dict = get_models() \n",
    "\n",
    "l_global_step = 0, l_epoch = 0 , l_loss = 0 \n",
    "\n",
    "if extras_dict:\n",
    "    l_global_step = extras_dict[\"global_step\"] \n",
    "    l_epoch       = extras_dict[\"epoch\"] \n",
    "    l_loss        = extras_dict[\"loss\"]\n",
    "\n",
    "# torch.mps.empty_cache()\n",
    "# import gc; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "formatted_str+= f\"\\nTotal Training Records = {len(train_dataloader)}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:27:49.087304Z",
     "start_time": "2025-09-30T03:27:28.355910Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:138: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:138: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/ww/qr_kh7fj37j544l07ttgdbfm0000gn/T/ipykernel_7296/1731898364.py:138: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
      "/var/folders/ww/qr_kh7fj37j544l07ttgdbfm0000gn/T/ipykernel_7296/1731898364.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Users/preetamverma/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/amp/grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters in encoder model: 218.376192 M\n",
      "Epoch 1: train_loss=0.0618, val_loss=7.8200\n",
      " SAVING INTO CHECKPOINT at global step 100 and loss 1.9676135778427124\n",
      "epoch 1/10 step 99/16000 Loss: 7.8705 | Elapsed: 0.57 min | ETA: 906.37 min | Total est: 906.93 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 2618.76\n",
      "Epoch 1: train_loss=0.1092, val_loss=6.9527\n",
      " SAVING INTO CHECKPOINT at global step 200 and loss 2.0078787803649902\n",
      "epoch 1/10 step 199/16000 Loss: 8.0315 | Elapsed: 1.12 min | ETA: 892.49 min | Total est: 893.60 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 3076.40\n",
      "Epoch 1: train_loss=0.1541, val_loss=6.6554\n",
      " SAVING INTO CHECKPOINT at global step 300 and loss 1.9099576473236084\n",
      "epoch 1/10 step 299/16000 Loss: 7.6398 | Elapsed: 1.68 min | ETA: 892.99 min | Total est: 894.67 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 2079.39\n",
      "Epoch 1: train_loss=0.1970, val_loss=6.5175\n",
      " SAVING INTO CHECKPOINT at global step 400 and loss 1.7997101545333862\n",
      "epoch 1/10 step 399/16000 Loss: 7.1988 | Elapsed: 2.38 min | ETA: 948.15 min | Total est: 950.53 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 1337.88\n",
      "Epoch 1: train_loss=0.2388, val_loss=6.3023\n",
      " SAVING INTO CHECKPOINT at global step 500 and loss 1.8637527227401733\n",
      "epoch 1/10 step 499/16000 Loss: 7.4550 | Elapsed: 3.24 min | ETA: 1033.77 min | Total est: 1037.02 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 1728.50\n",
      "Epoch 1: train_loss=0.2797, val_loss=6.3003\n",
      " SAVING INTO CHECKPOINT at global step 600 and loss 1.5344839096069336\n",
      "epoch 1/10 step 599/16000 Loss: 6.1379 | Elapsed: 3.93 min | ETA: 1044.99 min | Total est: 1048.92 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 463.10\n",
      "Epoch 1: train_loss=0.3193, val_loss=6.0781\n",
      " SAVING INTO CHECKPOINT at global step 700 and loss 1.503267765045166\n",
      "epoch 1/10 step 699/16000 Loss: 6.0131 | Elapsed: 4.51 min | ETA: 1025.73 min | Total est: 1030.23 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 408.74\n",
      "Epoch 1: train_loss=0.3582, val_loss=6.0149\n",
      " SAVING INTO CHECKPOINT at global step 800 and loss 1.4425979852676392\n",
      "epoch 1/10 step 799/16000 Loss: 5.7704 | Elapsed: 5.06 min | ETA: 1007.04 min | Total est: 1012.10 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 320.66\n",
      "Epoch 1: train_loss=0.3969, val_loss=5.7981\n",
      " SAVING INTO CHECKPOINT at global step 900 and loss 1.4783025979995728\n",
      "epoch 1/10 step 899/16000 Loss: 5.9132 | Elapsed: 5.60 min | ETA: 990.47 min | Total est: 996.07 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 369.89\n",
      "Epoch 1: train_loss=0.4345, val_loss=5.6809\n",
      " SAVING INTO CHECKPOINT at global step 1000 and loss 1.4866727590560913\n",
      "epoch 1/10 step 999/16000 Loss: 5.9467 | Elapsed: 6.16 min | ETA: 980.20 min | Total est: 986.37 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 382.49\n",
      "Epoch 1: train_loss=0.4714, val_loss=5.6524\n",
      " SAVING INTO CHECKPOINT at global step 1100 and loss 1.3164236545562744\n",
      "epoch 1/10 step 1099/16000 Loss: 5.2657 | Elapsed: 6.72 min | ETA: 970.91 min | Total est: 977.63 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 193.58\n",
      "Epoch 1: train_loss=0.5077, val_loss=5.5878\n",
      " SAVING INTO CHECKPOINT at global step 1200 and loss 1.384671688079834\n",
      "epoch 1/10 step 1199/16000 Loss: 5.5387 | Elapsed: 7.27 min | ETA: 961.94 min | Total est: 969.21 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 254.34\n",
      "Epoch 1: train_loss=0.5436, val_loss=5.5114\n",
      " SAVING INTO CHECKPOINT at global step 1300 and loss 1.482271671295166\n",
      "epoch 1/10 step 1299/16000 Loss: 5.9291 | Elapsed: 7.81 min | ETA: 953.58 min | Total est: 961.39 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 375.81\n",
      "Epoch 1: train_loss=0.5795, val_loss=5.5748\n",
      " SAVING INTO CHECKPOINT at global step 1400 and loss 1.481015682220459\n",
      "epoch 1/10 step 1399/16000 Loss: 5.9241 | Elapsed: 8.36 min | ETA: 946.98 min | Total est: 955.34 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 373.93\n",
      "Epoch 1: train_loss=0.6152, val_loss=5.4250\n",
      " SAVING INTO CHECKPOINT at global step 1500 and loss 1.47216796875\n",
      "epoch 1/10 step 1499/16000 Loss: 5.8887 | Elapsed: 8.91 min | ETA: 941.25 min | Total est: 950.16 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 360.93\n",
      "Epoch 1: train_loss=0.6505, val_loss=5.4394\n",
      " SAVING INTO CHECKPOINT at global step 1600 and loss 1.3283789157867432\n",
      "epoch 1/10 step 1599/16000 Loss: 5.3135 | Elapsed: 9.47 min | ETA: 937.56 min | Total est: 947.03 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 203.06\n",
      "Epoch 1: train_loss=0.6860, val_loss=5.5314\n",
      " SAVING INTO CHECKPOINT at global step 1700 and loss 1.4821370840072632\n",
      "epoch 1/10 step 1699/16000 Loss: 5.9285 | Elapsed: 10.01 min | ETA: 932.37 min | Total est: 942.38 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 375.61\n",
      "Epoch 1: train_loss=0.7207, val_loss=5.3318\n",
      " SAVING INTO CHECKPOINT at global step 1800 and loss 1.4406187534332275\n",
      "epoch 1/10 step 1799/16000 Loss: 5.7625 | Elapsed: 10.57 min | ETA: 928.59 min | Total est: 939.16 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 318.13\n",
      "Epoch 1: train_loss=0.7560, val_loss=5.4023\n",
      " SAVING INTO CHECKPOINT at global step 1900 and loss 1.382859706878662\n",
      "epoch 1/10 step 1899/16000 Loss: 5.5314 | Elapsed: 11.12 min | ETA: 924.97 min | Total est: 936.09 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 252.51\n",
      "Epoch 1: train_loss=0.7909, val_loss=5.4766\n",
      " SAVING INTO CHECKPOINT at global step 2000 and loss 1.4785492420196533\n",
      "epoch 1/10 step 1999/16000 Loss: 5.9142 | Elapsed: 11.67 min | ETA: 921.72 min | Total est: 933.39 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 370.26\n",
      "Epoch 1: train_loss=0.8255, val_loss=5.2887\n",
      " SAVING INTO CHECKPOINT at global step 2100 and loss 1.4503765106201172\n",
      "epoch 1/10 step 2099/16000 Loss: 5.8015 | Elapsed: 12.22 min | ETA: 918.60 min | Total est: 930.81 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 330.80\n",
      "Epoch 1: train_loss=0.8598, val_loss=5.3148\n",
      " SAVING INTO CHECKPOINT at global step 2200 and loss 1.35546875\n",
      "epoch 1/10 step 2199/16000 Loss: 5.4219 | Elapsed: 12.77 min | ETA: 915.83 min | Total est: 928.60 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 226.30\n",
      "Epoch 1: train_loss=0.8932, val_loss=5.3126\n",
      " SAVING INTO CHECKPOINT at global step 2300 and loss 1.3881480693817139\n",
      "epoch 1/10 step 2299/16000 Loss: 5.5526 | Elapsed: 13.32 min | ETA: 913.07 min | Total est: 926.39 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 257.91\n",
      "Epoch 1: train_loss=0.9273, val_loss=5.2772\n",
      " SAVING INTO CHECKPOINT at global step 2400 and loss 1.4099355936050415\n",
      "epoch 1/10 step 2399/16000 Loss: 5.6397 | Elapsed: 13.87 min | ETA: 910.81 min | Total est: 924.68 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 281.39\n",
      "Epoch 1: train_loss=0.9610, val_loss=5.2807\n",
      " SAVING INTO CHECKPOINT at global step 2500 and loss 1.4590137004852295\n",
      "epoch 1/10 step 2499/16000 Loss: 5.8361 | Elapsed: 14.42 min | ETA: 908.20 min | Total est: 922.61 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 342.43\n",
      "Epoch 1: train_loss=0.9945, val_loss=5.2136\n",
      " SAVING INTO CHECKPOINT at global step 2600 and loss 1.3629026412963867\n",
      "epoch 1/10 step 2599/16000 Loss: 5.4516 | Elapsed: 15.00 min | ETA: 908.11 min | Total est: 923.11 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 233.13\n",
      "Epoch 1: train_loss=1.0278, val_loss=5.2626\n",
      " SAVING INTO CHECKPOINT at global step 2700 and loss 1.342075228691101\n",
      "epoch 1/10 step 2699/16000 Loss: 5.3683 | Elapsed: 15.58 min | ETA: 907.69 min | Total est: 923.27 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 214.50\n",
      "Epoch 1: train_loss=1.0601, val_loss=5.1566\n",
      " SAVING INTO CHECKPOINT at global step 2800 and loss 1.3461564779281616\n",
      "epoch 1/10 step 2799/16000 Loss: 5.3846 | Elapsed: 16.15 min | ETA: 906.92 min | Total est: 923.07 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 218.03\n",
      "Epoch 1: train_loss=1.0930, val_loss=5.1574\n",
      " SAVING INTO CHECKPOINT at global step 2900 and loss 1.297635555267334\n",
      "epoch 1/10 step 2899/16000 Loss: 5.1905 | Elapsed: 16.74 min | ETA: 906.93 min | Total est: 923.67 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 179.57\n",
      "Epoch 1: train_loss=1.1255, val_loss=5.0928\n",
      " SAVING INTO CHECKPOINT at global step 3000 and loss 1.2147216796875\n",
      "epoch 1/10 step 2999/16000 Loss: 4.8589 | Elapsed: 17.33 min | ETA: 906.83 min | Total est: 924.16 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 128.88\n",
      "Epoch 1: train_loss=1.1583, val_loss=5.1792\n",
      " SAVING INTO CHECKPOINT at global step 3100 and loss 1.1387988328933716\n",
      "epoch 1/10 step 3099/16000 Loss: 4.5552 | Elapsed: 17.92 min | ETA: 906.84 min | Total est: 924.76 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 95.13\n",
      "Epoch 1: train_loss=1.1907, val_loss=5.1381\n",
      " SAVING INTO CHECKPOINT at global step 3200 and loss 1.1370340585708618\n",
      "epoch 1/10 step 3199/16000 Loss: 4.5481 | Elapsed: 18.50 min | ETA: 906.39 min | Total est: 924.88 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 94.46\n",
      "Epoch 1: train_loss=1.2228, val_loss=5.0217\n",
      " SAVING INTO CHECKPOINT at global step 3300 and loss 1.232239842414856\n",
      "epoch 1/10 step 3299/16000 Loss: 4.9290 | Elapsed: 19.08 min | ETA: 905.96 min | Total est: 925.04 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 138.24\n",
      "Epoch 1: train_loss=1.2554, val_loss=4.9905\n",
      " SAVING INTO CHECKPOINT at global step 3400 and loss 1.2788351774215698\n",
      "epoch 1/10 step 3399/16000 Loss: 5.1153 | Elapsed: 19.65 min | ETA: 905.28 min | Total est: 924.93 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 166.56\n",
      "Epoch 1: train_loss=1.2879, val_loss=5.0383\n",
      " SAVING INTO CHECKPOINT at global step 3500 and loss 1.2364637851715088\n",
      "epoch 1/10 step 3499/16000 Loss: 4.9459 | Elapsed: 20.24 min | ETA: 905.04 min | Total est: 925.28 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 140.59\n",
      "Epoch 1: train_loss=1.3195, val_loss=5.0510\n",
      " SAVING INTO CHECKPOINT at global step 3600 and loss 1.0860769748687744\n",
      "epoch 1/10 step 3599/16000 Loss: 4.3443 | Elapsed: 20.82 min | ETA: 904.39 min | Total est: 925.21 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 77.04\n",
      "Epoch 1: train_loss=1.3519, val_loss=4.9755\n",
      " SAVING INTO CHECKPOINT at global step 3700 and loss 1.3208752870559692\n",
      "epoch 1/10 step 3699/16000 Loss: 5.2835 | Elapsed: 21.39 min | ETA: 903.75 min | Total est: 925.15 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 197.06\n",
      "Epoch 1: train_loss=1.3844, val_loss=4.8705\n",
      " SAVING INTO CHECKPOINT at global step 3800 and loss 1.353857398033142\n",
      "epoch 1/10 step 3799/16000 Loss: 5.4154 | Elapsed: 21.97 min | ETA: 903.20 min | Total est: 925.17 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 224.85\n",
      "Epoch 1: train_loss=1.4168, val_loss=4.8697\n",
      " SAVING INTO CHECKPOINT at global step 3900 and loss 1.040987491607666\n",
      "epoch 1/10 step 3899/16000 Loss: 4.1639 | Elapsed: 22.55 min | ETA: 902.55 min | Total est: 925.09 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 64.33\n",
      "Epoch 1: train_loss=1.4482, val_loss=4.8125\n",
      " SAVING INTO CHECKPOINT at global step 4000 and loss 1.1405253410339355\n",
      "epoch 1/10 step 3999/16000 Loss: 4.5621 | Elapsed: 23.14 min | ETA: 902.62 min | Total est: 925.76 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 95.78\n",
      "Epoch 1: train_loss=1.4795, val_loss=4.8420\n",
      " SAVING INTO CHECKPOINT at global step 4100 and loss 1.2788347005844116\n",
      "epoch 1/10 step 4099/16000 Loss: 5.1153 | Elapsed: 23.72 min | ETA: 902.04 min | Total est: 925.76 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 166.56\n",
      "Epoch 1: train_loss=1.5108, val_loss=4.7967\n",
      " SAVING INTO CHECKPOINT at global step 4200 and loss 1.2634943723678589\n",
      "epoch 1/10 step 4199/16000 Loss: 5.0540 | Elapsed: 24.30 min | ETA: 901.30 min | Total est: 925.59 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 156.64\n",
      "Epoch 1: train_loss=1.5419, val_loss=4.7899\n",
      " SAVING INTO CHECKPOINT at global step 4300 and loss 1.2491105794906616\n",
      "epoch 1/10 step 4299/16000 Loss: 4.9964 | Elapsed: 24.86 min | ETA: 900.02 min | Total est: 924.87 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 147.89\n",
      "Epoch 1: train_loss=1.5728, val_loss=4.7428\n",
      " SAVING INTO CHECKPOINT at global step 4400 and loss 1.4559326171875\n",
      "epoch 1/10 step 4399/16000 Loss: 5.8237 | Elapsed: 25.43 min | ETA: 899.26 min | Total est: 924.69 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 338.23\n",
      "Epoch 1: train_loss=1.6041, val_loss=4.7288\n",
      " SAVING INTO CHECKPOINT at global step 4500 and loss 1.3372802734375\n",
      "epoch 1/10 step 4499/16000 Loss: 5.3491 | Elapsed: 26.00 min | ETA: 898.52 min | Total est: 924.52 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 210.42\n",
      "Epoch 1: train_loss=1.6352, val_loss=4.8695\n",
      " SAVING INTO CHECKPOINT at global step 4600 and loss 1.0480114221572876\n",
      "epoch 1/10 step 4599/16000 Loss: 4.1920 | Elapsed: 26.58 min | ETA: 897.90 min | Total est: 924.48 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 66.16\n",
      "Epoch 1: train_loss=1.6658, val_loss=4.6965\n",
      " SAVING INTO CHECKPOINT at global step 4700 and loss 1.1126590967178345\n",
      "epoch 1/10 step 4699/16000 Loss: 4.4506 | Elapsed: 27.17 min | ETA: 897.74 min | Total est: 924.91 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 85.68\n",
      "Epoch 1: train_loss=1.6968, val_loss=4.7580\n",
      " SAVING INTO CHECKPOINT at global step 4800 and loss 1.1606640815734863\n",
      "epoch 1/10 step 4799/16000 Loss: 4.6427 | Elapsed: 27.75 min | ETA: 897.10 min | Total est: 924.84 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 103.82\n",
      "Epoch 1: train_loss=1.7273, val_loss=4.6609\n",
      " SAVING INTO CHECKPOINT at global step 4900 and loss 1.1734439134597778\n",
      "epoch 1/10 step 4899/16000 Loss: 4.6938 | Elapsed: 28.32 min | ETA: 896.57 min | Total est: 924.89 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 109.26\n",
      "Epoch 1: train_loss=1.7581, val_loss=4.6818\n",
      " SAVING INTO CHECKPOINT at global step 5000 and loss 1.14603590965271\n",
      "epoch 1/10 step 4999/16000 Loss: 4.5841 | Elapsed: 28.89 min | ETA: 895.48 min | Total est: 924.36 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 97.92\n",
      "Epoch 1: train_loss=1.7884, val_loss=4.6213\n",
      " SAVING INTO CHECKPOINT at global step 5100 and loss 1.2042135000228882\n",
      "epoch 1/10 step 5099/16000 Loss: 4.8169 | Elapsed: 29.47 min | ETA: 894.99 min | Total est: 924.46 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 123.58\n",
      "Epoch 1: train_loss=1.8185, val_loss=4.6396\n",
      " SAVING INTO CHECKPOINT at global step 5200 and loss 1.3380824327468872\n",
      "epoch 1/10 step 5199/16000 Loss: 5.3523 | Elapsed: 30.04 min | ETA: 894.27 min | Total est: 924.31 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 211.10\n",
      "Epoch 1: train_loss=1.8488, val_loss=4.5508\n",
      " SAVING INTO CHECKPOINT at global step 5300 and loss 1.0237358808517456\n",
      "epoch 1/10 step 5299/16000 Loss: 4.0949 | Elapsed: 30.61 min | ETA: 893.41 min | Total est: 924.02 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 60.04\n",
      "Epoch 1: train_loss=1.8787, val_loss=4.5930\n",
      " SAVING INTO CHECKPOINT at global step 5400 and loss 1.017480492591858\n",
      "epoch 1/10 step 5399/16000 Loss: 4.0699 | Elapsed: 31.18 min | ETA: 892.78 min | Total est: 923.96 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 58.55\n",
      "Epoch 1: train_loss=1.9090, val_loss=4.5778\n",
      " SAVING INTO CHECKPOINT at global step 5500 and loss 1.1453328132629395\n",
      "epoch 1/10 step 5499/16000 Loss: 4.5813 | Elapsed: 31.75 min | ETA: 891.91 min | Total est: 923.66 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 97.64\n",
      "Epoch 1: train_loss=1.9384, val_loss=4.5239\n",
      " SAVING INTO CHECKPOINT at global step 5600 and loss 1.0907633304595947\n",
      "epoch 1/10 step 5599/16000 Loss: 4.3631 | Elapsed: 32.33 min | ETA: 891.50 min | Total est: 923.84 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 78.50\n",
      "Epoch 1: train_loss=1.9682, val_loss=4.5523\n",
      " SAVING INTO CHECKPOINT at global step 5700 and loss 1.1205556392669678\n",
      "epoch 1/10 step 5699/16000 Loss: 4.4822 | Elapsed: 32.92 min | ETA: 891.09 min | Total est: 924.01 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 88.43\n",
      "Epoch 1: train_loss=1.9978, val_loss=4.5551\n",
      " SAVING INTO CHECKPOINT at global step 5800 and loss 1.09033203125\n",
      "epoch 1/10 step 5799/16000 Loss: 4.3613 | Elapsed: 33.50 min | ETA: 890.68 min | Total est: 924.18 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 78.36\n",
      "Epoch 1: train_loss=2.0273, val_loss=4.6121\n",
      " SAVING INTO CHECKPOINT at global step 5900 and loss 1.0976333618164062\n",
      "epoch 1/10 step 5899/16000 Loss: 4.3905 | Elapsed: 34.08 min | ETA: 890.16 min | Total est: 924.24 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 80.68\n",
      "Epoch 1: train_loss=2.0568, val_loss=4.5395\n",
      " SAVING INTO CHECKPOINT at global step 6000 and loss 1.2066259384155273\n",
      "epoch 1/10 step 5999/16000 Loss: 4.8265 | Elapsed: 34.65 min | ETA: 889.41 min | Total est: 924.06 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 124.77\n",
      "Epoch 1: train_loss=2.0865, val_loss=4.6142\n",
      " SAVING INTO CHECKPOINT at global step 6100 and loss 1.2469003200531006\n",
      "epoch 1/10 step 6099/16000 Loss: 4.9876 | Elapsed: 35.23 min | ETA: 888.93 min | Total est: 924.17 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 146.58\n",
      "Epoch 1: train_loss=2.1157, val_loss=4.4841\n",
      " SAVING INTO CHECKPOINT at global step 6200 and loss 1.2364407777786255\n",
      "epoch 1/10 step 6199/16000 Loss: 4.9458 | Elapsed: 35.81 min | ETA: 888.22 min | Total est: 924.03 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 140.58\n",
      "Epoch 1: train_loss=2.1449, val_loss=4.5307\n",
      " SAVING INTO CHECKPOINT at global step 6300 and loss 0.9394734501838684\n",
      "epoch 1/10 step 6299/16000 Loss: 3.7579 | Elapsed: 36.37 min | ETA: 887.39 min | Total est: 923.76 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 42.86\n",
      "Epoch 1: train_loss=2.1744, val_loss=4.4483\n",
      " SAVING INTO CHECKPOINT at global step 6400 and loss 1.312475562095642\n",
      "epoch 1/10 step 6399/16000 Loss: 5.2499 | Elapsed: 36.95 min | ETA: 886.72 min | Total est: 923.67 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 190.55\n",
      "Epoch 1: train_loss=2.2036, val_loss=4.4550\n",
      " SAVING INTO CHECKPOINT at global step 6500 and loss 1.2310888767242432\n",
      "epoch 1/10 step 6499/16000 Loss: 4.9244 | Elapsed: 37.53 min | ETA: 886.34 min | Total est: 923.87 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 137.60\n",
      "Epoch 1: train_loss=2.2322, val_loss=4.5123\n",
      " SAVING INTO CHECKPOINT at global step 6600 and loss 1.1714322566986084\n",
      "epoch 1/10 step 6599/16000 Loss: 4.6857 | Elapsed: 38.12 min | ETA: 885.94 min | Total est: 924.06 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 108.39\n",
      "Epoch 1: train_loss=2.2609, val_loss=4.5765\n",
      " SAVING INTO CHECKPOINT at global step 6700 and loss 1.2417579889297485\n",
      "epoch 1/10 step 6699/16000 Loss: 4.9670 | Elapsed: 38.69 min | ETA: 885.34 min | Total est: 924.03 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 143.60\n",
      "Epoch 1: train_loss=2.2890, val_loss=4.4500\n",
      " SAVING INTO CHECKPOINT at global step 6800 and loss 1.1614346504211426\n",
      "epoch 1/10 step 6799/16000 Loss: 4.6457 | Elapsed: 39.26 min | ETA: 884.47 min | Total est: 923.73 min | Memory: 4.45 GB , \\ 8.02 GB | Perplexity 104.14\n",
      "Epoch 1: train_loss=2.3174, val_loss=4.4540\n",
      " SAVING INTO CHECKPOINT at global step 6900 and loss 1.1389399766921997\n",
      "epoch 1/10 step 6899/16000 Loss: 4.5558 | Elapsed: 39.82 min | ETA: 883.65 min | Total est: 923.47 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 95.18\n",
      "Epoch 1: train_loss=2.3459, val_loss=4.4563\n",
      " SAVING INTO CHECKPOINT at global step 7000 and loss 1.2291837930679321\n",
      "epoch 1/10 step 6999/16000 Loss: 4.9167 | Elapsed: 40.38 min | ETA: 882.61 min | Total est: 922.99 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 136.56\n",
      "Epoch 1: train_loss=2.3741, val_loss=4.4914\n",
      " SAVING INTO CHECKPOINT at global step 7100 and loss 1.29302978515625\n",
      "epoch 1/10 step 7099/16000 Loss: 5.1721 | Elapsed: 40.94 min | ETA: 881.65 min | Total est: 922.59 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 176.29\n",
      "Epoch 1: train_loss=2.4026, val_loss=4.5042\n",
      " SAVING INTO CHECKPOINT at global step 7200 and loss 1.0937234163284302\n",
      "epoch 1/10 step 7199/16000 Loss: 4.3749 | Elapsed: 41.50 min | ETA: 880.73 min | Total est: 922.23 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 79.43\n",
      "Epoch 1: train_loss=2.4312, val_loss=4.5058\n",
      " SAVING INTO CHECKPOINT at global step 7300 and loss 1.4867802858352661\n",
      "epoch 1/10 step 7299/16000 Loss: 5.9471 | Elapsed: 42.06 min | ETA: 879.80 min | Total est: 921.86 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 382.65\n",
      "Epoch 1: train_loss=2.4595, val_loss=4.5095\n",
      " SAVING INTO CHECKPOINT at global step 7400 and loss 1.3310058116912842\n",
      "epoch 1/10 step 7399/16000 Loss: 5.3240 | Elapsed: 42.62 min | ETA: 878.98 min | Total est: 921.60 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 205.21\n",
      "Epoch 1: train_loss=2.4880, val_loss=4.4667\n",
      " SAVING INTO CHECKPOINT at global step 7500 and loss 1.3315165042877197\n",
      "epoch 1/10 step 7499/16000 Loss: 5.3261 | Elapsed: 43.19 min | ETA: 878.14 min | Total est: 921.33 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 205.63\n",
      "Epoch 1: train_loss=2.5166, val_loss=4.3846\n",
      " SAVING INTO CHECKPOINT at global step 7600 and loss 1.2430931329727173\n",
      "epoch 1/10 step 7599/16000 Loss: 4.9724 | Elapsed: 43.77 min | ETA: 877.71 min | Total est: 921.48 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 144.37\n",
      "Epoch 1: train_loss=2.5444, val_loss=4.4305\n",
      " SAVING INTO CHECKPOINT at global step 7700 and loss 1.324981689453125\n",
      "epoch 1/10 step 7699/16000 Loss: 5.2999 | Elapsed: 44.33 min | ETA: 876.78 min | Total est: 921.11 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 200.32\n",
      "Epoch 1: train_loss=2.5723, val_loss=4.3313\n",
      " SAVING INTO CHECKPOINT at global step 7800 and loss 0.9029005765914917\n",
      "epoch 1/10 step 7799/16000 Loss: 3.6116 | Elapsed: 44.89 min | ETA: 875.87 min | Total est: 920.76 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 37.03\n",
      "Epoch 1: train_loss=2.6001, val_loss=4.4296\n",
      " SAVING INTO CHECKPOINT at global step 7900 and loss 1.0779080390930176\n",
      "epoch 1/10 step 7899/16000 Loss: 4.3116 | Elapsed: 45.45 min | ETA: 874.98 min | Total est: 920.43 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 74.56\n",
      "Epoch 1: train_loss=2.6282, val_loss=4.3310\n",
      " SAVING INTO CHECKPOINT at global step 8000 and loss 1.1752657890319824\n",
      "epoch 1/10 step 7999/16000 Loss: 4.7011 | Elapsed: 46.02 min | ETA: 874.42 min | Total est: 920.44 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 110.06\n",
      "Epoch 1: train_loss=2.6560, val_loss=4.3602\n",
      " SAVING INTO CHECKPOINT at global step 8100 and loss 1.130469560623169\n",
      "epoch 1/10 step 8099/16000 Loss: 4.5219 | Elapsed: 46.60 min | ETA: 873.95 min | Total est: 920.56 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 92.01\n",
      "Epoch 1: train_loss=2.6834, val_loss=4.3035\n",
      " SAVING INTO CHECKPOINT at global step 8200 and loss 1.070415735244751\n",
      "epoch 1/10 step 8199/16000 Loss: 4.2817 | Elapsed: 47.17 min | ETA: 873.15 min | Total est: 920.32 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 72.36\n",
      "Epoch 1: train_loss=2.7113, val_loss=4.3082\n",
      " SAVING INTO CHECKPOINT at global step 8300 and loss 1.2568049430847168\n",
      "epoch 1/10 step 8299/16000 Loss: 5.0272 | Elapsed: 47.74 min | ETA: 872.61 min | Total est: 920.36 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 152.51\n",
      "Epoch 1: train_loss=2.7397, val_loss=4.3771\n",
      " SAVING INTO CHECKPOINT at global step 8400 and loss 1.2353101968765259\n",
      "epoch 1/10 step 8399/16000 Loss: 4.9412 | Elapsed: 48.39 min | ETA: 873.36 min | Total est: 921.75 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 139.94\n",
      "Epoch 1: train_loss=2.7676, val_loss=4.3588\n",
      " SAVING INTO CHECKPOINT at global step 8500 and loss 1.3237258195877075\n",
      "epoch 1/10 step 8499/16000 Loss: 5.2949 | Elapsed: 49.07 min | ETA: 874.68 min | Total est: 923.75 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 199.32\n",
      "Epoch 1: train_loss=2.7951, val_loss=4.2804\n",
      " SAVING INTO CHECKPOINT at global step 8600 and loss 1.1719722747802734\n",
      "epoch 1/10 step 8599/16000 Loss: 4.6879 | Elapsed: 49.75 min | ETA: 875.76 min | Total est: 925.51 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 108.62\n",
      "Epoch 1: train_loss=2.8229, val_loss=4.2620\n",
      " SAVING INTO CHECKPOINT at global step 8700 and loss 1.1433184146881104\n",
      "epoch 1/10 step 8699/16000 Loss: 4.5733 | Elapsed: 50.35 min | ETA: 875.62 min | Total est: 925.97 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 96.86\n",
      "Epoch 1: train_loss=2.8506, val_loss=4.2993\n",
      " SAVING INTO CHECKPOINT at global step 8800 and loss 1.120835542678833\n",
      "epoch 1/10 step 8799/16000 Loss: 4.4833 | Elapsed: 51.00 min | ETA: 876.28 min | Total est: 927.28 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 88.53\n",
      "Epoch 1: train_loss=2.8779, val_loss=4.3216\n",
      " SAVING INTO CHECKPOINT at global step 8900 and loss 1.1085326671600342\n",
      "epoch 1/10 step 8899/16000 Loss: 4.4341 | Elapsed: 51.64 min | ETA: 876.65 min | Total est: 928.29 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 84.28\n",
      "Epoch 1: train_loss=2.9054, val_loss=4.3003\n",
      " SAVING INTO CHECKPOINT at global step 9000 and loss 1.1382434368133545\n",
      "epoch 1/10 step 8999/16000 Loss: 4.5530 | Elapsed: 52.19 min | ETA: 875.71 min | Total est: 927.91 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 94.91\n",
      "Epoch 1: train_loss=2.9325, val_loss=4.2714\n",
      " SAVING INTO CHECKPOINT at global step 9100 and loss 1.0582318305969238\n",
      "epoch 1/10 step 9099/16000 Loss: 4.2329 | Elapsed: 52.79 min | ETA: 875.43 min | Total est: 928.22 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 68.92\n",
      "Epoch 1: train_loss=2.9608, val_loss=4.2741\n",
      " SAVING INTO CHECKPOINT at global step 9200 and loss 1.2590783834457397\n",
      "epoch 1/10 step 9199/16000 Loss: 5.0363 | Elapsed: 53.45 min | ETA: 876.09 min | Total est: 929.54 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 153.90\n",
      "Epoch 1: train_loss=2.9883, val_loss=4.2405\n",
      " SAVING INTO CHECKPOINT at global step 9300 and loss 1.1120988130569458\n",
      "epoch 1/10 step 9299/16000 Loss: 4.4484 | Elapsed: 54.10 min | ETA: 876.60 min | Total est: 930.69 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 85.49\n",
      "Epoch 1: train_loss=3.0154, val_loss=4.3436\n",
      " SAVING INTO CHECKPOINT at global step 9400 and loss 0.954146146774292\n",
      "epoch 1/10 step 9399/16000 Loss: 3.8166 | Elapsed: 54.76 min | ETA: 877.31 min | Total est: 932.07 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 45.45\n",
      "Epoch 1: train_loss=3.0426, val_loss=4.2790\n",
      " SAVING INTO CHECKPOINT at global step 9500 and loss 0.881344199180603\n",
      "epoch 1/10 step 9499/16000 Loss: 3.5254 | Elapsed: 55.33 min | ETA: 876.48 min | Total est: 931.81 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 33.97\n",
      "Epoch 1: train_loss=3.0692, val_loss=4.2197\n",
      " SAVING INTO CHECKPOINT at global step 9600 and loss 0.9776114821434021\n",
      "epoch 1/10 step 9599/16000 Loss: 3.9104 | Elapsed: 55.89 min | ETA: 875.65 min | Total est: 931.54 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 49.92\n",
      "Epoch 1: train_loss=3.0970, val_loss=4.2554\n",
      " SAVING INTO CHECKPOINT at global step 9700 and loss 1.027392029762268\n",
      "epoch 1/10 step 9699/16000 Loss: 4.1096 | Elapsed: 56.47 min | ETA: 874.97 min | Total est: 931.44 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 60.92\n",
      "Epoch 1: train_loss=3.1250, val_loss=4.2387\n",
      " SAVING INTO CHECKPOINT at global step 9800 and loss 1.202567219734192\n",
      "epoch 1/10 step 9799/16000 Loss: 4.8103 | Elapsed: 57.06 min | ETA: 874.50 min | Total est: 931.56 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 122.76\n",
      "Epoch 1: train_loss=3.1522, val_loss=4.2731\n",
      " SAVING INTO CHECKPOINT at global step 9900 and loss 1.0245544910430908\n",
      "epoch 1/10 step 9899/16000 Loss: 4.0982 | Elapsed: 57.66 min | ETA: 874.23 min | Total est: 931.90 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 60.23\n",
      "Epoch 1: train_loss=3.1799, val_loss=4.2174\n",
      " SAVING INTO CHECKPOINT at global step 10000 and loss 0.904185950756073\n",
      "epoch 1/10 step 9999/16000 Loss: 3.6167 | Elapsed: 58.25 min | ETA: 873.80 min | Total est: 932.05 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 37.22\n",
      "Epoch 1: train_loss=3.2072, val_loss=4.2760\n",
      " SAVING INTO CHECKPOINT at global step 10100 and loss 1.0366863012313843\n",
      "epoch 1/10 step 10099/16000 Loss: 4.1467 | Elapsed: 58.85 min | ETA: 873.49 min | Total est: 932.34 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 63.23\n",
      "Epoch 1: train_loss=3.2343, val_loss=4.2708\n",
      " SAVING INTO CHECKPOINT at global step 10200 and loss 1.4782559871673584\n",
      "epoch 1/10 step 10199/16000 Loss: 5.9130 | Elapsed: 59.48 min | ETA: 873.52 min | Total est: 933.00 min | Memory: 4.45 GB , \\ 8.01 GB | Perplexity 369.82\n"
     ]
    }
   ],
   "source": [
    "#### TRAINING LOOP #####\n",
    "\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import numpy as np \n",
    "from utils import calculate_total_train_params, save_to_checkpoint\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # --------------------\n",
    "    #  Validation step\n",
    "    # --------------------\n",
    "    decoder_model.eval()\n",
    "    encoder_model.eval()\n",
    "    val_loss = 0\n",
    "    count = 0 \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            image_tensor, caption_tensor, attention_mask = [x.to(device) for x in val_batch]\n",
    "            with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "                x_embed = encoder_model(image_tensor)\n",
    "                _, val_caption_loss = decoder_model(x_embed, caption_tensor, attention_mask)\n",
    "            val_loss += val_caption_loss.item()\n",
    "            count+=1\n",
    "            if count > 2:break \n",
    "    val_loss /= count \n",
    "    decoder_model.train()\n",
    "    encoder_model.train()\n",
    "    print(f\"Epoch {epoch+1}: train_loss={total_loss/len(train_dataloader):.4f}, val_loss={val_loss:.4f}\")\n",
    "    return val_loss \n",
    "\n",
    "\n",
    "\n",
    "def should_stop(loss_list):\n",
    "    last_ten_loss = loss_list[-50*4:]\n",
    "    threshold = 0.5\n",
    "    if len(last_ten_loss)==50*4 and len(loss_list)>=50*4:\n",
    "        diffs = np.diff(last_ten_loss)\n",
    "        step_trends = []\n",
    "        for d in diffs:\n",
    "            if d > threshold:\n",
    "                step_trends.append(\"increasing\")\n",
    "            elif d < -threshold:\n",
    "                step_trends.append(\"decreasing\")\n",
    "            else:\n",
    "                step_trends.append(\"steady\")\n",
    "\n",
    "        if all(t == \"steady\" for t in step_trends):\n",
    "            return True \n",
    "        else:\n",
    "            print (\"Trend\", step_trends)\n",
    "    return False \n",
    "\n",
    "\n",
    "\n",
    "##### Setup Training #####\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "all_params = calculate_total_train_params(encoder_model, decoder_model)\n",
    "\n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TrainingConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/TrainingConfig.accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 10\n",
    "stop = False \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "N_EPOCHS = TrainingConfig.epochs - l_epoch\n",
    "\n",
    "print (f\"PREVIOUS LOSS {l_loss} AT GLOBAL STEP {l_global_step} AT EPOCH {l_epoch}\")\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1 \n",
    "\n",
    "        if global_step < l_global_step:continue   \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "            logits, caption_loss  = decoder_model(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "\n",
    "            loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "        # x_embed = image_encoder(image_tensor) # (B, N, embed_size) \n",
    "        # logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "        # loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "        if (step + 1) % TrainingConfig.accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        if global_step %100==0:\n",
    "            val_loss = eval()\n",
    "            loss_list.append(val_loss)\n",
    "            save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "\n",
    "            if should_stop(loss_list):\n",
    "                stop = True \n",
    "                break\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{TrainingConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*TrainingConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*TrainingConfig.accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            # save_model(image_encoder=image_encoder, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % TrainingConfig.accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    if stop:\n",
    "        save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "        break\n",
    "    \n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "tup = get_models() \n",
    "\n",
    "TrainingConfig, tokenizer = tup[0], tup[-1]\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "from transformers import LogitsProcessorList, MinLengthLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
    "\n",
    "from transformers.generation.logits_process import (\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "\n",
    "# Chain multiple warpers\n",
    "warpers = LogitsProcessorList([\n",
    "    TemperatureLogitsWarper(0.3),\n",
    "    TopKLogitsWarper(20),\n",
    "    TopPLogitsWarper(0.95),\n",
    "])\n",
    "\n",
    "processors = LogitsProcessorList([\n",
    "    RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "])\n",
    "\n",
    "from utils import load_from_checkpoint \n",
    "\n",
    "encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "def generate_caption(image_tensor, max_len=30, use_image=True):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "\n",
    "    # Only process image if use_image is True\n",
    "    if use_image:\n",
    "        x_embed = encoder(image_tensor)\n",
    "    else:\n",
    "        # Create dummy embeddings if not using image\n",
    "        x_embed = torch.zeros((1, 50, decoder.embed_size), device=device)\n",
    "    \n",
    "    # Start with the START token\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    generated_ids = torch.tensor([[start_id]], device=device)\n",
    "    end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\")\n",
    "\n",
    "    # Setup warpers and processors for better text generation\n",
    "    warpers_list = LogitsProcessorList([\n",
    "        TemperatureLogitsWarper(0.7),  # Lower temperature for less randomness\n",
    "        TopKLogitsWarper(50),         # More top-k options\n",
    "        TopPLogitsWarper(0.92),       # Slightly more conservative top-p\n",
    "    ])\n",
    "    \n",
    "    processors_list = LogitsProcessorList([\n",
    "        RepetitionPenaltyLogitsProcessor(penalty=1.5),  # Stronger repetition penalty\n",
    "        MinLengthLogitsProcessor(5, end_token_id)      # Ensure minimum caption length\n",
    "    ])\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            # The decoder internally handles creating the appropriate attention mask\n",
    "            # Just pass a simple mask for the text tokens - decoder will handle the rest\n",
    "            attn_mask = torch.ones(1, generated_ids.shape[1], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Important: Use mode=\"inference\" for generation\n",
    "            logits, _ = decoder(x_embed, generated_ids, attn_mask, mode=\"inference\")\n",
    "            \n",
    "            # Get the logits for the next token only (the last position)\n",
    "            # The -1 index accounts for the concatenated image tokens in the decoder\n",
    "            next_token_idx = -1\n",
    "            next_logits = logits[:, next_token_idx, :]\n",
    "            \n",
    "            # Process logits to avoid repetition\n",
    "            next_logits = processors_list(generated_ids, next_logits)\n",
    "            \n",
    "            # Apply temperature and top-k/top-p filtering\n",
    "            next_logits = warpers_list(generated_ids, next_logits)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            \n",
    "            # Sample the next token\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Add the new token to our sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "            \n",
    "            # Stop if we generated an END token\n",
    "            if next_token_id.item() == end_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the generated tokens to text, skipping special tokens\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "# Example usage\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    #caption_without_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=False)\n",
    "\n",
    "    caption_with_image = generate_caption(image_tensor[0], use_image=True)\n",
    "\n",
    "    plt.imshow(image_tensor[0].float().permute(1,2,0).cpu().numpy())\n",
    "    print(\"With image context:\\t \\t\", caption_with_image)\n",
    "    print (\"Actual\\t\\t\", tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True))\n",
    "    # print(\"Without image context: \", caption_without_image)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75993e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate_caption, visualize_caption\n",
    "import torch \n",
    "\n",
    "from utils import load_from_checkpoint \n",
    "\n",
    "encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "# Pick a random sample from the validation set\n",
    "import random\n",
    "\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True) \n",
    "    print(\"Ground Truth:\\t\", gt_caption)\n",
    "\n",
    "\n",
    "    # Generate a caption\n",
    "    caption = generate_caption(\n",
    "        image_tensor,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        temperature=0.7,  # Lower for less randomness\n",
    "        repetition_penalty=1.5  # Higher to avoid repetition\n",
    "    )\n",
    "    # Visualize the result\n",
    "    visualize_caption(image_tensor, caption, gt_caption)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45828345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed MHA implementation with multiple images\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_on_samples(num_samples=5, temperature_values=[0.6, 0.7, 0.8]):\n",
    "    \"\"\"Test the model on multiple samples with different temperature settings\"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Get a few random samples from validation set\n",
    "    val_batches = []\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        val_batches.append(batch)\n",
    "    \n",
    "    print(f\"Evaluating on {len(val_batches)} validation samples with different temperatures\")\n",
    "    \n",
    "    # Try different temperature values\n",
    "    for temp in temperature_values:\n",
    "        print(f\"\\nUsing temperature: {temp}\")\n",
    "        \n",
    "        for batch in val_batches:\n",
    "            image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2]\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            \n",
    "            # Get ground truth\n",
    "            gt = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True)\n",
    "            \n",
    "            # Generate caption with current temperature\n",
    "            caption = generate_caption(\n",
    "                image_tensor[0],\n",
    "                encoder,\n",
    "                decoder,\n",
    "                tokenizer,\n",
    "                device,\n",
    "                temperature=temp,\n",
    "                repetition_penalty=1.5\n",
    "            )\n",
    "            \n",
    "            # Save results\n",
    "            print(f\"GT: {gt}\")\n",
    "            print(f\"Generated: {caption}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            all_results.append({\n",
    "                \"temperature\": temp,\n",
    "                \"ground_truth\": gt,\n",
    "                \"generated\": caption\n",
    "            })\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Run evaluation\n",
    "try:\n",
    "    results = evaluate_on_samples(num_samples=3, temperature_values=[0.6, 0.8])\n",
    "    print(\"MHA fix successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in MHA: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6032ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention between image features and text\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def visualize_attention(image_tensor, encoder, decoder, tokenizer, device):\n",
    "    \"\"\"Visualize the cross-attention between image features and text tokens\"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Process image\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        img_features = encoder(image_tensor)\n",
    "        \n",
    "    # Create a simple prompt\n",
    "    start_token = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    input_ids = torch.tensor([[start_token]], device=device)\n",
    "    \n",
    "    # Get the embeddings\n",
    "    B = 1\n",
    "    queries = decoder.img_queries.unsqueeze(0).expand(B, -1, -1)  # (B, num_img_tokens, D)\n",
    "    \n",
    "    # Project image features\n",
    "    k = decoder.key_proj(img_features)\n",
    "    v = decoder.value_proj(img_features)\n",
    "    \n",
    "    # Get query projections\n",
    "    with torch.no_grad():\n",
    "        q = decoder.query_proj(queries)\n",
    "        \n",
    "    # Compute attention weights manually\n",
    "    q = q / np.sqrt(decoder.embed_size)\n",
    "    attention_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    attention_probs = F.softmax(attention_weights, dim=-1)\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Show the image\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(image_tensor[0].float().permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Show the attention map (average over heads)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    attention_map = attention_probs[0].mean(0).cpu().numpy()\n",
    "    plt.imshow(attention_map, cmap='viridis')\n",
    "    plt.title(\"Image-Text Attention Map\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Show a few sample attention distributions\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i in range(min(5, decoder.num_img_tokens)):\n",
    "        plt.plot(attention_probs[0, i].cpu().numpy(), \n",
    "                 label=f\"Query token {i}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Attention Distribution for First 5 Query Tokens\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return attention_probs\n",
    "\n",
    "# Get a sample image\n",
    "for batch in val_dataloader:\n",
    "    image_tensor = batch[0][0]\n",
    "    break\n",
    "\n",
    "# Visualize attention\n",
    "try:\n",
    "    attention_probs = visualize_attention(image_tensor, encoder, decoder, tokenizer, device)\n",
    "    print(\"Successfully visualized attention!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in attention visualization: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fcca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the improved model with anti-hallucination measures\n",
    "from utils import load_from_checkpoint\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reload the model with our improvements\n",
    "encoder, decoder, epoch, loss, global_step, tokenizer = load_from_checkpoint()\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "def test_hallucination_reduction():\n",
    "    \"\"\"Test if our improvements have reduced hallucinations\"\"\"\n",
    "    # Get some validation samples\n",
    "    val_samples = []\n",
    "    for i, batch in enumerate(val_dataloader):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        val_samples.append(batch)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Testing hallucination reduction with improved model:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for batch in val_samples:\n",
    "        image_tensor, caption_tensor, attention_mask = [x.to(device) for x in batch]\n",
    "        \n",
    "        # Get ground truth\n",
    "        gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True)\n",
    "        \n",
    "        # Generate caption with our improved model\n",
    "        try:\n",
    "            from generate import generate_caption\n",
    "            \n",
    "            caption = generate_caption(\n",
    "                image_tensor[0],\n",
    "                encoder,\n",
    "                decoder,\n",
    "                tokenizer,\n",
    "                device,\n",
    "                temperature=0.6,\n",
    "                repetition_penalty=1.8\n",
    "            )\n",
    "            \n",
    "            # Display results\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.imshow(image_tensor[0].float().permute(1, 2, 0).cpu().numpy())\n",
    "            plt.title(f\"Generated: {caption}\\nGround Truth: {gt_caption}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            results.append({\n",
    "                'gt': gt_caption,\n",
    "                'generated': caption,\n",
    "                'success': True\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating caption: {e}\")\n",
    "            results.append({\n",
    "                'gt': gt_caption,\n",
    "                'generated': None,\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "hallucination_test_results = test_hallucination_reduction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Q-Former style implementation\n",
    "import torch\n",
    "from utils import load_from_checkpoint\n",
    "from generate import generate_caption\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Testing the Q-Former style implementation...\")\n",
    "\n",
    "# Load the model\n",
    "try:\n",
    "    encoder, decoder, epoch, loss, global_step, tokenizer = load_from_checkpoint()\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Get a batch from the validation set\n",
    "    for batch in val_dataloader:\n",
    "        image_tensor, caption_tensor, attention_mask = [x.to(device) for x in batch]\n",
    "        break\n",
    "\n",
    "    # Get ground truth caption\n",
    "    gt_caption = tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True)\n",
    "    print(f\"Ground truth: {gt_caption}\")\n",
    "    \n",
    "    # Generate caption with Q-Former style model\n",
    "    caption = generate_caption(\n",
    "        image_tensor[0],\n",
    "        encoder,\n",
    "        decoder,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        temperature=0.6\n",
    "    )\n",
    "    \n",
    "    print(f\"Q-Former generated: {caption}\")\n",
    "    \n",
    "    # Visualize the image and captions\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image_tensor[0].cpu().permute(1, 2, 0).numpy())\n",
    "    plt.title(f\"Generated: {caption}\\nGround Truth: {gt_caption}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Q-Former test completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error testing Q-Former implementation: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69862487",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tokens = [1, 2, 3, 4]\n",
    "image_mask = torch.tensor([1, 1, 1, 1], dtype=torch.long).bool()\n",
    "\n",
    "text_tokens = [10, 11, 12, 13] \n",
    "causal_mask = torch.tril(torch.ones((len(text_tokens), len(text_tokens)))).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d454ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tensor = torch.cat([torch.tensor(image_tokens), torch.tensor(text_tokens)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c01a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b01fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.tril(torch.ones((len(embed_tensor), len(embed_tensor)))).bool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fd472",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960812f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask[0:image_mask.shape[0], :image_mask.shape[0]] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244d9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb2450",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask.unsqueeze(0).expand(4, -1, -1) .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf45578",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask = torch.cat([image_mask, image_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e174959",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_mask = image_mask.unsqueeze(0).unsqueeze(1) \n",
    "image_mask, image_mask.shape, causal_mask.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d83fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = image_mask & causal_mask\n",
    "r, r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab53e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "layer = nn.Linear(4, 1, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ca346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (next(layer.parameters()).dtype), print (next(layer.parameters()).device), print (next(layer.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491deeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 4, dtype=torch.bfloat16)\n",
    "x, x.shape, x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc63c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 123.123446789\n",
    "\n",
    "x1 = torch.tensor(x, dtype=torch.float32)\n",
    "x2 = torch.tensor(x, dtype=torch.float16)\n",
    "x3 = torch.tensor(x, dtype=torch.bfloat16)\n",
    "\n",
    "x1.item(), x1.dtype, x2.item(), x2.dtype , x3.item(), x3.dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8626602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(\"12344360351562\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.zeros(4, 4).bool()\n",
    "image_token = 2 \n",
    "image_mask = torch.ones(image_token, image_token).bool()\n",
    "\n",
    "mask[:image_token, :image_token] = image_mask;mask, image_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50babb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f63689",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_mask = torch.tril(torch.ones(2,4)).bool();causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[image_token:,image_token:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3074ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032af4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.repeat(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb787ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from torch.nn import functional as F \n",
    "\n",
    "sentences = [\n",
    "    \"The cat sits on the mat.\",       # A\n",
    "    \"A cat is sitting on a mat.\",     # B (very similar to A)\n",
    "    \"The stock market crashed today.\" # C (unrelated)\n",
    "]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences, normalize_embeddings=False)\n",
    "\n",
    "embeddings = torch.tensor(embeddings)\n",
    "\n",
    "first = embeddings[0, :]\n",
    "second = embeddings[1, :]\n",
    "third = embeddings[2, :]\n",
    "\n",
    "\n",
    "# first = F.normalize(first, p=2, dim=0)\n",
    "# second = F.normalize(second, p=2, dim=0)\n",
    "# third = F.normalize(third, p=2, dim=0)\n",
    "\n",
    "\n",
    "print (F.cosine_similarity(first, second, dim=0))\n",
    "print (F.cosine_similarity(first, third, dim=0)) \n",
    "print (F.cosine_similarity(second, third, dim=0)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "first[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b28ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
