{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test MultiScaleCLIPEncoder with Deformable Attention\n",
    "\n",
    "\n",
    "## 1. Import Required Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import sys\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Install einops if not available\n",
    "try:\n",
    "    import einops\n",
    "except ImportError:\n",
    "    !pip install einops\n",
    "    import einops\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# Add the parent directory to the path so we can import our modules\n",
    "sys.path.append('/Users/preetamverma/Desktop/multimodel')\n",
    "\n",
    "# Import our custom encoder modules\n",
    "from multiscale_encoder import MultiScaleCLIPEncoder\n",
    "from encoder import CLIPEncoder\n",
    "\n",
    "## 2. Helper Functions for Visualization\n",
    "\n",
    "def download_image(url):\n",
    "    \"\"\"Download an image from URL\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "def preprocess_image(image, size=224):\n",
    "    \"\"\"Preprocess image for CLIP\"\"\"\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                             std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "def show_attention_maps(image, attention_maps, titles=None):\n",
    "    \"\"\"Visualize attention maps\"\"\"\n",
    "    fig, axes = plt.subplots(1, len(attention_maps) + 1, figsize=(15, 5))\n",
    "    \n",
    "    # Show original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show attention maps\n",
    "    for i, attn_map in enumerate(attention_maps):\n",
    "        im = axes[i+1].imshow(attn_map, cmap='viridis')\n",
    "        axes[i+1].set_title(titles[i] if titles else f\"Attention Map {i+1}\")\n",
    "        axes[i+1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[i+1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "## 3. Sample Images for Testing\n",
    "\n",
    "# Download sample images\n",
    "image_urls = [\n",
    "    \"https://farm2.staticflickr.com/1533/26541536141_41abe98db3_z_d.jpg\",  # Dog\n",
    "    \"https://farm9.staticflickr.com/8596/16715636612_8d7a3ee6a6_z_d.jpg\",  # Urban scene\n",
    "    \"https://farm1.staticflickr.com/9/12715999_7a0f724bae_z_d.jpg\"         # Close-up object\n",
    "]\n",
    "\n",
    "sample_images = []\n",
    "for url in image_urls:\n",
    "    try:\n",
    "        img = download_image(url)\n",
    "        sample_images.append(img)\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download image: {e}\")\n",
    "\n",
    "## 4. Initialize Both Encoders\n",
    "\n",
    "# Parameters for both encoders\n",
    "embed_size = 768\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "freeze_vision = True\n",
    "\n",
    "# Initialize the standard CLIPEncoder\n",
    "standard_encoder = CLIPEncoder(\n",
    "    embed_size=embed_size,\n",
    "    model_name=model_name,\n",
    "    freeze_vision=freeze_vision\n",
    ")\n",
    "\n",
    "# Initialize our MultiScaleCLIPEncoder\n",
    "multiscale_encoder = MultiScaleCLIPEncoder(\n",
    "    embed_size=embed_size,\n",
    "    model_name=model_name,\n",
    "    freeze_vision=freeze_vision,\n",
    "    num_heads=8,\n",
    "    num_points=4,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move models to appropriate device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "standard_encoder = standard_encoder.to(device)\n",
    "multiscale_encoder = multiscale_encoder.to(device)\n",
    "\n",
    "## 5. Process Images and Extract Features\n",
    "\n",
    "# Set models to eval mode\n",
    "standard_encoder.eval()\n",
    "multiscale_encoder.eval()\n",
    "\n",
    "# Process sample images\n",
    "for i, img in enumerate(sample_images):\n",
    "    if img is None:\n",
    "        continue\n",
    "    \n",
    "    # Preprocess image\n",
    "    img_tensor = preprocess_image(img).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get features from standard encoder\n",
    "        standard_features = standard_encoder(img_tensor)\n",
    "        \n",
    "        # Get features from multi-scale encoder\n",
    "        multiscale_features = multiscale_encoder(img_tensor)\n",
    "        \n",
    "        # Print shapes for comparison\n",
    "        print(f\"\\nImage {i+1}:\")\n",
    "        print(f\"Standard features shape: {standard_features.shape}\")\n",
    "        print(f\"Multi-scale features shape: {multiscale_features.shape}\")\n",
    "\n",
    "## 6. Visualize Multi-Scale Attention\n",
    "\n",
    "def get_attention_maps(encoder, img_tensor):\n",
    "    \"\"\"Extract attention maps from the multi-scale encoder\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Forward pass to extract features\n",
    "        _ = encoder(img_tensor)\n",
    "        \n",
    "        # Access stored attention weights if available\n",
    "        if hasattr(encoder, 'deformable_attn') and hasattr(encoder.deformable_attn, 'last_attention_weights'):\n",
    "            attention_weights = encoder.deformable_attn.last_attention_weights\n",
    "            return attention_weights\n",
    "    return None\n",
    "\n",
    "# Process a sample image to visualize attention\n",
    "if sample_images and len(sample_images) > 0:\n",
    "    img = sample_images[0]  # Use the first image\n",
    "    img_tensor = preprocess_image(img).to(device)\n",
    "    \n",
    "    # Attempt to get attention maps\n",
    "    try:\n",
    "        # We need to modify our encoder to save attention weights\n",
    "        # For demonstration, we'll create dummy attention maps\n",
    "        attention_maps = []\n",
    "        \n",
    "        # In a real implementation, you would get actual attention maps from\n",
    "        # the model, but we'll create simulated ones for visualization\n",
    "        \n",
    "        # Simulated attention at different scales\n",
    "        h, w = img_tensor.shape[-2], img_tensor.shape[-1]\n",
    "        \n",
    "        # Scale 1: Fine details\n",
    "        attn_map1 = torch.rand(h//8, w//8).numpy()\n",
    "        \n",
    "        # Scale 2: Medium details\n",
    "        attn_map2 = torch.rand(h//16, w//16).numpy()\n",
    "        \n",
    "        # Scale 3: Coarse details/global attention\n",
    "        attn_map3 = torch.rand(h//32, w//32).numpy()\n",
    "        \n",
    "        attention_maps = [attn_map1, attn_map2, attn_map3]\n",
    "        show_attention_maps(\n",
    "            img, \n",
    "            attention_maps, \n",
    "            titles=[\"Fine Scale Attention\", \"Medium Scale Attention\", \"Coarse Scale Attention\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Could not visualize attention maps: {e}\")\n",
    "\n",
    "## 7. Compare Feature Richness\n",
    "\n",
    "def compare_feature_richness(standard_features, multiscale_features):\n",
    "    \"\"\"Compare the feature richness between standard and multi-scale features\"\"\"\n",
    "    # Feature statistics\n",
    "    std_mean = standard_features.mean().item()\n",
    "    std_std = standard_features.std().item()\n",
    "    std_min = standard_features.min().item()\n",
    "    std_max = standard_features.max().item()\n",
    "    \n",
    "    ms_mean = multiscale_features.mean().item()\n",
    "    ms_std = multiscale_features.std().item()\n",
    "    ms_min = multiscale_features.min().item()\n",
    "    ms_max = multiscale_features.max().item()\n",
    "    \n",
    "    # Plot feature distribution histograms\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(standard_features.flatten().cpu().numpy(), bins=50, alpha=0.7)\n",
    "    plt.title(f\"Standard Features\\nMean: {std_mean:.3f}, Std: {std_std:.3f}\")\n",
    "    plt.xlabel(\"Feature Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(multiscale_features.flatten().cpu().numpy(), bins=50, alpha=0.7)\n",
    "    plt.title(f\"Multi-scale Features\\nMean: {ms_mean:.3f}, Std: {ms_std:.3f}\")\n",
    "    plt.xlabel(\"Feature Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"Feature Statistics Comparison:\")\n",
    "    print(f\"{'':20s} {'Standard':15s} {'Multi-scale':15s}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"{'Mean':20s} {std_mean:15.5f} {ms_mean:15.5f}\")\n",
    "    print(f\"{'Std Dev':20s} {std_std:15.5f} {ms_std:15.5f}\")\n",
    "    print(f\"{'Min':20s} {std_min:15.5f} {ms_min:15.5f}\")\n",
    "    print(f\"{'Max':20s} {std_max:15.5f} {ms_max:15.5f}\")\n",
    "    print(f\"{'Range':20s} {std_max-std_min:15.5f} {ms_max-ms_min:15.5f}\")\n",
    "\n",
    "# Compare features for a sample image\n",
    "if sample_images and len(sample_images) > 0:\n",
    "    img = sample_images[0]  # Use the first image\n",
    "    img_tensor = preprocess_image(img).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        standard_features = standard_encoder(img_tensor)\n",
    "        multiscale_features = multiscale_encoder(img_tensor)\n",
    "        \n",
    "        # Compare feature richness\n",
    "        compare_feature_richness(standard_features, multiscale_features)\n",
    "\n",
    "## 8. Future Work and Improvements\n",
    "\n",
    "\"\"\"\n",
    "Future Improvements for the MultiScaleCLIPEncoder:\n",
    "\n",
    "1. Implement a more efficient deformable attention mechanism using \n",
    "   CUDA/C++ extensions for better performance\n",
    "\n",
    "2. Add more comprehensive visualization tools to better understand \n",
    "   how different scales contribute to the final features\n",
    "\n",
    "3. Fine-tune the model on specific downstream tasks to evaluate \n",
    "   the benefits of multi-scale features over standard features\n",
    "\n",
    "4. Experiment with different sampling strategies for the reference points\n",
    "   in the deformable attention mechanism\n",
    "\n",
    "5. Combine with other techniques like spatial pyramid pooling or\n",
    "   feature pyramid networks for enhanced multi-scale representation\n",
    "\n",
    "6. Integrate with detection heads to directly evaluate object detection\n",
    "   performance improvements\n",
    "\"\"\"\n",
    "\n",
    "# Usage example for the MultiScaleCLIPEncoder in an end-to-end pipeline\n",
    "def example_pipeline():\n",
    "    # 1. Load and preprocess image\n",
    "    img = sample_images[0] if sample_images else None\n",
    "    if img is None:\n",
    "        return\n",
    "    \n",
    "    img_tensor = preprocess_image(img).to(device)\n",
    "    \n",
    "    # 2. Extract multi-scale features\n",
    "    with torch.no_grad():\n",
    "        features = multiscale_encoder(img_tensor)\n",
    "    \n",
    "    # 3. Use features for downstream tasks\n",
    "    # (This would connect to your existing decoder or other models)\n",
    "    print(f\"Generated multi-scale features of shape {features.shape}\")\n",
    "    print(\"These features can now be passed to decoder_model for generation or detection tasks\")\n",
    "    \n",
    "    # Example: Features could now be passed to a decoder model\n",
    "    # outputs = decoder_model(features, ...)\n",
    "\n",
    "# Run example pipeline\n",
    "example_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
