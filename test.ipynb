{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class SwinBackboneMultiLevel(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", embed_dim=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            features_only=True,\n",
    "            out_indices=(1, 2, 3)\n",
    "        )\n",
    "        in_channels = self.backbone.feature_info.channels()\n",
    "        \n",
    "        # Project each feature map to same dim\n",
    "        self.proj_layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, embed_dim, kernel_size=1) for in_ch in in_channels\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        outs = {}\n",
    "        for i, (f, proj) in enumerate(zip(feats, self.proj_layers)):\n",
    "             # Ensure channels-first\n",
    "            if f.shape[-1] == self.backbone.feature_info.channels()[i]:\n",
    "                # f is [B, H, W, C], convert to [B, C, H, W]\n",
    "                f = f.permute(0, 3, 1, 2).contiguous()\n",
    "            outs[f\"C{i+3}\"] = proj(f)\n",
    "        return outs\n",
    "\n",
    "\n",
    "class FPNFusion(nn.Module):\n",
    "    def __init__(self, in_channels=256):\n",
    "        super().__init__()\n",
    "        self.smooth3 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.smooth4 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.smooth5 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, c3, c4, c5):\n",
    "        p5 = c5\n",
    "        p4 = c4 + F.interpolate(p5, size=c4.shape[-2:], mode='nearest')\n",
    "        p3 = c3 + F.interpolate(p4, size=c3.shape[-2:], mode='nearest')\n",
    "\n",
    "        p3 = self.smooth3(p3)\n",
    "        p4 = self.smooth4(p4)\n",
    "        p5 = self.smooth5(p5)\n",
    "        return {\"P3\": p3, \"P4\": p4, \"P5\": p5}\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "backbone = SwinBackboneMultiLevel(embed_dim=256).to(device)\n",
    "fpn = FPNFusion(in_channels=256).to(device)\n",
    "\n",
    "\n",
    "img = Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\") \n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "x = transform(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "x = x.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = backbone(x) \n",
    "    fpn_feats = fpn(feats[\"C3\"], feats[\"C4\"], feats[\"C5\"])  # P3, P4, P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_map(feat, title):\n",
    "    # feat: [B, C, H, W]\n",
    "    fmap = feat[0].mean(0).cpu()  # average over channels\n",
    "    plt.imshow(fmap, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Swin outputs\n",
    "# show_feature_map(feats[\"C3\"], \"C3 - Raw Swin\")\n",
    "show_feature_map(feats[\"C4\"], \"C4 - Raw Swin\")\n",
    "show_feature_map(feats[\"C5\"], \"C5 - Raw Swin\")\n",
    "\n",
    "# FPN fused outputs\n",
    "# show_feature_map(fpn_feats[\"P3\"], \"P3 - FPN Fusion\")\n",
    "# show_feature_map(fpn_feats[\"P4\"], \"P4 - FPN Fusion\")\n",
    "show_feature_map(fpn_feats[\"P5\"], \"P5 - FPN Fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"grounding_dino_tiny_local\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "vocab_size = len(tokenizer)\n",
    "print (\"Before\")\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "embed_layer =  model.model.text_backbone.get_input_embeddings()\n",
    "print(\"Embedding layer size:\", embed_layer.weight.size())\n",
    "\n",
    "new_tokens = [\"button\", \"flexcontainer\"]\n",
    "added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(\"Added tokens:\", added_tokens)\n",
    "\n",
    "if added_tokens > 0:\n",
    "    model.model.text_backbone.resize_token_embeddings(len(tokenizer))\n",
    "print (\"After\")\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "embed_layer = model.model.text_backbone.get_input_embeddings()\n",
    "print(\"Embedding layer size:\", embed_layer.weight.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, GroundingDinoForObjectDetection\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "# Print submodules\n",
    "for name, module in model.named_modules():\n",
    "    if \"text\" in name.lower() or \"lang\" in name.lower():\n",
    "        print(name, type(module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_id = tokenizer.encode(\"flexcontainer\")\n",
    "tokens_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\")\n",
    "text = [\"a ceramic mug on the table\"]\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from transformers import GroundingDinoConfig\n",
    "import torch \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "# # Load model directly\n",
    "# from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "# model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "\n",
    "\n",
    "# # load default config\n",
    "config = GroundingDinoConfig()\n",
    "\n",
    "# # override hidden dimension\n",
    "# config.d_model = 256  # example: change to 512 instead of 256\n",
    "\n",
    "\n",
    "# # optionally adjust other linked dimensions:\n",
    "# # e.g., if number of heads n_head is set, ensure d_model % n_head == 0\n",
    "# config.encoder_attention_heads = 16\n",
    "# config.decoder_attention_heads = 16\n",
    "# # Possibly adjust feedforward size:\n",
    "# config.encoder_ffn_dim = 2048\n",
    "# config.decoder_ffn_dim = 2048\n",
    "\n",
    "# model = GroundingDinoForObjectDetection()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# # For example override defaults:\n",
    "# processor.do_resize = True                # whether to resize\n",
    "# processor.size = {\"height\": 800,           # set target height\n",
    "#                   \"width\": 1333}           # set target width\n",
    "# processor.do_normalize = True              # whether to normalize pixel values\n",
    "# processor.image_mean = [0.485, 0.456, 0.406]\n",
    "# processor.image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "tokenizer = processor.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05960aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection, GroundingDinoConfig\n",
    "import pandas as pd\n",
    "import os \n",
    "import ast \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "with open(\"label_dict.json\", \"r\") as f:\n",
    "    label_dict = json.loads(f.read()) \n",
    "\n",
    "# Raw COCO id -> name mapping (already in file)\n",
    "id_to_name = label_dict[\"label_dict\"]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])  # contains bbox, image_id, category_id\n",
    "\n",
    "coco_images_df = pd.DataFrame(coco_ann['images'])  \n",
    "\n",
    "coco_grouped = coco_df.groupby(\"image_id\")\n",
    "\n",
    "# Load semantic model once globally\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "class RefCOCODataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, processor, dataset_split=\"train\", auto_add_tokens=False):\n",
    "        self.df = pd.read_parquet('coco_ref/ref_coco_train.parquet')\n",
    "        self.df[\"img_path\"] = self.df[\"image_id\"].apply(lambda x: os.path.join(\"train2017\", f\"{x:012d}.jpg\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.auto_add_tokens = auto_add_tokens\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_phrases_from_text(self, text: str) -> list[str]:\n",
    "        from keybert import KeyBERT\n",
    "        model = KeyBERT('all-MiniLM-L6-v2')\n",
    "        candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "        print (\"candidates\", candidates)\n",
    "        if not candidates:\n",
    "            return None \n",
    "        return [c[0] for c in candidates][0]\n",
    "\n",
    "\n",
    "    def xyxy_to_cxcywh(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert (x1, y1, x2, y2) to (cx, cy, w, h)\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "    def normalize_boxes(self, boxes: torch.Tensor, W: int, H: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize (cx, cy, w, h) to [0,1] range\n",
    "        \"\"\"\n",
    "        cx, cy, w, h = boxes.unbind(-1)\n",
    "        cx = cx / W\n",
    "        cy = cy / H\n",
    "        w = w / W\n",
    "        h = h / H\n",
    "        return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "    def build_positive_map(self, text: str, phrases: list[str], tokenizer) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "        aligning each phrase with the tokens that describe it.\n",
    "        \"\"\"\n",
    "        # Tokenize with offsets\n",
    "        encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        \n",
    "        #  Fix: offsets are nested under [0] since tokenizer returns batch dim\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        if isinstance(offsets[0], list):\n",
    "            offsets = offsets[0]  # shape: (num_tokens, 2)\n",
    "\n",
    "        num_tokens = len(offsets)\n",
    "        num_phrases = len(phrases)\n",
    "        positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "        # Normalize text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            phrase = phrase.strip().lower()\n",
    "            start_char = text_lower.find(phrase)\n",
    "            if start_char == -1:\n",
    "                continue\n",
    "            end_char = start_char + len(phrase)\n",
    "\n",
    "            # Iterate over token spans\n",
    "            for j, span in enumerate(offsets):\n",
    "                # Some tokenizers return (0, 0) for [CLS]/[SEP]\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "\n",
    "        return positive_map\n",
    "\n",
    "\n",
    "    def match_bbox(self, ref_bbox, coco_bboxes):\n",
    "        \"\"\"\n",
    "        ref_bbox: [x, y, w, h]\n",
    "        coco_bboxes: DataFrame with columns ['bbox', 'category_id']\n",
    "        returns category_id\n",
    "        \"\"\"\n",
    "        # convert to numpy arrays\n",
    "        ref_bbox = np.array(ref_bbox, dtype=np.float32)\n",
    "        \n",
    "        for _, row in coco_bboxes.iterrows():\n",
    "            coco_bbox = np.array(row['bbox'], dtype=np.float32)\n",
    "            # check if IoU > threshold or exact match\n",
    "            if np.allclose(ref_bbox, coco_bbox, atol=2.0):  # allow small rounding differences\n",
    "                return int(row['category_id'])\n",
    "        # fallback\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def get_category_id(self, image_id, ref_bbox):\n",
    "        category_id = 1\n",
    "        if image_id in coco_grouped.groups:\n",
    "            coco_bboxes = coco_grouped.get_group(image_id)\n",
    "            category_id = self.match_bbox(ref_bbox, coco_bboxes)\n",
    "\n",
    "        else:\n",
    "            print (\"Image not Found\")\n",
    "        \n",
    "        if category_id:\n",
    "            return category_id\n",
    "        \n",
    "        return 0 \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row\n",
    "        ex = self.df.iloc[idx]\n",
    "\n",
    "        image_id = ex[\"image_id\"]\n",
    "\n",
    "        img = Image.open(ex[\"img_path\"]).convert(\"RGB\")\n",
    "        # img = transform(img)\n",
    "\n",
    "        bbox_list = ex[\"bbox\"]\n",
    "\n",
    "        bbox  =  [float(np.float64(float(bbox))) for bbox in bbox_list]\n",
    "\n",
    "        # bbox = ast.literal_eval(bbox)  # [x, y, w, h]\n",
    "\n",
    "        raw_sentences = ast.literal_eval(ex[\"raw_sentences\"])\n",
    "\n",
    "        text = \". \".join([raw_text[\"raw\"].lower() for raw_text in raw_sentences])\n",
    "\n",
    "        category_id = str(int(ex['category_id']))\n",
    "\n",
    "        phrases = [] \n",
    "\n",
    "        for raw_text in raw_sentences:\n",
    "            if random.random() < 0.5:\n",
    "                res = self.get_phrases_from_text(raw_text[\"raw\"].lower()) \n",
    "                if res:\n",
    "                    phrases.append(res)\n",
    "                else:\n",
    "                    phrases.append(id_to_name[category_id])\n",
    "            else:\n",
    "                phrases.append(id_to_name[category_id])\n",
    "\n",
    "        class_labels = [index for index in range(len(phrases))]\n",
    "\n",
    "        bbox_final = [bbox]*len(phrases)\n",
    "\n",
    "        print (\"=*=\"*70)\n",
    "        print (\"Text:\", text)\n",
    "        print (\"Phrases:\", phrases)\n",
    "        print (\"Class Labels:\", class_labels)\n",
    "        print (\"BBoxes:\", bbox_final)\n",
    "\n",
    "\n",
    "        positive_map = self.build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "       \n",
    "        print (\"Positive Map:\", positive_map)\n",
    "        print (\"=*=\"*70)\n",
    "\n",
    "        # Optionally extend tokenizer on the fly\n",
    "        if self.auto_add_tokens:\n",
    "            tokens = text.split()\n",
    "            new_words = [t for t in tokens if t not in self.tokenizer.get_vocab()]\n",
    "            if new_words:\n",
    "                print (new_words)\n",
    "                LLLL\n",
    "                added = self.tokenizer.add_tokens(new_words)\n",
    "                if added > 0:\n",
    "                    model.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "                    print(f\" Added {added} new tokens:\", new_words)\n",
    "\n",
    "\n",
    "        return {\"image\": img, \"text\": text, \"bbox\": bbox_final, \"class_labels\": class_labels, \"positive_map\": positive_map, \"phrases\":phrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b731608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates [('left bear', 1.0)]\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "Text: left bear. left bear. left bear\n",
      "Phrases: ['left bear', 'bear', 'bear']\n",
      "Class Labels: [0, 1, 2]\n",
      "BBoxes: [[245.38, 159.23, 446.64, 422.90999999999997], [245.38, 159.23, 446.64, 422.90999999999997], [245.38, 159.23, 446.64, 422.90999999999997]]\n",
      "Positive Map: tensor([[False,  True,  True, False, False, False, False, False, False, False],\n",
      "        [False, False,  True, False, False, False, False, False, False, False],\n",
      "        [False, False,  True, False, False, False, False, False, False, False]])\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "candidates []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m data_loader = DataLoader(dataset, batch_size=\u001b[32m2\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn=collate_fn)\n\u001b[32m     33\u001b[39m model.model.text_backbone.resize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43menc\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m==== enc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/multimodel/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mRefCOCODataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m raw_text \u001b[38;5;129;01min\u001b[39;00m raw_sentences:\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m random.random() < \u001b[32m0.5\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m         phrases.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_phrases_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    183\u001b[39m         phrases.append(id_to_name[category_id])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mRefCOCODataset.get_phrases_from_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     56\u001b[39m candidates = model.extract_keywords(text, keyphrase_ngram_range=(\u001b[32m2\u001b[39m,\u001b[32m4\u001b[39m), stop_words=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m, top_n=\u001b[32m1\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m\"\u001b[39m\u001b[33mcandidates\u001b[39m\u001b[33m\"\u001b[39m, candidates)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    texts  = [item[\"text\"] for item in batch]\n",
    "\n",
    "    encodings = processor(images=images, text=texts, return_tensors=\"pt\", padding=True, truncation=True, size={\"shortest_edge\": 800, \"longest_edge\": 1333})\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        if isinstance(item[\"class_labels\"], list):\n",
    "            class_labels = torch.tensor(item[\"class_labels\"], dtype=torch.long)\n",
    "            boxes = torch.tensor(item[\"bbox\"], dtype=torch.float32)\n",
    "            positive_map = item[\"positive_map\"]\n",
    "        else:\n",
    "            class_labels = torch.tensor([item[\"class_labels\"]], dtype=torch.long)\n",
    "            boxes = torch.tensor([item[\"bbox\"]], dtype=torch.float32)\n",
    "            positive_map = item[\"positive_map\"]\n",
    "\n",
    "\n",
    "        \n",
    "        labels.append({\n",
    "            \"class_labels\": class_labels.to(device) if torch.is_tensor(class_labels) else class_labels,\n",
    "            \"boxes\": [box.to(device) for box in boxes] if isinstance(boxes, list) else boxes.to(device),\n",
    "            \"positive_map\":positive_map.to(device)\n",
    "        })\n",
    "   \n",
    "    return encodings, labels \n",
    "\n",
    "\n",
    "dataset = RefCOCODataset(tokenizer, processor, dataset_split=\"train\", auto_add_tokens=False)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model.model.text_backbone.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "for batch in data_loader:\n",
    "   \n",
    "    enc , labels = batch[0], batch[1]\n",
    "\n",
    "    print (\"==== enc\", enc)\n",
    "    print (\"==== labels\", labels)\n",
    "\n",
    "    enc = {k: v.to(device) if torch.is_tensor(v) else v for k, v in enc.items()}\n",
    "\n",
    "    outputs = model(**enc, labels=labels)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11578366",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Suppose two images:\n",
    "images = [\n",
    "    Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\"),\n",
    "    Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/dog_and_person.jpg\").convert(\"RGB\"),\n",
    "\n",
    "]\n",
    "\n",
    "texts = [[\"number 2\"], [\"flexcontainer\"]]\n",
    "\n",
    "inputs = processor(\n",
    "    images=images,\n",
    "    text=texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebc7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = box.cpu() \n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0334d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    threshold=0.3,\n",
    "    target_sizes=[img.size[::-1] for img in images]\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"boxes\"], result[\"labels\"], result[\"scores\"])\n",
    "\n",
    "\n",
    "output = results[0]\n",
    "\n",
    "#output = {key:[val_list[0:2]]for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(images[0], output[\"boxes\"], output[\"labels\"], output[\"scores\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654755c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape # [batch_size, num_queries, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa319716",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "# convert list of annotations to DataFrame\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87905a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])  # contains bbox, image_id, category_id\n",
    "\n",
    "coco_images_df = pd.DataFrame(coco_ann['images'])  \n",
    "\n",
    "coco_grouped = coco_df.groupby(\"image_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load semantic model once globally\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def build_positive_map(text: str, phrases: list[str], tokenizer, semantic_threshold: float = 0.6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "    aligning each phrase with tokens that describe it, using both\n",
    "    literal substring and semantic similarity matching.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    if isinstance(offsets[0], list):\n",
    "        offsets = offsets[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "    num_tokens = len(tokens)\n",
    "    num_phrases = len(phrases)\n",
    "    positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "    # Normalize text for literal matching\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Precompute semantic embeddings\n",
    "    token_embs = semantic_model.encode(tokens, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    phrase_embs = semantic_model.encode(phrases, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sim = util.cos_sim(phrase_embs, token_embs)  # (num_phrases, num_tokens)\n",
    "\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        phrase_lower = phrase.strip().lower()\n",
    "        start_char = text_lower.find(phrase_lower)\n",
    "        if start_char != -1:\n",
    "            end_char = start_char + len(phrase_lower)\n",
    "            for j, span in enumerate(offsets):\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "        else:\n",
    "            # Use semantic similarity if no direct text match\n",
    "            positive_map[i] = sim[i] > semantic_threshold\n",
    "\n",
    "    return positive_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# def xywh_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Convert bounding boxes from (x, y, w, h) format to (cx, cy, w, h) format.\n",
    "    \n",
    "#     Args:\n",
    "#         boxes (torch.Tensor): Tensor of shape (N, 4) with boxes in [x, y, w, h]\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Tensor of shape (N, 4) with boxes in [cx, cy, w, h]\n",
    "#     \"\"\"\n",
    "#     x, y, w, h = boxes.unbind(-1)\n",
    "#     cx = x + w / 2\n",
    "#     cy = y + h / 2\n",
    "#     return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def xyxy_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert (x1, y1, x2, y2) to (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def normalize_boxes(boxes: torch.Tensor, W: int, H: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize (cx, cy, w, h) to [0,1] range\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    cx = cx / W\n",
    "    cy = cy / H\n",
    "    w = w / W\n",
    "    h = h / H\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "def scale_boxes_to_encoder(boxes: torch.Tensor, W: int, H: int, W_proc: int, H_proc: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Scale boxes from original image to encoder input size.\n",
    "    Works for boxes in (cx, cy, w, h) format.\n",
    "    \n",
    "    Args:\n",
    "        boxes (torch.Tensor): Tensor of shape (N, 4) in [cx, cy, w, h]\n",
    "        W, H: Original image width and height\n",
    "        W_proc, H_proc: Encoder processed image width and height\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled boxes in [cx, cy, w, h] format\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    cx = cx * W_proc / W\n",
    "    cy = cy * H_proc / H\n",
    "    w = w * W_proc / W\n",
    "    h = h * H_proc / H\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# norm_boxes = torch.tensor([[50, 100, 200, 300]], dtype=torch.float32)  # x, y, w, h\n",
    "# cxcy_boxes = xywh_to_cxcywh(norm_boxes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40274f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "\n",
    "image = Image.open(\"/Users/preetamverma/Desktop/multimodel/train2017/000000000025.jpg\").convert(\"RGB\")\n",
    "text = \"giraffe. giraffe standing in the grass\"\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = processor.tokenizer \n",
    "\n",
    "enc = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "pixel_values = enc[\"pixel_values\"]\n",
    "\n",
    "\n",
    "_, _, H_proc, W_proc = pixel_values.shape\n",
    "\n",
    "\n",
    "phrases = [\"giraffe\", \"giraffe standing on the grass\"]\n",
    "\n",
    "positive_map = build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "print (positive_map)\n",
    "abs_boxes = []\n",
    "\n",
    "Org_Image_Width, Org_Image_Height = image.size\n",
    "\n",
    "norm_boxes = torch.tensor([[320, 30, 814, 530], [300, 30, 700, 430]], dtype=torch.float32)  # x, y, w, h\n",
    "cxcy_boxes = xyxy_to_cxcywh(norm_boxes)\n",
    "abs_boxes = normalize_boxes(cxcy_boxes, Org_Image_Width, Org_Image_Height)\n",
    "\n",
    "\n",
    "#abs_boxes = torch.tensor(abs_boxes, dtype=torch.float32)\n",
    "\n",
    "print (\"=\\n\\t abs_boxes\\t\\t\\t\",abs_boxes)\n",
    "\n",
    "\n",
    "labels = [{\n",
    "    \"class_labels\": torch.tensor([0, 1], dtype=torch.long),\n",
    "    \"boxes\": abs_boxes,\n",
    "    \"positive_map\": positive_map,\n",
    "}]\n",
    "\n",
    "outputs = model(**enc, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "print (loss)\n",
    "\n",
    "\n",
    "# ---- Visualization ----\n",
    "draw = ImageDraw.Draw(image)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "for i, (box, label_idx) in enumerate(zip(norm_boxes, labels[0][\"class_labels\"])):\n",
    "    x0, y0, x1, y1 = box.tolist()\n",
    "    color = \"red\" if i == 0 else \"blue\"\n",
    "    label_text = phrases[label_idx]\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=color, width=4)\n",
    "    draw.text((x0, y0 - 10), label_text, fill=color, font=font)\n",
    "\n",
    "image.show()\n",
    "\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    enc.input_ids,\n",
    "    threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n",
    "print (\"=*=\"*70)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i}: {result['boxes']}, {result['labels']}, {result['scores']}\")\n",
    "print (\"=*=\"*70)\n",
    "\n",
    "\n",
    "output = results[0]\n",
    "\n",
    "#output = {key:[val_list[0]] for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(image, output[\"boxes\"].detach().numpy(), output[\"labels\"], output[\"scores\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093eac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([200, 300, 400, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Human\"), tokenizer.encode(\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_phrases_from_text(text: str) -> list[str]:\n",
    "   from keybert import KeyBERT\n",
    "   model = KeyBERT('all-MiniLM-L6-v2')\n",
    "   candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "   return [c[0] for c in candidates]\n",
    "\n",
    "\n",
    "text = \"A person riding a horse in a field near a river.\"\n",
    "if random.random() < 0.5:\n",
    "    phrases = get_phrases_from_text(text)\n",
    "else:\n",
    "    phrases = None\n",
    "\n",
    "print(\"Extracted Phrases:\", phrases)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14259b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57d7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
