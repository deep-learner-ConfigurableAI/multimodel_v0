{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class SwinBackboneMultiLevel(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", embed_dim=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            features_only=True,\n",
    "            out_indices=(1, 2, 3)\n",
    "        )\n",
    "        in_channels = self.backbone.feature_info.channels()\n",
    "        \n",
    "        # Project each feature map to same dim\n",
    "        self.proj_layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, embed_dim, kernel_size=1) for in_ch in in_channels\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        outs = {}\n",
    "        for i, (f, proj) in enumerate(zip(feats, self.proj_layers)):\n",
    "             # Ensure channels-first\n",
    "            if f.shape[-1] == self.backbone.feature_info.channels()[i]:\n",
    "                # f is [B, H, W, C], convert to [B, C, H, W]\n",
    "                f = f.permute(0, 3, 1, 2).contiguous()\n",
    "            outs[f\"C{i+3}\"] = proj(f)\n",
    "        return outs\n",
    "\n",
    "\n",
    "class FPNFusion(nn.Module):\n",
    "    def __init__(self, in_channels=256):\n",
    "        super().__init__()\n",
    "        self.smooth3 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.smooth4 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.smooth5 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, c3, c4, c5):\n",
    "        p5 = c5\n",
    "        p4 = c4 + F.interpolate(p5, size=c4.shape[-2:], mode='nearest')\n",
    "        p3 = c3 + F.interpolate(p4, size=c3.shape[-2:], mode='nearest')\n",
    "\n",
    "        p3 = self.smooth3(p3)\n",
    "        p4 = self.smooth4(p4)\n",
    "        p5 = self.smooth5(p5)\n",
    "        return {\"P3\": p3, \"P4\": p4, \"P5\": p5}\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "backbone = SwinBackboneMultiLevel(embed_dim=256).to(device)\n",
    "fpn = FPNFusion(in_channels=256).to(device)\n",
    "\n",
    "\n",
    "img = Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\") \n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "x = transform(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "x = x.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = backbone(x) \n",
    "    fpn_feats = fpn(feats[\"C3\"], feats[\"C4\"], feats[\"C5\"])  # P3, P4, P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_map(feat, title):\n",
    "    # feat: [B, C, H, W]\n",
    "    fmap = feat[0].mean(0).cpu()  # average over channels\n",
    "    plt.imshow(fmap, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Swin outputs\n",
    "# show_feature_map(feats[\"C3\"], \"C3 - Raw Swin\")\n",
    "show_feature_map(feats[\"C4\"], \"C4 - Raw Swin\")\n",
    "show_feature_map(feats[\"C5\"], \"C5 - Raw Swin\")\n",
    "\n",
    "# FPN fused outputs\n",
    "# show_feature_map(fpn_feats[\"P3\"], \"P3 - FPN Fusion\")\n",
    "# show_feature_map(fpn_feats[\"P4\"], \"P4 - FPN Fusion\")\n",
    "show_feature_map(fpn_feats[\"P5\"], \"P5 - FPN Fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"grounding_dino_tiny_local\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "vocab_size = len(tokenizer)\n",
    "print (\"Before\")\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "embed_layer =  model.model.text_backbone.get_input_embeddings()\n",
    "print(\"Embedding layer size:\", embed_layer.weight.size())\n",
    "\n",
    "new_tokens = [\"button\", \"flexcontainer\"]\n",
    "added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(\"Added tokens:\", added_tokens)\n",
    "\n",
    "if added_tokens > 0:\n",
    "    model.model.text_backbone.resize_token_embeddings(len(tokenizer))\n",
    "print (\"After\")\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "embed_layer = model.model.text_backbone.get_input_embeddings()\n",
    "print(\"Embedding layer size:\", embed_layer.weight.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, GroundingDinoForObjectDetection\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "# Print submodules\n",
    "for name, module in model.named_modules():\n",
    "    if \"text\" in name.lower() or \"lang\" in name.lower():\n",
    "        print(name, type(module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_id = tokenizer.encode(\"flexcontainer\")\n",
    "tokens_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\")\n",
    "text = [\"a ceramic mug on the table\"]\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c00b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from transformers import GroundingDinoConfig\n",
    "import torch \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "# # Load model directly\n",
    "# from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "# model = AutoModelForZeroShotObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n",
    "\n",
    "\n",
    "# # load default config\n",
    "config = GroundingDinoConfig()\n",
    "\n",
    "# # override hidden dimension\n",
    "# config.d_model = 256  # example: change to 512 instead of 256\n",
    "\n",
    "\n",
    "# # optionally adjust other linked dimensions:\n",
    "# # e.g., if number of heads n_head is set, ensure d_model % n_head == 0\n",
    "# config.encoder_attention_heads = 16\n",
    "# config.decoder_attention_heads = 16\n",
    "# # Possibly adjust feedforward size:\n",
    "# config.encoder_ffn_dim = 2048\n",
    "# config.decoder_ffn_dim = 2048\n",
    "\n",
    "# model = GroundingDinoForObjectDetection()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# # For example override defaults:\n",
    "# processor.do_resize = True                # whether to resize\n",
    "# processor.size = {\"height\": 800,           # set target height\n",
    "#                   \"width\": 1333}           # set target width\n",
    "# processor.do_normalize = True              # whether to normalize pixel values\n",
    "# processor.image_mean = [0.485, 0.456, 0.406]\n",
    "# processor.image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "import os \n",
    "\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a65912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection, GroundingDinoConfig\n",
    "import pandas as pd\n",
    "import os \n",
    "import ast \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "class OSAtlasRefDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Converts OS-Atlas GUI grounding dataset to GroundingDINO format.\n",
    "    Each image may contain multiple referring expressions and bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path, image_root, tokenizer, \n",
    "                 split=\"train\", auto_add_tokens=False):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.auto_add_tokens = auto_add_tokens\n",
    "        self.image_root = image_root\n",
    "\n",
    "        # Load raw JSON\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        # Flatten image + elements into one dataframe\n",
    "        records = []\n",
    "        for item in self.data:\n",
    "            img_path = os.path.join(image_root, item[\"img_filename\"])\n",
    "            for el in item[\"elements\"]:\n",
    "                records.append({\n",
    "                    \"img_path\": img_path,\n",
    "                    \"instruction\": el[\"instruction\"].strip(),\n",
    "                    \"bbox\": el[\"bbox\"]\n",
    "                })\n",
    "        self.df = pd.DataFrame(records)\n",
    "        print(f\" Loaded {len(self.df)} referring expressions from {len(self.data)} images\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_phrases_from_text(self, text: str) -> list[str]:\n",
    "        from keybert import KeyBERT\n",
    "        model = KeyBERT('all-MiniLM-L6-v2')\n",
    "        candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "        print (\"candidates\", candidates)\n",
    "        if not candidates:\n",
    "            return None \n",
    "        return [c[0] for c in candidates][0]\n",
    "\n",
    "    \n",
    "    def build_positive_map(self, text: str, phrases: list[str], tokenizer) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "        aligning each phrase with the tokens that describe it.\n",
    "        \"\"\"\n",
    "        # Tokenize with offsets\n",
    "        encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        \n",
    "        #  Fix: offsets are nested under [0] since tokenizer returns batch dim\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        if isinstance(offsets[0], list):\n",
    "            offsets = offsets[0]  # shape: (num_tokens, 2)\n",
    "\n",
    "        num_tokens = len(offsets)\n",
    "        num_phrases = len(phrases)\n",
    "        positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "        # Normalize text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            phrase = phrase.strip().lower()\n",
    "            start_char = text_lower.find(phrase)\n",
    "            if start_char == -1:\n",
    "                continue\n",
    "            end_char = start_char + len(phrase)\n",
    "\n",
    "            # Iterate over token spans\n",
    "            for j, span in enumerate(offsets):\n",
    "                # Some tokenizers return (0, 0) for [CLS]/[SEP]\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "\n",
    "        return positive_map\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"img_path\"]\n",
    "        text = row[\"instruction\"]\n",
    "        bbox = row[\"bbox\"]  # normalized xyxy\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "        bbox_final = [bbox]\n",
    "\n",
    "        category_id = self.get_phrases_from_text(text.lower()) \n",
    "\n",
    "        phrases = []\n",
    "        phrases.append(category_id)\n",
    "        \n",
    "        class_labels = [index for index in range(len(phrases))]\n",
    "\n",
    "        positive_map = self.build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "        return {\"image\": image, \"text\": text, \"bbox\": bbox_final, \"class_labels\": class_labels, \"positive_map\": positive_map, \"phrases\":phrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7169c4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 16660 referring expressions from 5682 images\n",
      "candidates [('text right place face', 0.865)]\n",
      "select the text which is to the right of place my face\n",
      "[[0.806, 0.024, 0.988, 0.071]]\n",
      "<PIL.Image.Image image mode=RGB size=1080x1920 at 0x36989D340>\n",
      "[0]\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False]])\n",
      "['text right place face']\n"
     ]
    }
   ],
   "source": [
    "dataset = OSAtlasRefDataset(\n",
    "    json_path=\"/Users/preetamverma/Downloads/uibert_raw.json\",  \n",
    "    image_root=\"/Users/preetamverma/Downloads\",      \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "sample = dataset[1]\n",
    "print(sample[\"text\"])\n",
    "print(sample[\"bbox\"])\n",
    "print(sample[\"image\"])\n",
    "print (sample[\"class_labels\"])\n",
    "print (sample[\"positive_map\"])\n",
    "print (sample[\"phrases\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a41c8703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image'] [3] tensor([[False, False, False, False,  True, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer\n",
    "import re \n",
    "\n",
    "\n",
    "UI_VOCAB = [\n",
    "    \"button\", \"icon\", \"text\", \"image\", \"input\", \"checkbox\", \"link\",\n",
    "    \"menu item\", \"banner\", \"avatar\", \"logo\", \"label\", \"switch\",\n",
    "    \"tab\", \"card\", \"popup\", \"dropdown\", \"textfield\", \"container\"\n",
    "]\n",
    "\n",
    "UI_TEXT_MAP = {\n",
    "    \"image\": [\"picture\", \"photo\", \"avatar\", \"logo\", \"icon\"],\n",
    "    \"button\": [\"button\", \"tap\", \"click\", \"submit\"],\n",
    "    \"text\": [\"text\", \"label\"],\n",
    "    \"input\": [\"input\", \"field\", \"search\", \"textbox\"],\n",
    "}\n",
    "\n",
    "\n",
    "class UIElementClassifier:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.vocab_emb = self.model.encode(vocab, normalize_embeddings=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "    def build_positive_map(self, text: str, phrases: list[str]) -> torch.Tensor:\n",
    "        offsets = self.tokenizer(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "        positive_map = torch.zeros((len(phrases), len(offsets)), dtype=torch.bool)\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            candidates = UI_TEXT_MAP.get(phrase, [phrase])\n",
    "            for cand in candidates:\n",
    "                for match in re.finditer(re.escape(cand), text_lower):\n",
    "                    start, end = match.span()\n",
    "                    for j, (s_tok, e_tok) in enumerate(offsets):\n",
    "                        if e_tok > start and s_tok < end:\n",
    "                            positive_map[i, j] = True\n",
    "        return positive_map\n",
    "\n",
    "\n",
    "    def classify(self, text):\n",
    "        \"\"\"Return best-matching UI term for the referring expression.\"\"\"\n",
    "        query_emb = self.model.encode(text, normalize_embeddings=True)\n",
    "        sim = util.cos_sim(query_emb, self.vocab_emb)[0]\n",
    "        idx = torch.argmax(sim).item()\n",
    "        return self.vocab[idx], sim[idx].item()\n",
    "\n",
    "\n",
    "ui_classifier = UIElementClassifier(UI_VOCAB)\n",
    "\n",
    "text = \"Select the profile picture in the top bar\"\n",
    "\n",
    "ui_label, score = ui_classifier.classify(text.lower())\n",
    "phrases = [ui_label]\n",
    "\n",
    "positive_map = ui_classifier.build_positive_map(text, phrases)\n",
    "\n",
    "class_labels = [UI_VOCAB.index(ui_label)]\n",
    "\n",
    "\n",
    "print (phrases, class_labels, positive_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05960aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection, GroundingDinoConfig\n",
    "import pandas as pd\n",
    "import os \n",
    "import ast \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "with open(\"label_dict.json\", \"r\") as f:\n",
    "    label_dict = json.loads(f.read()) \n",
    "\n",
    "# Raw COCO id -> name mapping (already in file)\n",
    "id_to_name = label_dict[\"label_dict\"]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])  # contains bbox, image_id, category_id\n",
    "\n",
    "coco_images_df = pd.DataFrame(coco_ann['images'])  \n",
    "\n",
    "coco_grouped = coco_df.groupby(\"image_id\")\n",
    "\n",
    "# Load semantic model once globally\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "class RefCOCODataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, processor, dataset_split=\"train\", auto_add_tokens=False):\n",
    "        self.df = pd.read_parquet('coco_ref/ref_coco_train.parquet')\n",
    "        self.df[\"img_path\"] = self.df[\"image_id\"].apply(lambda x: os.path.join(\"train2017\", f\"{x:012d}.jpg\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.auto_add_tokens = auto_add_tokens\n",
    "        self.ui_classifier = UIElementClassifier(UI_VOCAB)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_phrases_from_text(self, text: str) -> list[str]:\n",
    "        from keybert import KeyBERT\n",
    "        model = KeyBERT('all-MiniLM-L6-v2')\n",
    "        candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "        print (\"candidates\", candidates)\n",
    "        if not candidates:\n",
    "            return None \n",
    "        return [c[0] for c in candidates][0]\n",
    "\n",
    "\n",
    "    def xyxy_to_cxcywh(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert (x1, y1, x2, y2) to (cx, cy, w, h)\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "    def normalize_boxes(self, boxes: torch.Tensor, W: int, H: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize (cx, cy, w, h) to [0,1] range\n",
    "        \"\"\"\n",
    "        cx, cy, w, h = boxes.unbind(-1)\n",
    "        cx = cx / W\n",
    "        cy = cy / H\n",
    "        w = w / W\n",
    "        h = h / H\n",
    "        return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "    def build_positive_map(self, text: str, phrases: list[str], tokenizer) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "        aligning each phrase with the tokens that describe it.\n",
    "        \"\"\"\n",
    "        # Tokenize with offsets\n",
    "        encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        \n",
    "        #  Fix: offsets are nested under [0] since tokenizer returns batch dim\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        if isinstance(offsets[0], list):\n",
    "            offsets = offsets[0]  # shape: (num_tokens, 2)\n",
    "\n",
    "        num_tokens = len(offsets)\n",
    "        num_phrases = len(phrases)\n",
    "        positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "        # Normalize text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            phrase = phrase.strip().lower()\n",
    "            start_char = text_lower.find(phrase)\n",
    "            if start_char == -1:\n",
    "                continue\n",
    "            end_char = start_char + len(phrase)\n",
    "\n",
    "            # Iterate over token spans\n",
    "            for j, span in enumerate(offsets):\n",
    "                # Some tokenizers return (0, 0) for [CLS]/[SEP]\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "\n",
    "        return positive_map\n",
    "\n",
    "\n",
    "    def match_bbox(self, ref_bbox, coco_bboxes):\n",
    "        \"\"\"\n",
    "        ref_bbox: [x, y, w, h]\n",
    "        coco_bboxes: DataFrame with columns ['bbox', 'category_id']\n",
    "        returns category_id\n",
    "        \"\"\"\n",
    "        # convert to numpy arrays\n",
    "        ref_bbox = np.array(ref_bbox, dtype=np.float32)\n",
    "        \n",
    "        for _, row in coco_bboxes.iterrows():\n",
    "            coco_bbox = np.array(row['bbox'], dtype=np.float32)\n",
    "            # check if IoU > threshold or exact match\n",
    "            if np.allclose(ref_bbox, coco_bbox, atol=2.0):  # allow small rounding differences\n",
    "                return int(row['category_id'])\n",
    "        # fallback\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def get_category_id(self, image_id, ref_bbox):\n",
    "        category_id = 1\n",
    "        if image_id in coco_grouped.groups:\n",
    "            coco_bboxes = coco_grouped.get_group(image_id)\n",
    "            category_id = self.match_bbox(ref_bbox, coco_bboxes)\n",
    "\n",
    "        else:\n",
    "            print (\"Image not Found\")\n",
    "        \n",
    "        if category_id:\n",
    "            return category_id\n",
    "        \n",
    "        return 0 \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row\n",
    "        ex = self.df.iloc[idx]\n",
    "\n",
    "        image_id = ex[\"image_id\"]\n",
    "\n",
    "        img = Image.open(ex[\"img_path\"]).convert(\"RGB\")\n",
    "        # img = transform(img)\n",
    "\n",
    "        bbox_list = ex[\"bbox\"]\n",
    "\n",
    "        bbox  =  [float(np.float64(float(bbox))) for bbox in bbox_list]\n",
    "\n",
    "        # bbox = ast.literal_eval(bbox)  # [x, y, w, h]\n",
    "\n",
    "        raw_sentences = ast.literal_eval(ex[\"raw_sentences\"])\n",
    "\n",
    "        text = \". \".join([raw_text[\"raw\"].lower() for raw_text in raw_sentences])\n",
    "\n",
    "        category_id = str(int(ex['category_id']))\n",
    "\n",
    "        phrases = [] \n",
    "\n",
    "        for raw_text in raw_sentences:\n",
    "            if random.random() < 0.5:\n",
    "                res = self.get_phrases_from_text(raw_text[\"raw\"].lower()) \n",
    "                if res:\n",
    "                    phrases.append(res)\n",
    "                else:\n",
    "                    phrases.append(id_to_name[category_id])\n",
    "            else:\n",
    "                phrases.append(id_to_name[category_id])\n",
    "\n",
    "        class_labels = [index for index in range(len(phrases))]\n",
    "\n",
    "        bbox_final = [bbox]*len(phrases)\n",
    "\n",
    "        # print (\"=*=\"*70)\n",
    "        # print (\"Text:\", text)\n",
    "        # print (\"Phrases:\", phrases)\n",
    "        # print (\"Class Labels:\", class_labels)\n",
    "        # print (\"BBoxes:\", bbox_final)\n",
    "\n",
    "\n",
    "        positive_map = self.build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "       \n",
    "        # print (\"Positive Map:\", positive_map)\n",
    "        # print (\"=*=\"*70)\n",
    "\n",
    "        # Optionally extend tokenizer on the fly\n",
    "        if self.auto_add_tokens:\n",
    "            tokens = text.split()\n",
    "            new_words = [t for t in tokens if t not in self.tokenizer.get_vocab()]\n",
    "            if new_words:\n",
    "                print (new_words)\n",
    "                LLLL\n",
    "                added = self.tokenizer.add_tokens(new_words)\n",
    "                if added > 0:\n",
    "                    model.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "                    print(f\" Added {added} new tokens:\", new_words)\n",
    "\n",
    "\n",
    "        return {\"image\": img, \"text\": text, \"bbox\": bbox_final, \"class_labels\": class_labels, \"positive_map\": positive_map, \"phrases\":phrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b731608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates [('girafe lower right', 1.0)]\n",
      "candidates [('giraffe right cut poff', 0.9766)]\n",
      "candidates [('banana center', 0.7456)]\n",
      "candidates [('single banana picture', 0.8821)]\n",
      "candidates [('man left', 0.6916)]\n",
      "candidates []\n",
      "candidates []\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[\"image\"] for item in batch]\n",
    "    texts  = [item[\"text\"] for item in batch]\n",
    "\n",
    "    encodings = processor(images=images, text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        if isinstance(item[\"class_labels\"], list):\n",
    "            class_labels = torch.tensor(item[\"class_labels\"], dtype=torch.long)\n",
    "            boxes = torch.tensor(item[\"bbox\"], dtype=torch.float32)\n",
    "            positive_map = item[\"positive_map\"]\n",
    "        else:\n",
    "            class_labels = torch.tensor([item[\"class_labels\"]], dtype=torch.long)\n",
    "            boxes = torch.tensor([item[\"bbox\"]], dtype=torch.float32)\n",
    "            positive_map = item[\"positive_map\"]\n",
    "\n",
    "\n",
    "        \n",
    "        labels.append({\n",
    "            \"class_labels\": class_labels.to(device) if torch.is_tensor(class_labels) else class_labels,\n",
    "            \"boxes\": [box.to(device) for box in boxes] if isinstance(boxes, list) else boxes.to(device),\n",
    "            \"positive_map\":positive_map.to(device)\n",
    "        })\n",
    "   \n",
    "    return encodings, labels \n",
    "\n",
    "\n",
    "dataset = RefCOCODataset(tokenizer, processor, dataset_split=\"train\", auto_add_tokens=False)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "model.model.text_backbone.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "for batch in data_loader:\n",
    "   \n",
    "    enc , labels = batch[0], batch[1]\n",
    "\n",
    "    # print (\"==== enc\", enc)\n",
    "    # print (\"==== labels\", labels)\n",
    "\n",
    "    enc = {k: v.to(device) if torch.is_tensor(v) else v for k, v in enc.items()}\n",
    "\n",
    "    outputs = model(**enc, labels=labels)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11578366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_ce': tensor(4.7377, device='mps:0', grad_fn=<DivBackward0>),\n",
       " 'loss_bbox': tensor(1410.8395, device='mps:0', grad_fn=<DivBackward0>),\n",
       " 'loss_giou': tensor(1.1720, device='mps:0', grad_fn=<DivBackward0>),\n",
       " 'cardinality_error': tensor(897., device='mps:0'),\n",
       " 'loss_ce_enc': tensor(27094.9375, device='mps:0', grad_fn=<DivBackward0>),\n",
       " 'loss_bbox_enc': tensor(1411.2059, device='mps:0'),\n",
       " 'loss_giou_enc': tensor(1.1716, device='mps:0'),\n",
       " 'cardinality_error_enc': tensor(897., device='mps:0')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "outputs.loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Suppose two images:\n",
    "images = [\n",
    "    Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\"),\n",
    "    Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/dog_and_person.jpg\").convert(\"RGB\"),\n",
    "\n",
    "]\n",
    "\n",
    "texts = [[\"number 2\"], [\"flexcontainer\"]]\n",
    "\n",
    "inputs = processor(\n",
    "    images=images,\n",
    "    text=texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebc7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = box.cpu() \n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0334d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    threshold=0.3,\n",
    "    target_sizes=[img.size[::-1] for img in images]\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"boxes\"], result[\"labels\"], result[\"scores\"])\n",
    "\n",
    "\n",
    "output = results[0]\n",
    "\n",
    "#output = {key:[val_list[0:2]]for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(images[0], output[\"boxes\"], output[\"labels\"], output[\"scores\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654755c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape # [batch_size, num_queries, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa319716",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "# convert list of annotations to DataFrame\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87905a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])  # contains bbox, image_id, category_id\n",
    "\n",
    "coco_images_df = pd.DataFrame(coco_ann['images'])  \n",
    "\n",
    "coco_grouped = coco_df.groupby(\"image_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load semantic model once globally\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def build_positive_map(text: str, phrases: list[str], tokenizer, semantic_threshold: float = 0.6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "    aligning each phrase with tokens that describe it, using both\n",
    "    literal substring and semantic similarity matching.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    if isinstance(offsets[0], list):\n",
    "        offsets = offsets[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "    num_tokens = len(tokens)\n",
    "    num_phrases = len(phrases)\n",
    "    positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "    # Normalize text for literal matching\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Precompute semantic embeddings\n",
    "    token_embs = semantic_model.encode(tokens, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    phrase_embs = semantic_model.encode(phrases, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sim = util.cos_sim(phrase_embs, token_embs)  # (num_phrases, num_tokens)\n",
    "\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        phrase_lower = phrase.strip().lower()\n",
    "        start_char = text_lower.find(phrase_lower)\n",
    "        if start_char != -1:\n",
    "            end_char = start_char + len(phrase_lower)\n",
    "            for j, span in enumerate(offsets):\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "        else:\n",
    "            # Use semantic similarity if no direct text match\n",
    "            positive_map[i] = sim[i] > semantic_threshold\n",
    "\n",
    "    return positive_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# def xywh_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Convert bounding boxes from (x, y, w, h) format to (cx, cy, w, h) format.\n",
    "    \n",
    "#     Args:\n",
    "#         boxes (torch.Tensor): Tensor of shape (N, 4) with boxes in [x, y, w, h]\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Tensor of shape (N, 4) with boxes in [cx, cy, w, h]\n",
    "#     \"\"\"\n",
    "#     x, y, w, h = boxes.unbind(-1)\n",
    "#     cx = x + w / 2\n",
    "#     cy = y + h / 2\n",
    "#     return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def xyxy_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert (x1, y1, x2, y2) to (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def normalize_boxes(boxes: torch.Tensor, W: int, H: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize (cx, cy, w, h) to [0,1] range\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    cx = cx / W\n",
    "    cy = cy / H\n",
    "    w = w / W\n",
    "    h = h / H\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "def scale_boxes_to_encoder(boxes: torch.Tensor, W: int, H: int, W_proc: int, H_proc: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Scale boxes from original image to encoder input size.\n",
    "    Works for boxes in (cx, cy, w, h) format.\n",
    "    \n",
    "    Args:\n",
    "        boxes (torch.Tensor): Tensor of shape (N, 4) in [cx, cy, w, h]\n",
    "        W, H: Original image width and height\n",
    "        W_proc, H_proc: Encoder processed image width and height\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled boxes in [cx, cy, w, h] format\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    cx = cx * W_proc / W\n",
    "    cy = cy * H_proc / H\n",
    "    w = w * W_proc / W\n",
    "    h = h * H_proc / H\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# norm_boxes = torch.tensor([[50, 100, 200, 300]], dtype=torch.float32)  # x, y, w, h\n",
    "# cxcy_boxes = xywh_to_cxcywh(norm_boxes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40274f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "\n",
    "image = Image.open(\"/Users/preetamverma/Desktop/multimodel/train2017/000000000025.jpg\").convert(\"RGB\")\n",
    "text = \"giraffe. giraffe standing in the grass\"\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = processor.tokenizer \n",
    "\n",
    "enc = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "pixel_values = enc[\"pixel_values\"]\n",
    "\n",
    "\n",
    "_, _, H_proc, W_proc = pixel_values.shape\n",
    "\n",
    "\n",
    "phrases = [\"giraffe\", \"giraffe standing on the grass\"]\n",
    "\n",
    "positive_map = build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "print (positive_map)\n",
    "abs_boxes = []\n",
    "\n",
    "Org_Image_Width, Org_Image_Height = image.size\n",
    "\n",
    "norm_boxes = torch.tensor([[320, 30, 814, 530], [300, 30, 700, 430]], dtype=torch.float32)  # x, y, w, h\n",
    "cxcy_boxes = xyxy_to_cxcywh(norm_boxes)\n",
    "abs_boxes = normalize_boxes(cxcy_boxes, Org_Image_Width, Org_Image_Height)\n",
    "\n",
    "\n",
    "#abs_boxes = torch.tensor(abs_boxes, dtype=torch.float32)\n",
    "\n",
    "print (\"=\\n\\t abs_boxes\\t\\t\\t\",abs_boxes)\n",
    "\n",
    "\n",
    "labels = [{\n",
    "    \"class_labels\": torch.tensor([0, 1], dtype=torch.long),\n",
    "    \"boxes\": abs_boxes,\n",
    "    \"positive_map\": positive_map,\n",
    "}]\n",
    "\n",
    "outputs = model(**enc, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "print (loss)\n",
    "\n",
    "\n",
    "# ---- Visualization ----\n",
    "draw = ImageDraw.Draw(image)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "for i, (box, label_idx) in enumerate(zip(norm_boxes, labels[0][\"class_labels\"])):\n",
    "    x0, y0, x1, y1 = box.tolist()\n",
    "    color = \"red\" if i == 0 else \"blue\"\n",
    "    label_text = phrases[label_idx]\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=color, width=4)\n",
    "    draw.text((x0, y0 - 10), label_text, fill=color, font=font)\n",
    "\n",
    "image.show()\n",
    "\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    enc.input_ids,\n",
    "    threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n",
    "print (\"=*=\"*70)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i}: {result['boxes']}, {result['labels']}, {result['scores']}\")\n",
    "print (\"=*=\"*70)\n",
    "\n",
    "\n",
    "output = results[0]\n",
    "\n",
    "#output = {key:[val_list[0]] for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(image, output[\"boxes\"].detach().numpy(), output[\"labels\"], output[\"scores\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093eac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([200, 300, 400, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Human\"), tokenizer.encode(\"Person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_phrases_from_text(text: str) -> list[str]:\n",
    "   from keybert import KeyBERT\n",
    "   model = KeyBERT('all-MiniLM-L6-v2')\n",
    "   candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "   return [c[0] for c in candidates]\n",
    "\n",
    "\n",
    "text = \"A person riding a horse in a field near a river.\"\n",
    "if random.random() < 0.5:\n",
    "    phrases = get_phrases_from_text(text)\n",
    "else:\n",
    "    phrases = None\n",
    "\n",
    "print(\"Extracted Phrases:\", phrases)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14259b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57d7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
