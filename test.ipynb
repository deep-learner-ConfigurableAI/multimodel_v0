{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class SwinBackboneMultiLevel(nn.Module):\n",
    "    def __init__(self, model_name=\"swin_tiny_patch4_window7_224\", embed_dim=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=pretrained,\n",
    "            features_only=True,\n",
    "            out_indices=(1, 2, 3)\n",
    "        )\n",
    "        in_channels = self.backbone.feature_info.channels()\n",
    "        \n",
    "        # Project each feature map to same dim\n",
    "        self.proj_layers = nn.ModuleList([\n",
    "            nn.Conv2d(in_ch, embed_dim, kernel_size=1) for in_ch in in_channels\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        outs = {}\n",
    "        for i, (f, proj) in enumerate(zip(feats, self.proj_layers)):\n",
    "             # Ensure channels-first\n",
    "            if f.shape[-1] == self.backbone.feature_info.channels()[i]:\n",
    "                # f is [B, H, W, C], convert to [B, C, H, W]\n",
    "                f = f.permute(0, 3, 1, 2).contiguous()\n",
    "            outs[f\"C{i+3}\"] = proj(f)\n",
    "        return outs\n",
    "\n",
    "\n",
    "class FPNFusion(nn.Module):\n",
    "    def __init__(self, in_channels=256):\n",
    "        super().__init__()\n",
    "        self.smooth3 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.smooth4 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.smooth5 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, c3, c4, c5):\n",
    "        p5 = c5\n",
    "        p4 = c4 + F.interpolate(p5, size=c4.shape[-2:], mode='nearest')\n",
    "        p3 = c3 + F.interpolate(p4, size=c3.shape[-2:], mode='nearest')\n",
    "\n",
    "        p3 = self.smooth3(p3)\n",
    "        p4 = self.smooth4(p4)\n",
    "        p5 = self.smooth5(p5)\n",
    "        return {\"P3\": p3, \"P4\": p4, \"P5\": p5}\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "backbone = SwinBackboneMultiLevel(embed_dim=256).to(device)\n",
    "fpn = FPNFusion(in_channels=256).to(device)\n",
    "\n",
    "\n",
    "img = Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\") \n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "x = transform(img).unsqueeze(0)  # [1, 3, 224, 224]\n",
    "x = x.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats = backbone(x) \n",
    "    fpn_feats = fpn(feats[\"C3\"], feats[\"C4\"], feats[\"C5\"])  # P3, P4, P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317418bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_map(feat, title):\n",
    "    # feat: [B, C, H, W]\n",
    "    fmap = feat[0].mean(0).cpu()  # average over channels\n",
    "    plt.imshow(fmap, cmap='viridis')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Swin outputs\n",
    "# show_feature_map(feats[\"C3\"], \"C3 - Raw Swin\")\n",
    "show_feature_map(feats[\"C4\"], \"C4 - Raw Swin\")\n",
    "show_feature_map(feats[\"C5\"], \"C5 - Raw Swin\")\n",
    "\n",
    "# FPN fused outputs\n",
    "# show_feature_map(fpn_feats[\"P3\"], \"P3 - FPN Fusion\")\n",
    "# show_feature_map(fpn_feats[\"P4\"], \"P4 - FPN Fusion\")\n",
    "show_feature_map(fpn_feats[\"P5\"], \"P5 - FPN Fusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df5377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"grounding_dino_tiny_local\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "tokenizer = processor.tokenizer\n",
    "vocab_size = len(tokenizer)\n",
    "print (\"Before\")\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "embed_layer =  model.model.text_backbone.get_input_embeddings()\n",
    "print(\"Embedding layer size:\", embed_layer.weight.size())\n",
    "\n",
    "new_tokens = [\"button\", \"flexcontainer\"]\n",
    "added_tokens = tokenizer.add_tokens(new_tokens)\n",
    "print(\"Added tokens:\", added_tokens)\n",
    "\n",
    "if added_tokens > 0:\n",
    "    model.model.text_backbone.resize_token_embeddings(len(tokenizer))\n",
    "print (\"After\")\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "\n",
    "embed_layer = model.model.text_backbone.get_input_embeddings()\n",
    "print(\"Embedding layer size:\", embed_layer.weight.size())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ed659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, GroundingDinoForObjectDetection\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "# Print submodules\n",
    "for name, module in model.named_modules():\n",
    "    if \"text\" in name.lower() or \"lang\" in name.lower():\n",
    "        print(name, type(module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_id = tokenizer.encode(\"flexcontainer\")\n",
    "tokens_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"/Users/preetamverma/Desktop/image_cap_model_test_images/gettyimages-144103223-2048x2048.jpg\").convert(\"RGB\")\n",
    "text = [\"a ceramic mug on the table\"]\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c00b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from transformers import GroundingDinoConfig\n",
    "import torch \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "config = GroundingDinoConfig()\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "model = model.to(device)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "\n",
    "\n",
    "if not hasattr(torch.nn.functional, \"_original_grid_sample\"):\n",
    "    torch.nn.functional._original_grid_sample = torch.nn.functional.grid_sample\n",
    "\n",
    "    def safe_grid_sample(input, grid, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=None):\n",
    "        if input.device.type == \"mps\":\n",
    "            # Move to CPU + float32\n",
    "            input_cpu = input.to(\"cpu\", dtype=torch.float32)\n",
    "            grid_cpu = grid.to(\"cpu\", dtype=torch.float32)\n",
    "            with torch.no_grad():\n",
    "                output_cpu = torch.nn.functional._original_grid_sample(\n",
    "                    input_cpu, grid_cpu, mode=mode, padding_mode=padding_mode, align_corners=align_corners\n",
    "                )\n",
    "            return output_cpu.to(\"mps\", dtype=input.dtype)\n",
    "        else:\n",
    "            return torch.nn.functional._original_grid_sample(\n",
    "                input, grid, mode=mode, padding_mode=padding_mode, align_corners=align_corners\n",
    "            )\n",
    "\n",
    "torch.nn.functional.grid_sample = safe_grid_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602dd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer\n",
    "import re \n",
    "\n",
    "\n",
    "UI_VOCAB = [\n",
    "    \"button\", \"icon\", \"text\", \"image\", \"input\", \"checkbox\", \"link\",\n",
    "    \"menu item\", \"banner\", \"avatar\", \"logo\", \"label\", \"switch\",\n",
    "    \"tab\", \"card\", \"popup\", \"dropdown\", \"textfield\", \"container\"\n",
    "]\n",
    "\n",
    "UI_TEXT_MAP = {\n",
    "    \"image\": [\"picture\", \"photo\", \"avatar\", \"logo\", \"icon\"],\n",
    "    \"button\": [\"button\", \"tap\", \"click\", \"submit\"],\n",
    "    \"text\": [\"text\", \"label\"],\n",
    "    \"input\": [\"input\", \"field\", \"search\", \"textbox\"],\n",
    "}\n",
    "\n",
    "\n",
    "class UIElementClassifier:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.vocab_emb = self.model.encode(vocab, normalize_embeddings=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "    def build_positive_map(self, text: str, phrases: list[str]) -> torch.Tensor:\n",
    "        offsets = self.tokenizer(text, return_offsets_mapping=True)[\"offset_mapping\"]\n",
    "        positive_map = torch.zeros((len(phrases), len(offsets)), dtype=torch.bool)\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            candidates = UI_TEXT_MAP.get(phrase, [phrase])\n",
    "            for cand in candidates:\n",
    "                for match in re.finditer(re.escape(cand), text_lower):\n",
    "                    start, end = match.span()\n",
    "                    for j, (s_tok, e_tok) in enumerate(offsets):\n",
    "                        if e_tok > start and s_tok < end:\n",
    "                            positive_map[i, j] = True\n",
    "        return positive_map\n",
    "\n",
    "\n",
    "    def classify(self, text):\n",
    "        \"\"\"Return best-matching UI term for the referring expression.\"\"\"\n",
    "        query_emb = self.model.encode(text, normalize_embeddings=True)\n",
    "        sim = util.cos_sim(query_emb, self.vocab_emb)[0]\n",
    "        idx = torch.argmax(sim).item()\n",
    "        return self.vocab[idx], sim[idx].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import pandas as pd\n",
    "import os \n",
    "import ast \n",
    "import numpy as np \n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "\n",
    "UI_VOCAB = [\n",
    "    \"button\", \"icon\", \"text\", \"image\", \"input\", \"checkbox\", \"link\",\n",
    "    \"menu item\", \"banner\", \"avatar\", \"logo\", \"label\", \"switch\",\n",
    "    \"tab\", \"card\", \"popup\", \"dropdown\", \"textfield\", \"container\"\n",
    "]\n",
    "\n",
    "UI_TEXT_MAP = {\n",
    "    \"image\": [\"picture\", \"photo\", \"avatar\", \"logo\", \"icon\"],\n",
    "    \"button\": [\"button\", \"tap\", \"click\", \"submit\"],\n",
    "    \"text\": [\"text\", \"label\"],\n",
    "    \"input\": [\"input\", \"field\", \"search\", \"textbox\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "class OSAtlasRefDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Converts OS-Atlas GUI grounding dataset to GroundingDINO format.\n",
    "    Each image may contain multiple referring expressions and bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_path, image_root, tokenizer, \n",
    "                 split=\"train\", auto_add_tokens=False):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.auto_add_tokens = auto_add_tokens\n",
    "        self.image_root = image_root\n",
    "\n",
    "        # Load raw JSON\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        # Flatten image + elements into one dataframe\n",
    "        records = []\n",
    "        for item in self.data:\n",
    "            img_path = os.path.join(image_root, item[\"img_filename\"])\n",
    "            for el in item[\"elements\"]:\n",
    "                records.append({\n",
    "                    \"img_path\": img_path,\n",
    "                    \"instruction\": el[\"instruction\"].strip(),\n",
    "                    \"bbox\": el[\"bbox\"]\n",
    "                })\n",
    "        self.df = pd.DataFrame(records)\n",
    "        print(f\" Loaded {len(self.df)} referring expressions from {len(self.data)} images\")\n",
    "\n",
    "        self.ui_classifier = UIElementClassifier(UI_VOCAB)\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_phrases_from_text(self, text: str) -> list[str]:\n",
    "        from keybert import KeyBERT\n",
    "        model = KeyBERT('all-MiniLM-L6-v2')\n",
    "        candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "        print (\"candidates\", candidates)\n",
    "        if not candidates:\n",
    "            return None \n",
    "        return [c[0] for c in candidates][0]\n",
    "\n",
    "    \n",
    "    def build_positive_map(self, text: str, phrases: list[str], tokenizer) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "        aligning each phrase with the tokens that describe it.\n",
    "        \"\"\"\n",
    "        # Tokenize with offsets\n",
    "        encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        \n",
    "        #  Fix: offsets are nested under [0] since tokenizer returns batch dim\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        if isinstance(offsets[0], list):\n",
    "            offsets = offsets[0]  # shape: (num_tokens, 2)\n",
    "\n",
    "        num_tokens = len(offsets)\n",
    "        num_phrases = len(phrases)\n",
    "        positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "        # Normalize text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            phrase = phrase.strip().lower()\n",
    "            start_char = text_lower.find(phrase)\n",
    "            if start_char == -1:\n",
    "                continue\n",
    "            end_char = start_char + len(phrase)\n",
    "\n",
    "            # Iterate over token spans\n",
    "            for j, span in enumerate(offsets):\n",
    "                # Some tokenizers return (0, 0) for [CLS]/[SEP]\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "\n",
    "        return positive_map\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"img_path\"]\n",
    "        text = row[\"instruction\"]\n",
    "        bbox = row[\"bbox\"]  # normalized xyxy\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        ui_label, score = self.ui_classifier.classify(text.lower())\n",
    "\n",
    "        phrases = [ui_label]\n",
    "\n",
    "        print (\"=== phrases\", phrases)\n",
    "\n",
    "        bbox_final = [bbox]*len(phrases)\n",
    "\n",
    "\n",
    "        positive_map = self.ui_classifier.build_positive_map(text, phrases)\n",
    "\n",
    "        class_labels = [index for index in range(len(phrases))]\n",
    "\n",
    "        return {\"image\": image, \"text\": text, \"bbox\": bbox_final, \"class_labels\": class_labels, \"positive_map\": positive_map, \"phrases\":phrases}\n",
    "\n",
    "\n",
    "dataset = OSAtlasRefDataset(\n",
    "    json_path=\"/Users/preetamverma/Downloads/uibert_raw.json\",  \n",
    "    image_root=\"/Users/preetamverma/Downloads\",      \n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b731608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        box = box\n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    images  = [item[\"image\"] for item in batch]\n",
    "    texts   = [item[\"text\"] for item in batch]\n",
    "    phrases = [item[\"phrases\"] for item in batch]\n",
    "\n",
    "    print (\"texts\", texts)\n",
    "\n",
    "    encodings = processor(images=images, text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        if isinstance(item[\"class_labels\"], list):\n",
    "            class_labels = torch.tensor(item[\"class_labels\"], dtype=torch.long)\n",
    "            boxes = torch.tensor(item[\"bbox\"], dtype=torch.float32)\n",
    "            positive_map = item[\"positive_map\"]\n",
    "        else:\n",
    "            class_labels = torch.tensor([item[\"class_labels\"]], dtype=torch.long)\n",
    "            boxes = torch.tensor([item[\"bbox\"]], dtype=torch.float32)\n",
    "            positive_map = item[\"positive_map\"]\n",
    "\n",
    "\n",
    "        \n",
    "        labels.append({\n",
    "            \"class_labels\": class_labels.to(device) if torch.is_tensor(class_labels) else class_labels,\n",
    "            \"boxes\": [box.to(device) for box in boxes] if isinstance(boxes, list) else boxes.to(device),\n",
    "            \"positive_map\":positive_map.to(device)\n",
    "        })\n",
    "   \n",
    "    return encodings, labels , images, phrases\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb95c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAINING LOOP #####\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "from utils import calculate_total_train_params, save_to_checkpoint\n",
    "import torch.nn as nn \n",
    "from typing import ClassVar\n",
    "from pydantic import BaseModel\n",
    "import torch.nn.functional as F\n",
    "import time \n",
    "\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    batch_size: ClassVar[int] = 4\n",
    "    steps: ClassVar[int] = 0\n",
    "    epochs: ClassVar[int] = 10\n",
    "    lr: ClassVar[float] = 3e-4\n",
    "    accumulation_steps: ClassVar[int] = 2\n",
    "    save_every : ClassVar[int] = 1000\n",
    "    checkpoint_path : ClassVar[str] = \"checkpoint.pth\"\n",
    "\n",
    "\n",
    "##### Setup Training #####\n",
    "\n",
    "all_params = calculate_total_train_params(model)\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "\n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TrainingConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/TrainingConfig.accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 10\n",
    "stop = False \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "l_epoch = 0 \n",
    "l_loss = 0 \n",
    "l_epoch =0\n",
    "l_global_step = 0\n",
    "\n",
    "\n",
    "N_EPOCHS = TrainingConfig.epochs - l_epoch\n",
    "\n",
    "print (f\"PREVIOUS LOSS {l_loss} AT GLOBAL STEP {l_global_step} AT EPOCH {l_epoch}\")\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Unpack batch\n",
    "        enc , labels, images, phrases = batch[0], batch[1], batch[2], batch[3]\n",
    "\n",
    "        print (\"labels\", labels)\n",
    "\n",
    "        # for lbl, enc in zip(labels, enc[\"input_ids\"]):\n",
    "        #     if lbl[\"positive_map\"].shape[-1] != enc.shape[-1]:\n",
    "        #         print(\"Mismatch:\", lbl[\"positive_map\"].shape, enc.shape)\n",
    "\n",
    "   \n",
    "        enc = {k: v.to(device) if torch.is_tensor(v) else v for k, v in enc.items()}\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1 \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "\n",
    "            outputs = model(**enc, labels=labels)\n",
    "            loss_1 = outputs.loss \n",
    "            loss_dict = outputs.loss_dict\n",
    "\n",
    "            # if global_step % 300 == 0:\n",
    "            #     print (loss_dict)\n",
    "\n",
    "            #     # print (f\"LM LOSS: {lm_loss.item():.4f} | \"\n",
    "            #     #        f\"BBOX LOSS: {loss_bbox.item():.4f} |\"\n",
    "            #     #         f\"GIOU LOSS: {loss_giou.item():.4f} |\"\n",
    "            #     #          f\"CE LOSS: {loss_ce.item():.4f} |\"\n",
    "            #     #           f\"OBJECTIVENESS LOSS: {objectness_loss.item():.4f} |\"\n",
    "            #     #        )\n",
    "\n",
    "                  \n",
    "            loss =  loss_1 / TrainingConfig.accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "       \n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "\n",
    "\n",
    "        if (step + 1) % TrainingConfig.accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * TrainingConfig.accumulation_steps\n",
    "\n",
    "\n",
    "        # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{TrainingConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*TrainingConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*TrainingConfig.accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "\n",
    "    if (step + 1) % TrainingConfig.accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    " \n",
    "    \n",
    "    # del enc , labels, images, phrases \n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd60b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    enc[\"input_ids\"],\n",
    "    threshold=0.3,\n",
    "    target_sizes=[img.size[::-1] for img in images]\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"boxes\"], result[\"labels\"], result[\"scores\"])\n",
    "\n",
    "\n",
    "#output = {key:[val_list[0:2]]for key, val_list in output.items()}\n",
    "\n",
    "# Grab first image results\n",
    "result = results[0]\n",
    "\n",
    "boxes = result[\"boxes\"].detach().cpu().numpy().tolist()  # detach first\n",
    "result_labels = result[\"labels\"]  # if labels are tensors\n",
    "scores = result[\"scores\"].detach().cpu().numpy().tolist()  # if scores are tensors\n",
    "\n",
    "\n",
    "top_k = 2\n",
    "\n",
    "indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "\n",
    "boxes = [boxes[i] for i in indices]\n",
    "result_labels = [result_labels[i] for i in indices]\n",
    "scores = [scores[i] for i in indices]\n",
    "\n",
    "\n",
    "visualize_boxes(images[0], boxes, result_labels, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a21993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Visualization ----\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "draw = ImageDraw.Draw(images[0])\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "img_w, img_h = images[0].size\n",
    "\n",
    "\n",
    "\n",
    "for i, (box, label_idx) in enumerate(zip(labels[0][\"boxes\"], labels[0][\"class_labels\"])):\n",
    "\n",
    "    print (\"=\"*30)\n",
    "    print (box)\n",
    "    print (label_idx)\n",
    "    print (\"=\"*30)\n",
    "\n",
    "\n",
    "    x0, y0, x1, y1 = box.tolist()\n",
    "\n",
    "    # box is [x_min, y_min, x_max, y_max] in normalized [0,1]\n",
    "    x0 = box[0] * img_w\n",
    "    y0 = box[1] * img_h\n",
    "    x1 = box[2] * img_w\n",
    "    y1 = box[3] * img_h\n",
    "\n",
    "\n",
    "\n",
    "    color = \"red\" if i == 0 else \"blue\"\n",
    "    label_text = phrases[label_idx.item()]\n",
    "\n",
    "    print (\"label_text\", label_text)\n",
    "    print (x0, y0, x1, y1)\n",
    "\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=color, width=4)\n",
    "\n",
    "    # draw.text((x0, max(0, y0 - 10)), label_text, fill=color, font=font)\n",
    "\n",
    "\n",
    "images[0].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b5097f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Suppose two images:\u001b[39;00m\n\u001b[32m      6\u001b[39m images = [\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mImage\u001b[49m.open(\u001b[33m\"\u001b[39m\u001b[33m/Users/preetamverma/Downloads/screenshots/screens9946.png\u001b[39m\u001b[33m\"\u001b[39m).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m ]\n\u001b[32m     11\u001b[39m texts = [[\u001b[33m\"\u001b[39m\u001b[33mbutton\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     13\u001b[39m inputs = processor(\n\u001b[32m     14\u001b[39m     images=images,\n\u001b[32m     15\u001b[39m     text=texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     19\u001b[39m ).to(device)\n",
      "\u001b[31mNameError\u001b[39m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Suppose two images:\n",
    "images = [\n",
    "    Image.open(\"/Users/preetamverma/Downloads/screenshots/screens9946.png\").convert(\"RGB\"),\n",
    "\n",
    "]\n",
    "\n",
    "texts = [[\"button\"]]\n",
    "\n",
    "inputs = processor(\n",
    "    images=images,\n",
    "    text=texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "inputs[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0334d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    threshold=0.3,\n",
    "    target_sizes=[img.size[::-1] for img in images]\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(result[\"boxes\"], result[\"labels\"], result[\"scores\"])\n",
    "\n",
    "\n",
    "output = results[0]\n",
    "\n",
    "#output = {key:[val_list[0:2]]for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(images[0], output[\"boxes\"], output[\"labels\"], output[\"scores\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654755c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47206ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape # [batch_size, num_queries, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa319716",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "# convert list of annotations to DataFrame\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbd191",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87905a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])  # contains bbox, image_id, category_id\n",
    "\n",
    "coco_images_df = pd.DataFrame(coco_ann['images'])  \n",
    "\n",
    "coco_grouped = coco_df.groupby(\"image_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load semantic model once globally\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def build_positive_map(text: str, phrases: list[str], tokenizer, semantic_threshold: float = 0.6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "    aligning each phrase with tokens that describe it, using both\n",
    "    literal substring and semantic similarity matching.\n",
    "    \"\"\"\n",
    "    # Tokenize text\n",
    "    encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "    offsets = encoding[\"offset_mapping\"]\n",
    "    if isinstance(offsets[0], list):\n",
    "        offsets = offsets[0]\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0])\n",
    "    num_tokens = len(tokens)\n",
    "    num_phrases = len(phrases)\n",
    "    positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "    # Normalize text for literal matching\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Precompute semantic embeddings\n",
    "    token_embs = semantic_model.encode(tokens, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    phrase_embs = semantic_model.encode(phrases, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sim = util.cos_sim(phrase_embs, token_embs)  # (num_phrases, num_tokens)\n",
    "\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        phrase_lower = phrase.strip().lower()\n",
    "        start_char = text_lower.find(phrase_lower)\n",
    "        if start_char != -1:\n",
    "            end_char = start_char + len(phrase_lower)\n",
    "            for j, span in enumerate(offsets):\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "        else:\n",
    "            # Use semantic similarity if no direct text match\n",
    "            positive_map[i] = sim[i] > semantic_threshold\n",
    "\n",
    "    return positive_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# def xywh_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Convert bounding boxes from (x, y, w, h) format to (cx, cy, w, h) format.\n",
    "    \n",
    "#     Args:\n",
    "#         boxes (torch.Tensor): Tensor of shape (N, 4) with boxes in [x, y, w, h]\n",
    "    \n",
    "#     Returns:\n",
    "#         torch.Tensor: Tensor of shape (N, 4) with boxes in [cx, cy, w, h]\n",
    "#     \"\"\"\n",
    "#     x, y, w, h = boxes.unbind(-1)\n",
    "#     cx = x + w / 2\n",
    "#     cy = y + h / 2\n",
    "#     return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def xyxy_to_cxcywh(boxes: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert (x1, y1, x2, y2) to (cx, cy, w, h)\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "    cx = (x1 + x2) / 2\n",
    "    cy = (y1 + y2) / 2\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "def normalize_boxes(boxes: torch.Tensor, W: int, H: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize (cx, cy, w, h) to [0,1] range\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    cx = cx / W\n",
    "    cy = cy / H\n",
    "    w = w / W\n",
    "    h = h / H\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "    \n",
    "\n",
    "\n",
    "def scale_boxes_to_encoder(boxes: torch.Tensor, W: int, H: int, W_proc: int, H_proc: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Scale boxes from original image to encoder input size.\n",
    "    Works for boxes in (cx, cy, w, h) format.\n",
    "    \n",
    "    Args:\n",
    "        boxes (torch.Tensor): Tensor of shape (N, 4) in [cx, cy, w, h]\n",
    "        W, H: Original image width and height\n",
    "        W_proc, H_proc: Encoder processed image width and height\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled boxes in [cx, cy, w, h] format\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = boxes.unbind(-1)\n",
    "    cx = cx * W_proc / W\n",
    "    cy = cy * H_proc / H\n",
    "    w = w * W_proc / W\n",
    "    h = h * H_proc / H\n",
    "    return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# norm_boxes = torch.tensor([[50, 100, 200, 300]], dtype=torch.float32)  # x, y, w, h\n",
    "# cxcy_boxes = xywh_to_cxcywh(norm_boxes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_boxes(image, boxes, labels, scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x0, y0, x1, y1 = box\n",
    "        width, height = x1 - x0, y1 - y0\n",
    "        rect = patches.Rectangle((x0, y0), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x0, y0, f\"{label}: {score:.2f}\", color='white', fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40274f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "\n",
    "image = Image.open(\"/Users/preetamverma/Desktop/multimodel/train2017/000000000025.jpg\").convert(\"RGB\")\n",
    "text = \"giraffe. giraffe standing in the grass\"\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id)\n",
    "\n",
    "tokenizer = processor.tokenizer \n",
    "\n",
    "enc = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "pixel_values = enc[\"pixel_values\"]\n",
    "\n",
    "\n",
    "_, _, H_proc, W_proc = pixel_values.shape\n",
    "\n",
    "\n",
    "phrases = [\"giraffe\", \"giraffe standing on the grass\"]\n",
    "\n",
    "positive_map = build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "print (positive_map)\n",
    "abs_boxes = []\n",
    "\n",
    "Org_Image_Width, Org_Image_Height = image.size\n",
    "\n",
    "norm_boxes = torch.tensor([[320, 30, 814, 530], [300, 30, 700, 430]], dtype=torch.float32)  # x, y, w, h\n",
    "cxcy_boxes = xyxy_to_cxcywh(norm_boxes)\n",
    "abs_boxes = normalize_boxes(cxcy_boxes, Org_Image_Width, Org_Image_Height)\n",
    "\n",
    "\n",
    "#abs_boxes = torch.tensor(abs_boxes, dtype=torch.float32)\n",
    "\n",
    "print (\"=\\n\\t abs_boxes\\t\\t\\t\",abs_boxes)\n",
    "\n",
    "\n",
    "labels = [{\n",
    "    \"class_labels\": torch.tensor([0, 1], dtype=torch.long),\n",
    "    \"boxes\": abs_boxes,\n",
    "    \"positive_map\": positive_map,\n",
    "}]\n",
    "\n",
    "outputs = model(**enc, labels=labels)\n",
    "loss = outputs.loss\n",
    "\n",
    "print (loss)\n",
    "\n",
    "\n",
    "# ---- Visualization ----\n",
    "draw = ImageDraw.Draw(image)\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "for i, (box, label_idx) in enumerate(zip(norm_boxes, labels[0][\"class_labels\"])):\n",
    "    x0, y0, x1, y1 = box.tolist()\n",
    "    color = \"red\" if i == 0 else \"blue\"\n",
    "    label_text = phrases[label_idx]\n",
    "    draw.rectangle([x0, y0, x1, y1], outline=color, width=4)\n",
    "    draw.text((x0, y0 - 10), label_text, fill=color, font=font)\n",
    "\n",
    "image.show()\n",
    "\n",
    "\n",
    "# Then post-process:\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    enc.input_ids,\n",
    "    threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n",
    "print (\"=*=\"*70)\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Result {i}: {result['boxes']}, {result['labels']}, {result['scores']}\")\n",
    "print (\"=*=\"*70)\n",
    "\n",
    "\n",
    "output = results[0]\n",
    "\n",
    "#output = {key:[val_list[0]] for key, val_list in output.items()}\n",
    "\n",
    "visualize_boxes(image, output[\"boxes\"].detach().numpy(), output[\"labels\"], output[\"scores\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093eac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([3746, 12696])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"image\"), tokenizer.encode(\"icon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def get_phrases_from_text(text: str) -> list[str]:\n",
    "   from keybert import KeyBERT\n",
    "   model = KeyBERT('all-MiniLM-L6-v2')\n",
    "   candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "   return [c[0] for c in candidates]\n",
    "\n",
    "\n",
    "text = \"A person riding a horse in a field near a river.\"\n",
    "if random.random() < 0.5:\n",
    "    phrases = get_phrases_from_text(text)\n",
    "else:\n",
    "    phrases = None\n",
    "\n",
    "print(\"Extracted Phrases:\", phrases)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "from transformers.models.grounding_dino import GroundingDinoForObjectDetection, GroundingDinoConfig\n",
    "import pandas as pd\n",
    "import os \n",
    "import ast \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "\n",
    "with open(\"label_dict.json\", \"r\") as f:\n",
    "    label_dict = json.loads(f.read()) \n",
    "\n",
    "# Raw COCO id -> name mapping (already in file)\n",
    "id_to_name = label_dict[\"label_dict\"]\n",
    "\n",
    "\n",
    "\n",
    "with open(\"annotations/instances_train2017.json\") as f:\n",
    "    coco_ann = json.load(f)\n",
    "\n",
    "coco_df = pd.DataFrame(coco_ann['annotations'])  # contains bbox, image_id, category_id\n",
    "\n",
    "coco_images_df = pd.DataFrame(coco_ann['images'])  \n",
    "\n",
    "coco_grouped = coco_df.groupby(\"image_id\")\n",
    "\n",
    "# Load semantic model once globally\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "\n",
    "class RefCOCODataset(Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, processor, dataset_split=\"train\", auto_add_tokens=False):\n",
    "        self.df = pd.read_parquet('coco_ref/ref_coco_train.parquet')\n",
    "        self.df[\"img_path\"] = self.df[\"image_id\"].apply(lambda x: os.path.join(\"train2017\", f\"{x:012d}.jpg\"))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.processor = processor\n",
    "        self.auto_add_tokens = auto_add_tokens\n",
    "        self.ui_classifier = UIElementClassifier(UI_VOCAB)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def get_phrases_from_text(self, text: str) -> list[str]:\n",
    "        from keybert import KeyBERT\n",
    "        model = KeyBERT('all-MiniLM-L6-v2')\n",
    "        candidates = model.extract_keywords(text, keyphrase_ngram_range=(2,4), stop_words='english', top_n=1)\n",
    "        print (\"candidates\", candidates)\n",
    "        if not candidates:\n",
    "            return None \n",
    "        return [c[0] for c in candidates][0]\n",
    "\n",
    "\n",
    "    def xyxy_to_cxcywh(self, boxes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert (x1, y1, x2, y2) to (cx, cy, w, h)\n",
    "        \"\"\"\n",
    "        x1, y1, x2, y2 = boxes.unbind(-1)\n",
    "        cx = (x1 + x2) / 2\n",
    "        cy = (y1 + y2) / 2\n",
    "        w = x2 - x1\n",
    "        h = y2 - y1\n",
    "        return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "    def normalize_boxes(self, boxes: torch.Tensor, W: int, H: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize (cx, cy, w, h) to [0,1] range\n",
    "        \"\"\"\n",
    "        cx, cy, w, h = boxes.unbind(-1)\n",
    "        cx = cx / W\n",
    "        cy = cy / H\n",
    "        w = w / W\n",
    "        h = h / H\n",
    "        return torch.stack([cx, cy, w, h], dim=-1)\n",
    "\n",
    "\n",
    "    def build_positive_map(self, text: str, phrases: list[str], tokenizer) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Builds a positive_map tensor of shape [num_phrases, num_tokens]\n",
    "        aligning each phrase with the tokens that describe it.\n",
    "        \"\"\"\n",
    "        # Tokenize with offsets\n",
    "        encoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "        \n",
    "        #  Fix: offsets are nested under [0] since tokenizer returns batch dim\n",
    "        offsets = encoding[\"offset_mapping\"]\n",
    "        if isinstance(offsets[0], list):\n",
    "            offsets = offsets[0]  # shape: (num_tokens, 2)\n",
    "\n",
    "        num_tokens = len(offsets)\n",
    "        num_phrases = len(phrases)\n",
    "        positive_map = torch.zeros((num_phrases, num_tokens), dtype=torch.bool)\n",
    "\n",
    "        # Normalize text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for i, phrase in enumerate(phrases):\n",
    "            phrase = phrase.strip().lower()\n",
    "            start_char = text_lower.find(phrase)\n",
    "            if start_char == -1:\n",
    "                continue\n",
    "            end_char = start_char + len(phrase)\n",
    "\n",
    "            # Iterate over token spans\n",
    "            for j, span in enumerate(offsets):\n",
    "                # Some tokenizers return (0, 0) for [CLS]/[SEP]\n",
    "                if not isinstance(span, (list, tuple)) or len(span) != 2:\n",
    "                    continue\n",
    "                start_tok, end_tok = span\n",
    "                if end_tok > start_char and start_tok < end_char:\n",
    "                    positive_map[i, j] = True\n",
    "\n",
    "        return positive_map\n",
    "\n",
    "\n",
    "    def match_bbox(self, ref_bbox, coco_bboxes):\n",
    "        \"\"\"\n",
    "        ref_bbox: [x, y, w, h]\n",
    "        coco_bboxes: DataFrame with columns ['bbox', 'category_id']\n",
    "        returns category_id\n",
    "        \"\"\"\n",
    "        # convert to numpy arrays\n",
    "        ref_bbox = np.array(ref_bbox, dtype=np.float32)\n",
    "        \n",
    "        for _, row in coco_bboxes.iterrows():\n",
    "            coco_bbox = np.array(row['bbox'], dtype=np.float32)\n",
    "            # check if IoU > threshold or exact match\n",
    "            if np.allclose(ref_bbox, coco_bbox, atol=2.0):  # allow small rounding differences\n",
    "                return int(row['category_id'])\n",
    "        # fallback\n",
    "        return 1\n",
    "\n",
    "\n",
    "    def get_category_id(self, image_id, ref_bbox):\n",
    "        category_id = 1\n",
    "        if image_id in coco_grouped.groups:\n",
    "            coco_bboxes = coco_grouped.get_group(image_id)\n",
    "            category_id = self.match_bbox(ref_bbox, coco_bboxes)\n",
    "\n",
    "        else:\n",
    "            print (\"Image not Found\")\n",
    "        \n",
    "        if category_id:\n",
    "            return category_id\n",
    "        \n",
    "        return 0 \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get row\n",
    "        ex = self.df.iloc[idx]\n",
    "\n",
    "        image_id = ex[\"image_id\"]\n",
    "\n",
    "        img = Image.open(ex[\"img_path\"]).convert(\"RGB\")\n",
    "        # img = transform(img)\n",
    "\n",
    "        bbox_list = ex[\"bbox\"]\n",
    "\n",
    "        bbox  =  [float(np.float64(float(bbox))) for bbox in bbox_list]\n",
    "\n",
    "        # bbox = ast.literal_eval(bbox)  # [x, y, w, h]\n",
    "\n",
    "        raw_sentences = ast.literal_eval(ex[\"raw_sentences\"])\n",
    "\n",
    "        text = \". \".join([raw_text[\"raw\"].lower() for raw_text in raw_sentences])\n",
    "\n",
    "        category_id = str(int(ex['category_id']))\n",
    "\n",
    "        phrases = [] \n",
    "\n",
    "        for raw_text in raw_sentences:\n",
    "            if random.random() < 0.5:\n",
    "                res = self.get_phrases_from_text(raw_text[\"raw\"].lower()) \n",
    "                if res:\n",
    "                    phrases.append(res)\n",
    "                else:\n",
    "                    phrases.append(id_to_name[category_id])\n",
    "            else:\n",
    "                phrases.append(id_to_name[category_id])\n",
    "\n",
    "        class_labels = [index for index in range(len(phrases))]\n",
    "\n",
    "        bbox_final = [bbox]*len(phrases)\n",
    "\n",
    "        # print (\"=*=\"*70)\n",
    "        # print (\"Text:\", text)\n",
    "        # print (\"Phrases:\", phrases)\n",
    "        # print (\"Class Labels:\", class_labels)\n",
    "        # print (\"BBoxes:\", bbox_final)\n",
    "\n",
    "\n",
    "        positive_map = self.build_positive_map(text, phrases, tokenizer)\n",
    "\n",
    "       \n",
    "        # print (\"Positive Map:\", positive_map)\n",
    "        # print (\"=*=\"*70)\n",
    "\n",
    "        # Optionally extend tokenizer on the fly\n",
    "        if self.auto_add_tokens:\n",
    "            tokens = text.split()\n",
    "            new_words = [t for t in tokens if t not in self.tokenizer.get_vocab()]\n",
    "            if new_words:\n",
    "                print (new_words)\n",
    "                LLLL\n",
    "                added = self.tokenizer.add_tokens(new_words)\n",
    "                if added > 0:\n",
    "                    model.text_encoder.resize_token_embeddings(len(self.tokenizer))\n",
    "                    print(f\" Added {added} new tokens:\", new_words)\n",
    "\n",
    "\n",
    "        return {\"image\": img, \"text\": text, \"bbox\": bbox_final, \"class_labels\": class_labels, \"positive_map\": positive_map, \"phrases\":phrases}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57d7fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
