{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:26:43.835029Z",
     "start_time": "2025-09-30T03:26:36.736736Z"
    }
   },
   "outputs": [],
   "source": [
    "#### FINAL VERSION ####\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer = get_models() \n",
    "\n",
    "# torch.mps.empty_cache()\n",
    "# import gc; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:27:49.087304Z",
     "start_time": "2025-09-30T03:27:28.355910Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#### TRAINING LOOP #####\n",
    "\n",
    "\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "import numpy as np \n",
    "from utils import calculate_total_train_params, save_to_checkpoint\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # --------------------\n",
    "    #  Validation step\n",
    "    # --------------------\n",
    "    decoder_model.eval()\n",
    "    encoder_model.eval()\n",
    "    val_loss = 0\n",
    "    count = 0 \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            image_tensor, caption_tensor, attention_mask = [x.to(device) for x in val_batch]\n",
    "            with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "                x_embed = encoder_model(image_tensor)\n",
    "                _, val_caption_loss = decoder_model(x_embed, caption_tensor, attention_mask)\n",
    "            val_loss += val_caption_loss.item()\n",
    "            count+=1\n",
    "            if count > 2:break \n",
    "    val_loss /= count \n",
    "    decoder_model.train()\n",
    "    encoder_model.train()\n",
    "    print(f\"Epoch {epoch+1}: train_loss={total_loss/len(train_dataloader):.4f}, val_loss={val_loss:.4f}\")\n",
    "    return val_loss \n",
    "\n",
    "\n",
    "\n",
    "def should_stop(loss_list):\n",
    "    last_ten_loss = loss_list[-50:]\n",
    "    threshold = 0.5\n",
    "    if len(last_ten_loss)==50 and len(loss_list)>=50:\n",
    "        diffs = np.diff(last_ten_loss)\n",
    "        step_trends = []\n",
    "        for d in diffs:\n",
    "            if d > threshold:\n",
    "                step_trends.append(\"increasing\")\n",
    "            elif d < -threshold:\n",
    "                step_trends.append(\"decreasing\")\n",
    "            else:\n",
    "                step_trends.append(\"steady\")\n",
    "\n",
    "        if all(t == \"steady\" for t in step_trends):\n",
    "            return True \n",
    "        else:\n",
    "            print (\"Trend\", step_trends)\n",
    "    return False \n",
    "\n",
    "\n",
    "\n",
    "##### Setup Training #####\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "all_params = calculate_total_train_params(encoder_model, decoder_model)\n",
    "\n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TrainingConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/TrainingConfig.accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 10\n",
    "stop = False \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "for epoch in range(TrainingConfig.epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1     \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "            logits, caption_loss  = decoder_model(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "\n",
    "            loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "        # x_embed = image_encoder(image_tensor) # (B, N, embed_size) \n",
    "        # logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "        # loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "        if (step + 1) % TrainingConfig.accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        if global_step %100==0:\n",
    "            val_loss = eval()\n",
    "            loss_list.append(val_loss)\n",
    "            save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "\n",
    "            if should_stop(loss_list):\n",
    "                stop = True \n",
    "                break\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{TrainingConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*TrainingConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*TrainingConfig.accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            # save_model(image_encoder=image_encoder, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % TrainingConfig.accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    if stop:\n",
    "        save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "        break\n",
    "    \n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "tup = get_models() \n",
    "\n",
    "TrainingConfig, tokenizer = tup[0], tup[-1]\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "from transformers import LogitsProcessorList, MinLengthLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
    "\n",
    "from transformers.generation.logits_process import (\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "\n",
    "# Chain multiple warpers\n",
    "warpers = LogitsProcessorList([\n",
    "    TemperatureLogitsWarper(0.8),\n",
    "    TopKLogitsWarper(50),\n",
    "    TopPLogitsWarper(0.95),\n",
    "])\n",
    "\n",
    "processors = LogitsProcessorList([\n",
    "    RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "])\n",
    "\n",
    "from utils import load_from_checkpoint \n",
    "\n",
    "encoder, decoder , epoch, loss , global_step, tokenizer = load_from_checkpoint()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "def generate_caption(image_tensor, max_len=20, use_image=True):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "\n",
    "    x_embed = encoder(image_tensor) \n",
    "\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    generated_ids = torch.tensor([[start_id]], device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            attn_mask = torch.ones(1, generated_ids.shape[1], dtype=torch.bfloat16, device=device)\n",
    "\n",
    "            logits, _ = decoder(x_embed, generated_ids, attn_mask, mode=\"train\")\n",
    "\n",
    "            next_logits = logits[:, -1, :]\n",
    "\n",
    "            next_logits = next_logits.to(torch.bfloat16)\n",
    "\n",
    "            next_logits = processors(generated_ids.cpu(), next_logits.cpu())\n",
    "\n",
    "            next_logits = warpers(None, next_logits.cpu())\n",
    "\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "        \n",
    "            # Sample on CPU to avoid MPS issues\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).to(device)\n",
    "\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.convert_tokens_to_ids(\"<END>\"):\n",
    "            break\n",
    "\n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "# Example usage\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    #caption_without_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=False)\n",
    "\n",
    "    caption_with_image = generate_caption(image_tensor[0], use_image=True)\n",
    "\n",
    "    plt.imshow(image_tensor[0].float().permute(1,2,0).cpu().numpy())\n",
    "    print(\"With image context:\\t \\t\", caption_with_image)\n",
    "    print (\"Actual\\t\\t\", tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True))\n",
    "    # print(\"Without image context: \", caption_without_image)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75993e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
