{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 2\n",
    "epochs = 10\n",
    "learning_rate = 1e-2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.datasets import  mnist \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def conv2d_vectorized(x, conv_filter, stride=1, padding=0):\n",
    "    # x: [H, W], conv_filter: [kH, kW]\n",
    "    x = x.unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "    x_unf = F.unfold(x, kernel_size=conv_filter.shape, stride=stride, padding=padding)\n",
    "    # x_unf: [1, kH*kW, out_H*out_W]\n",
    "    conv_flat = conv_filter.flatten().unsqueeze(1)  # [kH*kW, 1]\n",
    "    out = torch.matmul(conv_flat.T, x_unf)  # [1, out_H*out_W]\n",
    "    out_H = (x.shape[2] + 2*padding - conv_filter.shape[0]) // stride + 1\n",
    "    out_W = (x.shape[3] + 2*padding - conv_filter.shape[1]) // stride + 1\n",
    "    return out.view(1, out_H, out_W).squeeze(0)\n",
    "\n",
    "\n",
    "def conv2d(x, conv_filter, stride, padding):\n",
    "    H, W = x.shape \n",
    "\n",
    "    x_padded = torch.zeros(H+2*padding, W+2*padding, device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = conv_filter.shape\n",
    "    \n",
    "    out_H = (H + 2*padding - kH)//stride + 1 \n",
    "    out_W = (W + 2*padding - kW)//stride + 1 \n",
    "\n",
    "    output_map = torch.zeros(out_H, out_W, device=device)\n",
    "\n",
    "    for i in range(0, out_H*stride, stride):\n",
    "        for j in range(0, out_W*stride, stride):\n",
    "            output_map[i//stride, j//stride] = torch.sum((x_padded[i:i+kH, j:j+kW] * conv_filter))\n",
    "    return output_map\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, padding, stride, filter_size):\n",
    "        super().__init__()\n",
    "\n",
    "        #self.filters = nn.Parameter(torch.randn(number_of_filter, filter_size, filter_size) * 0.1)\n",
    "        self.filters = nn.Parameter(torch.randn(output_channels, input_channels, filter_size, filter_size, device=device) * 0.1)\n",
    "\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filter_size = filter_size \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape  ### [B, C, H, W]\n",
    "\n",
    "        filters_flat = self.filters.view(self.output_channels, -1) \n",
    "\n",
    "        x_unf = F.unfold(x, kernel_size=self.filter_size, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        filters_flat_exp = filters_flat.unsqueeze(0).expand(B, -1, -1)\n",
    "\n",
    "\n",
    "        out = torch.bmm(filters_flat_exp, x_unf)\n",
    "        out_H = (H + 2*self.padding - self.filter_size)//self.stride + 1\n",
    "        out_W = (W + 2*self.padding - self.filter_size)//self.stride + 1\n",
    "        out = out.view(B, self.number_of_filter, out_H, out_W)\n",
    "        return out\n",
    "\n",
    "\n",
    "        # output = [] \n",
    "        \n",
    "        # for b in range(B):\n",
    "        #     feature_map = []\n",
    "        #     for j in range(self.number_of_filter):\n",
    "        #         #output_conv2d = conv2d(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "        #         output_conv2d = conv2d_vectorized(x[b, 0], self.filters[j].to(device),  self.stride, self.padding)\n",
    "        #         feature_map.append(output_conv2d) ##[1, out_H, out_W]\n",
    "        #     output.append(torch.stack(feature_map)) ##[F, out_H, out_W]\n",
    "        # return torch.stack(output) ##[B, F, out_H, out_W]\n",
    "    \n",
    "\n",
    "class CNNMOdel(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = ConvolutionLayer(input_channels=1, output_channels=2 padding=1, stride=1, filter_size=3)\n",
    "        self.conv2 = ConvolutionLayer(input_channels=2, output_channels=2, padding=1, stride=1, filter_size=3)\n",
    "        num_features = 2*28*28\n",
    "        self.classifier = nn.Linear(num_features, classes, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten [B, 8*28*28]\n",
    "        x = self.classifier(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images[:10]\n",
    "train_labels = train_labels[:10]\n",
    "test_images = test_images[:10]\n",
    "test_labels = test_labels[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert to float tensors and normalize\n",
    "train_images = torch.tensor(train_images, dtype=torch.bfloat16) / 255.0\n",
    "test_images = torch.tensor(test_images, dtype=torch.bfloat16) / 255.0\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Add channel dimension: (N, C, H, W)\n",
    "train_images = train_images.unsqueeze(1)  # (N, 1, 28, 28)\n",
    "test_images = test_images.unsqueeze(1)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Model\n",
    "model = CNNMOdel(classes=10)\n",
    "model  = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "total_steps = epochs * train_loader.__len__()\n",
    "\n",
    "steps = 0 \n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps+=1\n",
    "\n",
    "        if steps%5==0:\n",
    "            print (f\"steps {steps} Loss {loss.item()}\")\n",
    "            \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_cocooptions = CocoCaptions(\n",
    "    root='train2017',\n",
    "    annFile='annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_detection = CocoDetection(\n",
    "    root='train2017',\n",
    "    annFile='annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "\n",
    "all_captions = \"\\n\".join([caption for captions_list in train_dataset_cocooptions for caption in captions_list[1]])\n",
    "all_words = list(all_captions.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "for word in all_words:\n",
    "    counter[word]+=1\n",
    "\n",
    "vocab = [word for word, cnt in counter.items() if cnt>5]\n",
    "vocab +=[\"UNK\", \"<START>\", \"<END>\", \"<PAD>\"]\n",
    "\n",
    "\n",
    "word2idx =  {item:i for i, item in enumerate(vocab)}\n",
    "idx2word =  {i:item for i, item in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def encode(stri):\n",
    "    all_tensor = [word2idx.get(word, word2idx[\"UNK\"]) for word in stri.split(\" \")]\n",
    "    return all_tensor \n",
    "\n",
    "def decode(input_tensor):\n",
    "    return [idx2word[each] for each in input_tensor]\n",
    "    \n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=50):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "        caption_tensor = encode(caption)\n",
    "\n",
    "        if len(caption_tensor) < self.caption_length:\n",
    "            caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "\n",
    "        else:\n",
    "            caption_tensor = caption_tensor[:self.caption_length]\n",
    "        return image_tensor, torch.tensor(caption_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f69fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.functional import F \n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from torchvision.datasets import CocoCaptions, CocoDetection\n",
    "from torchvision import transforms\n",
    "import torch \n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import GPT2LMHeadModel\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "special_tokens = {\"additional_special_tokens\": [\"<START>\", \"<END>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(\"<PAD>\")\n",
    "start_token_id = tokenizer.convert_tokens_to_ids(\"<START>\") \n",
    "end_token_id = tokenizer.convert_tokens_to_ids(\"<END>\") \n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_cocooptions = CocoCaptions(\n",
    "    root='train2017',\n",
    "    annFile='annotations/captions_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset_detection = CocoDetection(\n",
    "    root='train2017',\n",
    "    annFile='annotations/instances_train2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "N = 1000\n",
    "\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset_cocooptions = Subset(train_dataset_cocooptions, range(N))\n",
    "train_dataset_detection = Subset(train_dataset_detection, range(N))\n",
    "\n",
    "\n",
    "# all_captions = \"\\n\".join([caption for captions_list in train_dataset_cocooptions for caption in captions_list[1]])\n",
    "# all_words = list(all_captions.split(\" \"))\n",
    "\n",
    "\n",
    "\n",
    "# counter = Counter()\n",
    "# for word in all_words:\n",
    "#     counter[word]+=1\n",
    "\n",
    "# vocab = [word for word, cnt in counter.items() if cnt>5]\n",
    "# vocab +=[\"UNK\", \"<START>\", \"<END>\", \"<PAD>\"]\n",
    "\n",
    "\n",
    "# word2idx =  {item:i for i, item in enumerate(vocab)}\n",
    "# idx2word =  {i:item for i, item in enumerate(vocab)}\n",
    "\n",
    "\n",
    "# def encode(stri):\n",
    "#     all_tensor = [word2idx.get(word, word2idx[\"UNK\"]) for word in stri.split(\" \")]\n",
    "#     return all_tensor \n",
    "\n",
    "# def decode(input_tensor):\n",
    "#     return [idx2word[each] for each in input_tensor]\n",
    "\n",
    "\n",
    "class DataLoaderLite(Dataset):\n",
    "    def __init__(self, train_dataset_cocooptions, caption_length=50, tokenizer=tokenizer):\n",
    "        self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "        self.caption_length = caption_length\n",
    "        self.tokenizer = tokenizer \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "        # prepend <START>, append <END>\n",
    "        caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "\n",
    "        # tokenize with GPT2 tokenizer\n",
    "        tokens = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.caption_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return image_tensor, tokens[\"input_ids\"].squeeze(0), tokens[\"attention_mask\"].squeeze(0)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "\n",
    "    #     # Prepend <IMG> + <START>, Append <END>\n",
    "    #     caption = \"<IMG> <START> \" + image_captions[0] + \" <END>\"\n",
    "    #     caption_tensor = encode(caption)\n",
    "\n",
    "    #     if len(caption_tensor) < self.caption_length:\n",
    "    #         caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "    #     else:\n",
    "    #         caption_tensor = caption_tensor[:self.caption_length]\n",
    "\n",
    "    #     return image_tensor, torch.tensor(caption_tensor)\n",
    "\n",
    "    \n",
    "\n",
    "# class DataLoaderLite(Dataset):\n",
    "\n",
    "#     def __init__(self, train_dataset_cocooptions, caption_length=50):\n",
    "#         self.train_dataset_cocooptions = train_dataset_cocooptions\n",
    "#         self.caption_length = caption_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.train_dataset_cocooptions)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         image_tensor, image_captions = self.train_dataset_cocooptions[idx]\n",
    "#         caption = \"<START> \" + image_captions[0] + \" <END>\"\n",
    "#         caption_tensor = encode(caption)\n",
    "\n",
    "#         if len(caption_tensor) < self.caption_length:\n",
    "#             caption_tensor += [word2idx[\"<PAD>\"]] * (self.caption_length - len(caption_tensor))\n",
    "\n",
    "#         else:\n",
    "#             caption_tensor = caption_tensor[:self.caption_length]\n",
    "#         return image_tensor, torch.tensor(caption_tensor)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def conv2d(x, kernel, stride, padding):\n",
    "    H, W = x.shape\n",
    "    device = x.device \n",
    "    x_padded = torch.zeros((H+2*padding, W+2*padding), device=device)\n",
    "    x_padded[padding:H+padding, padding:W+padding] = x \n",
    "\n",
    "    kH, kW = kernel.shape \n",
    "    out_H = (H+2*padding-kH)//stride +1\n",
    "    out_W = (W+2*padding-kW)//stride +1\n",
    "\n",
    "    feature_map = torch.zeros((out_H, out_W), device=device)\n",
    "\n",
    "    for i in range(0, (H+2*padding-kH+1), stride):\n",
    "        for j in range(0, (W+2*padding-kW+1), stride):\n",
    "            region = x_padded[i:i+kH, j:j+kW] \n",
    "            feature_map[i//stride, j//stride] = torch.sum(kernel.to(region.device) * region)\n",
    "    return feature_map\n",
    "\n",
    "\n",
    "\n",
    "class ConvolutionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels, output_channels, padding, stride, kernel_size):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.padding = padding\n",
    "        self.stride = stride \n",
    "        self.kernel_size = kernel_size \n",
    "        self.kernel = nn.Parameter(torch.randn(self.output_channels, self.input_channels, self.kernel_size, self.kernel_size))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        output = []\n",
    "        for batch in range(B):\n",
    "            output_feature_map = []\n",
    "            for each_output_channel in range(self.output_channels):\n",
    "                feature_map = torch.zeros(((H+2*self.padding-self.kernel_size)//self.stride +1, (W+2*self.padding-self.kernel_size)//self.stride +1), device=device)\n",
    "\n",
    "                for each_input_channel in range(self.input_channels):\n",
    "                    feature_map += conv2d(x[batch, each_input_channel], self.kernel[each_output_channel, each_input_channel], self.stride, self.padding)\n",
    "                output_feature_map.append(feature_map)\n",
    "            output.append(torch.stack(output_feature_map))\n",
    "        return torch.stack(output)\n",
    "    \n",
    "\n",
    "\n",
    "class ResnetGPT2Wrapper(nn.Module):\n",
    "    def __init__(self, gpt_decoder, embed_size, vocab_size, num_img_tokens=5):\n",
    "        super().__init__()\n",
    "        self.gpt_decoder = gpt_decoder\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_img_tokens = num_img_tokens\n",
    "\n",
    "\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.layernorm = nn.LayerNorm(embed_size, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.img_queries = nn.Parameter(torch.randn(num_img_tokens, embed_size) * 0.01)\n",
    "\n",
    "\n",
    "    def forward(self, img_features, captions_tensor, attention_mask=None):\n",
    "        img_features = img_features.float()\n",
    "         # 1. Token embeddings from GPT2\n",
    "        tok_embeds = self.gpt_decoder.transformer.wte(captions_tensor)  # (B, T, D)\n",
    "\n",
    "        B = tok_embeds.shape[0]\n",
    "\n",
    "        queries = self.img_queries.unsqueeze(0).expand(B, -1, -1)  # (B, num_img_tokens, D)\n",
    "\n",
    "\n",
    "        B, T, D = tok_embeds.shape\n",
    "        N = img_features.shape[1]\n",
    "\n",
    "        k = self.key_proj(img_features)              # (B, N, D)\n",
    "        v = self.value_proj(img_features)            # (B, N, D)\n",
    "        \n",
    "        enriched, _ = self.mha(self.query_proj(queries), k, v)  # (B, M, D)\n",
    "\n",
    "        enriched = self.layernorm(queries + enriched) \n",
    "\n",
    "        fused = torch.cat([enriched, tok_embeds], dim=1)  # (B, M+T, D)\n",
    "\n",
    "        # query = self.query_proj(tok_embeds)\n",
    "        # keys  = self.key_proj(img_features)\n",
    "        # values = self.value_proj(img_features)\n",
    "        # enriched, attn_weights = self.mha(query, keys, values)\n",
    "\n",
    "        enriched = self.dropout(fused)\n",
    "\n",
    "\n",
    "        # enriched = enriched + tok_embeds  # residual connection\n",
    "\n",
    "        inputs_embeds = enriched[:, :-1, :].contiguous()\n",
    "        labels = captions_tensor[:, 1:].contiguous()\n",
    "\n",
    "\n",
    "        pad_for_img = torch.full((B, self.num_img_tokens, ), pad_token_id, dtype=torch.long, device=labels.device)\n",
    "\n",
    "        labels = torch.cat([pad_for_img, labels], dim=1)   # (B, M + T - 1)\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            img_mask = torch.ones(B, self.num_img_tokens, device=attention_mask.device)\n",
    "            attention_mask = torch.cat([img_mask, attention_mask], dim=1)\n",
    "            attention_mask = attention_mask[:, :-1].contiguous()\n",
    "\n",
    "        outputs = self.gpt_decoder(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        return outputs.logits, outputs.loss \n",
    "\n",
    "    \n",
    "class LSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm_layer = nn.LSTM(2*embed_size, hidden_size=hidden_size, batch_first=True, num_layers=3)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_size, num_heads=4, batch_first=True)\n",
    "        self.key_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.value_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.query_proj = nn.Linear(embed_size, embed_size, dtype=torch.bfloat16)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding_layer(captions[:, :-1])  # teacher forcing\n",
    "        # features = features.unsqueeze(1)  # (B, 1, embed_size)\n",
    "\n",
    "        \n",
    "        query = self.query_proj(embeddings)\n",
    "\n",
    "        keys  = self.key_proj(features) \n",
    "\n",
    "        values = self.value_proj(features)\n",
    "\n",
    "        # keys = keys.unsqueeze(1)\n",
    "        # values = values.unsqueeze(1)\n",
    "\n",
    "\n",
    "        attn_out, attn_weights = self.mha(query, keys, values)\n",
    "\n",
    "\n",
    "        attn_out = torch.cat((embeddings, attn_out), dim=-1)\n",
    "\n",
    "\n",
    "        # print (f\"==== attn_weights\", attn_weights.shape)\n",
    "        # LLLL\n",
    "\n",
    "        # inputs = torch.cat((attn_out, embeddings), dim=1)\n",
    "        outputs, _ = self.lstm_layer(attn_out)\n",
    "\n",
    "\n",
    "        outputs = self.fc(outputs)  # (B, T, vocab_size)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def get_2d_sincos_pos_embed(embed_dim, grid_h, grid_w):\n",
    "    \"\"\"Return 2D sine-cosine positional embeddings\"\"\"\n",
    "    grid_y = torch.arange(grid_h, dtype=torch.bfloat16)\n",
    "    grid_x = torch.arange(grid_w, dtype=torch.bfloat16)\n",
    "    grid = torch.meshgrid(grid_y, grid_x, indexing='ij')  # (H, W)\n",
    "    grid = torch.stack(grid, dim=-1)  # (H, W, 2)\n",
    "\n",
    "    # flatten\n",
    "    grid = grid.reshape(-1, 2)  # (H*W, 2)\n",
    "\n",
    "    # compute embeddings\n",
    "    pos_emb = []\n",
    "    for dim in range(embed_dim // 2):\n",
    "        div_term = 10000 ** (2 * (dim // 2) / embed_dim)\n",
    "        pos_emb.append(torch.sin(grid / div_term) if dim % 2 == 0 else torch.cos(grid / div_term))\n",
    "    pos_emb = torch.cat(pos_emb, dim=1)  # (H*W, embed_dim)\n",
    "    return pos_emb\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResnetEncoder(nn.Module):\n",
    "    def __init__(self, embed_size, freeze_until_layer=5):\n",
    "        super().__init__()\n",
    "        # load pretrained ResNet\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]  # remove the last fc layer\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "\n",
    "        # freeze layers\n",
    "        child_counter = 0\n",
    "        for child in self.backbone.children():\n",
    "            if child_counter < freeze_until_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            child_counter += 1\n",
    "       \n",
    "\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (B, 3, 224, 224) -> (B, 2048, H/32, W/32)\n",
    "        feats = self.backbone(x)\n",
    "        B, C, H, W = feats.shape\n",
    "        feats = feats.view(B, C, -1).permute(0, 2, 1)  # (B, H*W, C)\n",
    "        feats = self.fc(feats)  # (B, H*W, embed_size)\n",
    "        return feats\n",
    "\n",
    "\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "\n",
    "def save_model(image_encoder, caption_encoder):\n",
    "    model_dict = {\n",
    "        \"image_encoder_state\": image_encoder.state_dict(),\n",
    "        \"caption_encoder_state\": caption_encoder.state_dict(),\n",
    "        \"image_encoder_class\": image_encoder.__class__,\n",
    "        \"caption_encoder_class\": caption_encoder.__class__,\n",
    "        \"image_encoder_args\": image_encoder.args if hasattr(image_encoder, \"args\") else (),\n",
    "        \"caption_encoder_args\": caption_encoder.args if hasattr(caption_encoder, \"args\") else (),\n",
    "    }\n",
    "    torch.save(model_dict, checkpoint_path)\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model_dict = torch.load(checkpoint_path, map_location=\"mps\")\n",
    "\n",
    "    image_encoder = model_dict[\"image_encoder_class\"](*model_dict[\"image_encoder_args\"])\n",
    "    caption_encoder = model_dict[\"caption_encoder_class\"](*model_dict[\"caption_encoder_args\"])\n",
    "\n",
    "    image_encoder.load_state_dict(model_dict[\"image_encoder_state\"])\n",
    "    caption_encoder.load_state_dict(model_dict[\"caption_encoder_state\"])\n",
    "\n",
    "    return image_encoder, caption_encoder\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, input_shape):\n",
    "        super().__init__()\n",
    "   \n",
    "        # More filters + strides to reduce spatial dims\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)  # 224 -> 112\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # 112 -> 56\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # 56 -> 28\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1) # 28 -> 14\n",
    "\n",
    "        pos_emb = get_2d_sincos_pos_embed(embed_size, 14, 14)  # (196, embed_size)\n",
    "        self.register_buffer(\"pos_embed\", pos_emb.unsqueeze(0))  # (1, 196, embed_size)\n",
    "\n",
    "\n",
    "        self.to(device)\n",
    "        with torch.no_grad():\n",
    "            B, C, H, W = input_shape[:]\n",
    "            x_dummy = torch.randn((B, C, H, W), device=device)\n",
    "            x_dummy = self.conv1(x_dummy)\n",
    "            x_dummy = self.conv2(x_dummy)\n",
    "            x_dummy = self.conv3(x_dummy)\n",
    "            x_dummy = self.conv4(x_dummy)\n",
    "            B, C, H, W = x_dummy.shape \n",
    "            del x_dummy\n",
    "            torch.mps.empty_cache()  # if using MPS\n",
    "            import gc; gc.collect()\n",
    "        self.fc = nn.Linear(C, embed_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) \n",
    "        x = self.conv2(x) \n",
    "        x = F.relu(x) \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H*W)\n",
    "        x = x.permute(0, 2, 1)  # (B, H*W, C)\n",
    "        # x_embed = self.fc(x)  # (B, H*W, embed_size\n",
    "        B, N, C = x.shape\n",
    "        x_embed = self.fc(x)   # (B, N, embed_size) \n",
    "        x_embed = x_embed + self.pos_embed[:, :N, :].to(x_embed.device)  # add positional embedding\n",
    "        return x_embed  \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# # caption_encoder = LSTMEncoder(embed_size, hidden_size, vocab_size)\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# gpt_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# gpt_decoder.resize_token_embeddings(gpt_decoder.get_input_embeddings().num_embeddings + 2)  # Example: add 3 tokens\n",
    "# vocab_size = gpt_decoder.get_input_embeddings().num_embeddings\n",
    "\n",
    "\n",
    "\n",
    "# gpt_hidden_size = gpt_decoder.config.hidden_size\n",
    "# embed_size = gpt_hidden_size  # to match GPT2 hidden size\n",
    "# hidden_size = gpt_hidden_size\n",
    "# batch_size = 4\n",
    "# input_channels = 3  \n",
    "# image_h, image_w = 224, 224\n",
    "# steps = 0\n",
    "# epochs = 1\n",
    "# lr = 1e-5\n",
    "# accumulation_steps = 4  # simulate batch_size * 2\n",
    "\n",
    " \n",
    "\n",
    "gpt_decoder = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt_decoder.resize_token_embeddings(gpt_decoder.get_input_embeddings().num_embeddings + 2)  # Example: add 3 tokens\n",
    "\n",
    "\n",
    "checkpoint_path = \"checkpoint.pth\"\n",
    "device = \"mps\"\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    gpt_hidden_size = gpt_decoder.config.hidden_size\n",
    "    embed_size = gpt_hidden_size  # to match GPT2 hidden size\n",
    "    hidden_size = gpt_hidden_size\n",
    "    batch_size = 4\n",
    "    input_channels = 3  \n",
    "    image_h, image_w = 224, 224\n",
    "    steps = 0\n",
    "    epochs = 1\n",
    "    lr = 1e-5\n",
    "    accumulation_steps = 4  # simulate batch_size * 2\n",
    "    vocab_size = gpt_decoder.get_input_embeddings().num_embedding\n",
    "\n",
    "\n",
    "train_dataset_cocooptions=DataLoaderLite(train_dataset_cocooptions, caption_length=50, tokenizer=tokenizer)           \n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "total_steps = len(train_dataloader)  * epochs\n",
    "import math \n",
    "\n",
    "formatted_str = f\"Training details vocab size {vocab_size} batch size {batch_size} image size {image_h}x{image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(vocab_size))}\"\n",
    "\n",
    "\n",
    "print (formatted_str)\n",
    "scaler = GradScaler()\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "#encoder_model = CNNEncoder(embed_size, [batch_size, input_channels, image_h, image_w])\n",
    "\n",
    "encoder_model = ResnetEncoder(embed_size)\n",
    "encoder_model = encoder_model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "caption_encoder = ResnetGPT2Wrapper(gpt_decoder, embed_size, vocab_size)\n",
    "\n",
    "caption_encoder = caption_encoder.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_params = list([param for param in encoder_model.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "\n",
    "# print (\"Trainable parameters in encoder model:\")\n",
    "# print (sum(p.numel() for p in all_params if p.requires_grad))\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(all_params, lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # print (f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1\n",
    "\n",
    "        # print (f\"Step {step+1}/{len(train_dataloader)} Global step {global_step}/{total_steps}\")\n",
    "     \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "\n",
    "            # print (\"Running encoder model\")\n",
    "            # print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            # print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "\n",
    "            # print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "            # print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "\n",
    "            #x_caption = caption_encoder(x_embed, caption_tensor)\n",
    "\n",
    "\n",
    "            logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "\n",
    "            # B, T, C = logits.shape\n",
    "            # preds = logits.reshape(B*T, C)\n",
    "            # targets = caption_tensor[:, 1:].reshape(-1)\n",
    "\n",
    "            # print (f\" prediction {preds.shape} targets: {targets.shape}\")\n",
    "            \n",
    "            # caption_loss = loss_fn(preds, targets)\n",
    "            loss = caption_loss / accumulation_steps  \n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        all_params = list([param for param in encoder_model.parameters() if param.requires_grad]) + list(caption_encoder.parameters())\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            save_model(image_encoder=encoder_model, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        # if step % 100 == 0:\n",
    "        #     print(f\" epoch {epoch+1}/{epochs} step {step}/{total_steps} Loss: {loss.item()}\")\n",
    "\n",
    "        # if step % 1 == 0:\n",
    "        #     print(\"current allocated memory:\", torch.mps.current_allocated_memory() / 1e9, \"GB\")\n",
    "        #     print(\"driver allocated memory:\", torch.mps.driver_allocated_memory() / 1e9, \"GB\")\n",
    "\n",
    "\n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb5791",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:25:36.932996Z",
     "start_time": "2025-09-30T03:25:28.876906Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "import os \n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class GPT2WithCross(GPT2LMHeadModel):\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        attention_mask=None,\n",
    "        img_feats=None,\n",
    "        labels=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            inputs_embeds = self.transformer.wte(input_ids)\n",
    "\n",
    "        # Run through transformer, passing img_feats to each block\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            hidden_states = block(hidden_states, img_feats=img_feats, attention_mask=attention_mask)[0]\n",
    "\n",
    "        # Layer norm\n",
    "        hidden_states = self.transformer.ln_f(hidden_states)\n",
    "\n",
    "\n",
    "        # LM head\n",
    "        logits = self.lm_head(hidden_states)\n",
    "\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift for language modeling loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81680d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:26:43.835029Z",
     "start_time": "2025-09-30T03:26:36.736736Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import setup_data\n",
    "from setup_model import get_models \n",
    "from datasetlite import DataLoaderLite \n",
    "\n",
    "\n",
    "import math \n",
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#----- Model Setup -------\n",
    "\n",
    "TrainingConfig, encoder_model, decoder_model , pad_token_id, tokenizer = get_models() \n",
    "\n",
    "\n",
    "\n",
    "torch.mps.empty_cache()\n",
    "import gc; gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, input_ids, attention_mask = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    # pad input_ids and attention_mask to the max length in this batch\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return images, input_ids, attention_mask\n",
    "\n",
    "\n",
    "train_dataset_cocooptions, val_dataset_cocooptions, train_dataset_detection , val_dataset_detection = setup_data(TrainingConfig.number_of_items)\n",
    "train_dataset_cocooptions = DataLoaderLite(train_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "val_dataset_cocooptions = DataLoaderLite(val_dataset_cocooptions, caption_length=TrainingConfig.caption_len, tokenizer=tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset_cocooptions, batch_size=TrainingConfig.batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader)  * TrainingConfig.epochs\n",
    "formatted_str = f\"Training details vocab size {TrainingConfig.vocab_size} batch size {TrainingConfig.batch_size} image size {TrainingConfig.image_h}x{TrainingConfig.image_w}\"\n",
    "formatted_str+= f\" total steps {total_steps} epochs {TrainingConfig.epochs}\"\n",
    "formatted_str+= f\"Max loss {math.log(TrainingConfig.vocab_size)}\"\n",
    "formatted_str+= f\"Perplexity {math.exp(math.log(TrainingConfig.vocab_size))}\"\n",
    "\n",
    "print (formatted_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5586223",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T03:27:49.087304Z",
     "start_time": "2025-09-30T03:27:28.355910Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler\n",
    "import numpy as np \n",
    "from utils import calculate_total_train_params, save_to_checkpoint\n",
    "import torch.nn as nn \n",
    "\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # --------------------\n",
    "    #  Validation step\n",
    "    # --------------------\n",
    "    decoder_model.eval()\n",
    "    encoder_model.eval()\n",
    "    val_loss = 0\n",
    "    count = 0 \n",
    "    with torch.no_grad():\n",
    "        for val_batch in val_dataloader:\n",
    "            image_tensor, caption_tensor, attention_mask = [x.to(device) for x in val_batch]\n",
    "            with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "                x_embed = encoder_model(image_tensor)\n",
    "                _, val_caption_loss = decoder_model(x_embed, caption_tensor, attention_mask)\n",
    "            val_loss += val_caption_loss.item()\n",
    "            count+=1\n",
    "            if count > 2:break \n",
    "    val_loss /= count \n",
    "    decoder_model.train()\n",
    "    encoder_model.train()\n",
    "    print(f\"Epoch {epoch+1}: train_loss={total_loss/len(train_dataloader):.4f}, val_loss={val_loss:.4f}\")\n",
    "    return val_loss \n",
    "\n",
    "\n",
    "\n",
    "def should_stop(loss_list):\n",
    "    last_ten_loss = loss_list[-100:]\n",
    "    threshold = 0.4\n",
    "    if len(last_ten_loss)==100 and len(loss_list)>=100:\n",
    "        diffs = np.diff(last_ten_loss)\n",
    "        step_trends = []\n",
    "        for d in diffs:\n",
    "            if d > threshold:\n",
    "                step_trends.append(\"increasing\")\n",
    "            elif d < -threshold:\n",
    "                step_trends.append(\"decreasing\")\n",
    "            else:\n",
    "                step_trends.append(\"steady\")\n",
    "\n",
    "        if all(t == \"steady\" for t in step_trends):\n",
    "            return True \n",
    "        else:\n",
    "            print (\"Trend\", step_trends)\n",
    "    return False \n",
    "\n",
    "\n",
    "\n",
    "##### Setup Training #####\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "all_params = calculate_total_train_params(encoder_model, decoder_model)\n",
    "\n",
    "\n",
    "print (f\"Trainable parameters in encoder model: {sum(p.numel() for p in all_params if p.requires_grad)/1e6} M\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(all_params, lr=TrainingConfig.lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps/TrainingConfig.accumulation_steps, eta_min=1e-6)\n",
    "\n",
    "import time \n",
    "start_time = time.time()\n",
    "total_loss = 0 \n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "steps_no_improve = 0\n",
    "patience_steps = 10\n",
    "stop = False \n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "for epoch in range(TrainingConfig.epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "        image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "        B, C, H, W = image_tensor.shape\n",
    "\n",
    "        global_step = epoch * len(train_dataloader) + step + 1     \n",
    "\n",
    "        with torch.autocast(\"mps\", enabled=True, dtype=torch.bfloat16):\n",
    "            x_embed = encoder_model(image_tensor) # (B, N, embed_size) \n",
    "            logits, caption_loss  = decoder_model(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "            loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "        # x_embed = image_encoder(image_tensor) # (B, N, embed_size) \n",
    "        # logits, caption_loss  = caption_encoder(x_embed, caption_tensor, attention_mask)  # (B, T-1, vocab_size)\n",
    "        # loss = caption_loss / TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, max_norm=5.0)\n",
    "        if (step + 1) % TrainingConfig.accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * TrainingConfig.accumulation_steps  \n",
    "\n",
    "\n",
    "        if global_step %10==0:\n",
    "            val_loss = eval()\n",
    "            loss_list.append(val_loss)\n",
    "            save_to_checkpoint(encoder_model, decoder_model, optimizer, epoch, loss, global_step)\n",
    "\n",
    "            if should_stop(loss_list):\n",
    "                stop = True \n",
    "                break\n",
    "\n",
    "          # estimate remaining time every 100 steps\n",
    "        if global_step % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            steps_per_sec = global_step / elapsed\n",
    "            remaining_steps = total_steps - global_step\n",
    "            est_remaining = remaining_steps / steps_per_sec\n",
    "            est_total = total_steps / steps_per_sec\n",
    "\n",
    "            print(f\"epoch {epoch+1}/{TrainingConfig.epochs} step {step}/{len(train_dataloader)} \"\n",
    "                  f\"Loss: {loss.item()*TrainingConfig.accumulation_steps:.4f} | \"\n",
    "                  f\"Elapsed: {elapsed/60:.2f} min | \"\n",
    "                  f\"ETA: {est_remaining/60:.2f} min | \"\n",
    "                  f\"Total est: {est_total/60:.2f} min | \"\n",
    "                  f\"Memory: {torch.mps.current_allocated_memory() / 1e9:.2f} GB , \\ {torch.mps.driver_allocated_memory() / 1e9:.2f} GB | \"\n",
    "                  f\"Perplexity {math.exp(loss.item()*TrainingConfig.accumulation_steps):.2f}\"\n",
    "                  )\n",
    "            \n",
    "            # save_model(image_encoder=image_encoder, caption_encoder=caption_encoder)\n",
    "\n",
    "    if (step + 1) % TrainingConfig.accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(all_params, 5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    if stop:\n",
    "        break\n",
    "    \n",
    "    del image_tensor, caption_tensor, x_embed, logits\n",
    "    torch.mps.empty_cache()\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fcda42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export PYTORCH_ENABLE_MPS_FALLBACK=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "from transformers import LogitsProcessorList, MinLengthLogitsProcessor, RepetitionPenaltyLogitsProcessor\n",
    "\n",
    "from transformers.generation.logits_process import (\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "\n",
    "# Chain multiple warpers\n",
    "warpers = LogitsProcessorList([\n",
    "    TemperatureLogitsWarper(0.8),\n",
    "    TopKLogitsWarper(50),\n",
    "    TopPLogitsWarper(0.95),\n",
    "])\n",
    "\n",
    "processors = LogitsProcessorList([\n",
    "    RepetitionPenaltyLogitsProcessor(penalty=1.2)\n",
    "])\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "def generate_caption(image_tensor, max_len=20, use_image=True):\n",
    "    caption_encoder.eval()\n",
    "    image_encoder.eval()\n",
    "\n",
    "    image_tensor = image_tensor.to(device).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    x_embed = image_encoder(image_tensor) \n",
    "\n",
    "\n",
    "\n",
    "    start_id = tokenizer.convert_tokens_to_ids(\"<START>\")\n",
    "    generated_ids = torch.tensor([[start_id]], device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            attn_mask = torch.ones(1, generated_ids.shape[1], dtype=torch.bfloat16, device=device)\n",
    "\n",
    "\n",
    "            logits, _ = caption_encoder(x_embed, generated_ids, attn_mask, mode=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "            next_logits = logits[:, -1, :]\n",
    "\n",
    "            next_logits = next_logits.to(torch.bfloat16)\n",
    "\n",
    "            logits = processors(generated_ids.cpu(), next_logits.cpu())\n",
    "\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "        \n",
    "            # Sample on CPU to avoid MPS issues\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1).to(device)\n",
    "\n",
    "\n",
    "        generated_ids = torch.cat([generated_ids, next_token_id], dim=1)\n",
    "\n",
    "        if next_token_id.item() == tokenizer.convert_tokens_to_ids(\"<END>\"):\n",
    "            break\n",
    "\n",
    " \n",
    "    caption = tokenizer.decode(generated_ids.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "    return caption\n",
    "\n",
    "\n",
    "counter = 0 \n",
    "# Example usage\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    image_tensor, caption_tensor, attention_mask = batch[0], batch[1], batch[2] # [B, 3, 224, 224], [B, T], [B, T] \n",
    "    image_tensor, caption_tensor, attention_mask = image_tensor.to(device), caption_tensor.to(device), attention_mask.to(device)\n",
    "    B, C, H, W = image_tensor.shape\n",
    "\n",
    "    #caption_without_image = generate_caption(encoder_model, caption_encoder, image_tensor[0], tokenizer, use_image=False)\n",
    "\n",
    "    caption_with_image = generate_caption(image_tensor[0], use_image=True)\n",
    "\n",
    "    plt.imshow(image_tensor[0].permute(1,2,0).cpu().numpy())\n",
    "    print(\"With image context:\\t \\t\", caption_with_image)\n",
    "    print (\"Actual\\t\\t\", tokenizer.decode(caption_tensor[0].tolist(), skip_special_tokens=True))\n",
    "    # print(\"Without image context: \", caption_without_image)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "globals().get(\"encoder\") or \"DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea82d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
